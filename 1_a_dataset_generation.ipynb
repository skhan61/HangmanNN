{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from scr.dataset import *\n",
    "from scr.game import *\n",
    "import gc\n",
    "from scr.utils import print_scenarios\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from scr.feature_engineering import \\\n",
    "    calculate_char_frequencies, calculate_word_frequencies\n",
    "from scr.utils import read_words, save_words_to_file\n",
    "from scr.dataset import HangmanDataset\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "# Custom library paths\n",
    "sys.path.extend(['../', './scr'])\n",
    "\n",
    "from scr.utils import set_seed\n",
    "from scr.utils import read_words\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Read and Shuffle Word List\n",
    "word_list = read_words('data/words_250000_train.txt') # , limit=10000)\n",
    "# word_list = read_words('data/250k.txt', limit=10000)\n",
    "random.shuffle(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Dataset Function\n",
    "def split_dataset(word_list, train_ratio=0.7, val_ratio=0.15):\n",
    "    total_words = len(word_list)\n",
    "    train_size = int(total_words * train_ratio)\n",
    "    val_size = int(total_words * val_ratio)\n",
    "    random.shuffle(word_list)\n",
    "    return word_list[:train_size], word_list[train_size:train_size + val_size], \\\n",
    "        word_list[train_size + val_size:]\n",
    "\n",
    "\n",
    "# Splitting the word list\n",
    "train_words, val_words, test_words = split_dataset(word_list)\n",
    "\n",
    "# Save split datasets to files\n",
    "save_words_to_file(train_words, base_dataset_dir / 'train_words.txt')\n",
    "save_words_to_file(val_words, base_dataset_dir / 'val_words.txt')\n",
    "save_words_to_file(test_words, base_dataset_dir / 'test_words.txt')\n",
    "\n",
    "# Calculate Frequencies and Max Word Length\n",
    "word_frequencies = calculate_word_frequencies(train_words)\n",
    "char_frequency = calculate_char_frequencies(train_words)\n",
    "max_word_length = max(len(word) for word in train_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Reading and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define the base directory where you want to save the dataset\n",
    "base_dataset_dir = Path('./dataset/')\n",
    "# Ensure the base directory exists\n",
    "base_dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "pkls_dir = base_dataset_dir / 'pkl'\n",
    "pkls_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_repetitive_characters(word):\n",
    "    char_count = {}\n",
    "    for char in word:\n",
    "        char_count[char] = char_count.get(char, 0) + 1\n",
    "    return sum(1 for count in char_count.values() if count > 1)\n",
    "\n",
    "def adaptive_mask_prob_and_variants(word):\n",
    "    word_length = len(word)\n",
    "    unique_char_count = len(set(word))\n",
    "    repetitive_char_count = count_repetitive_characters(word)\n",
    "\n",
    "    # Lower mask probability for longer and more unique words\n",
    "    if word_length <= 5:\n",
    "        mask_prob = 0.8 if repetitive_char_count > (word_length / 2) else 0.9\n",
    "    \n",
    "    elif word_length <= 8:\n",
    "        mask_prob = 0.6 if unique_char_count > 4 else 0.7\n",
    "    \n",
    "    else:\n",
    "        mask_prob = 0.4 if unique_char_count > 6 else 0.5\n",
    "\n",
    "    # Adjust max variants based on the complexity of the word\n",
    "    if unique_char_count <= 3 or repetitive_char_count > (word_length / 2):\n",
    "        max_variants = max(5, word_length)\n",
    "    \n",
    "    elif unique_char_count <= 6:\n",
    "        max_variants = max(10, word_length)\n",
    "    \n",
    "    else:\n",
    "        max_variants = min(15, word_length)\n",
    "\n",
    "    return mask_prob, max_variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"noon\"\n",
    "mask_prob, max_variants = adaptive_mask_prob_and_variants(word)\n",
    "\n",
    "mask_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Game State Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.game import simulate_game_progress, play_game_with_a_word, process_word\n",
    "\n",
    "# Example word and initial state\n",
    "# Example usage\n",
    "# word = \"mississippi\"\n",
    "# word = \"cat\"\n",
    "mask_prob, max_variants = adaptive_mask_prob_and_variants(word)\n",
    "\n",
    "initial_states = process_word(word, mask_prob=mask_prob, \\\n",
    "    max_variants=max_variants)\n",
    "\n",
    "# Print generated initial states\n",
    "print(\"Generated Initial States:\")\n",
    "for initial_state in initial_states:\n",
    "    # Simulate the game\n",
    "    print(initial_state)\n",
    "    print(f\"For initial state: {initial_state}\")\n",
    "    won, game_progress = simulate_game_progress(\n",
    "        model=None,  # Assuming model is not used in this example\n",
    "        word=word, \n",
    "        initial_state=initial_state, \n",
    "        char_frequency={},  # Assuming char_frequency is not used in this example\n",
    "        max_word_length=len(word), \n",
    "        device=None,  # Assuming device is not used in this example\n",
    "        max_attempts=6, \n",
    "        normalize=True,\n",
    "        difficulty=\"medium\", \n",
    "        outcome_preference='win'\n",
    "    )\n",
    "\n",
    "    # Display game progress\n",
    "    for step in game_progress:\n",
    "        print(f\"Guessed: '{step[0]}', New State: '{step[1]}', Correct: {step[2]}\")\n",
    "\n",
    "        # break\n",
    "\n",
    "    # break\n",
    "\n",
    "    # print(\"Game Result:\", \"Won\" if won else \"Lost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(initial_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number words for state generation: 2007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 518/2007 [10:08<29:02,  1.17s/it]"
     ]
    }
   ],
   "source": [
    "from scr.custom_sampler import stratified_sample_by_length_and_frequency\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "# train_words = ['cat']\n",
    "\n",
    "# print(f'train words len: {len(train_words)}')\n",
    "\n",
    "NUM_STRATIFIED_SAMPLES = 2000\n",
    "# Main loop\n",
    "iteration = 0\n",
    "base_sample_size = 10  # Base number of samples per difficulty-outcome category\n",
    "\n",
    "sampled_words = stratified_sample_by_length_and_frequency(train_words, \\\n",
    "    word_frequencies, \\\n",
    "    NUM_STRATIFIED_SAMPLES)\n",
    "\n",
    "print(f'Number words for state generation: {len(sampled_words)}')\n",
    "\n",
    "for word in tqdm(sampled_words, miniters=2, leave=False, mininterval=2.0): \n",
    "    # , miniters=2, leave=False, mininterval=2.0):\n",
    "    # print(word)\n",
    "    all_scenarios = []\n",
    "    # Process the word to get initial masked states\n",
    "    # initial_masked_states = process_word(word, mask_prob=0.9, max_variants=10)\n",
    "\n",
    "    mask_prob, max_variants = adaptive_mask_prob_and_variants(word)\n",
    "\n",
    "    game_states = process_word(word, mask_prob=mask_prob, \\\n",
    "        max_variants=max_variants)\n",
    "\n",
    "    for initial_state in game_states:\n",
    "        \n",
    "        difficulties = [\"easy\", \"medium\", \"hard\"]\n",
    "        outcomes = [\"win\", \"lose\"]\n",
    "\n",
    "        for difficulty in difficulties:\n",
    "            for outcome in outcomes:\n",
    "                # print(f'{word} from initial state: {initial_state}: \\\n",
    "                # Difficulty: {difficulty}, Outcome: {outcome}')\n",
    "                won, game_progress = simulate_game_progress(\n",
    "                                        model=None, \n",
    "                                        word=word, \n",
    "                                        initial_state=initial_state, \n",
    "                                        char_frequency=char_frequency, \n",
    "                                        max_word_length=max_word_length, \n",
    "                                        device=device, \n",
    "                                        max_attempts=6, \n",
    "                                        normalize=True, \n",
    "                                        difficulty=difficulty, \n",
    "                                        outcome_preference=outcome\n",
    "                                    )\n",
    "\n",
    "                # all_scenarios.append({'word': word, 'difficulty': difficulty, \\\n",
    "                #     'outcome': outcome, 'data': (won, game_progress)})\n",
    "\n",
    "                all_scenarios.append({\n",
    "                            'word': word, \n",
    "                            'difficulty': difficulty,\n",
    "                            'outcome': outcome, \n",
    "                            'initial_state': initial_state,  # Added 'initial_state' key\n",
    "                            'data': (won, game_progress)\n",
    "                        })  # all game state\n",
    "    \n",
    "    # Create a directory for the current strarified samples\n",
    "    \n",
    "    current_batch_dir = pkls_dir / str(iteration)\n",
    "    current_batch_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # print(all_scenarios)\n",
    "\n",
    "    # print(current_batch_dir)\n",
    "\n",
    "    for scenario in all_scenarios:\n",
    "        try:\n",
    "            game_states = [scenario['data']]\n",
    "            difficulty = scenario['difficulty']\n",
    "            outcome = scenario['outcome']\n",
    "            initial_state = scenario['initial_state']  # This should be the correct scope\n",
    "            file_path = current_batch_dir / f\"{word}_from_{initial_state}_{difficulty}_{outcome}.pkl\"\n",
    "\n",
    "            # print(f\"Saving scenario for {word}: {file_path}\")\n",
    "\n",
    "            with open(file_path, 'wb') as file:\n",
    "                pickle.dump(game_states, file)\n",
    "\n",
    "            # print(f\"Saved {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving {file_path}: {e}\")\n",
    "\n",
    "    # Clear memory\n",
    "    del all_scenarios # , sampled_scenarios\n",
    "\n",
    "    # Manual garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    train_words = [word for word in train_words if word not in sampled_words]\n",
    "    # print(len(train_words))\n",
    "    # print(iteration)\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset_dir = Path('dataset/pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_list = []\n",
    "\n",
    "# Iterate over all batch directories\n",
    "for batch_dir in sorted(base_dataset_dir.iterdir(), key=lambda x: int(x.name) \\\n",
    "    if x.name.isdigit() else float('inf')):\n",
    "    if batch_dir.is_dir():\n",
    "        # List all .pkl files in the current batch directory\n",
    "        pkl_files = list(batch_dir.glob(\"*.pkl\"))\n",
    "\n",
    "        for pkl_file in pkl_files:\n",
    "            with open(pkl_file, 'rb') as file:\n",
    "                game_data = pickle.load(file)\n",
    "                # Extract information from file name\n",
    "                parts = pkl_file.stem.split('_from_')\n",
    "                word_and_state = parts[0].split('_')\n",
    "                word = '_'.join(word_and_state[:-1])\n",
    "                initial_state = word_and_state[-1]\n",
    "                difficulty, outcome = parts[1].split('_')[-2:]\n",
    "\n",
    "                # Assuming game_data is a list of tuples (game_won, guesses)\n",
    "                for data in game_data:\n",
    "                    game_won, guesses = data\n",
    "                    # Create a scenario dictionary for each data tuple\n",
    "                    scenario = {\n",
    "                        'word': word,\n",
    "                        'difficulty': difficulty,\n",
    "                        'outcome': outcome,\n",
    "                        'data': (game_won, guesses)\n",
    "                    }\n",
    "                    pkl_list.append((pkl_file, scenario))  # Add scenario to the list\n",
    "\n",
    "# Accessing an individual pickle file's content by index\n",
    "index_to_access = 0  # Change this index to access different files\n",
    "if index_to_access < len(pkl_list):\n",
    "    file_path, scenario = pkl_list[index_to_access]\n",
    "    print(f\"Contents of {file_path}:\")\n",
    "    print_scenarios([scenario])  # Wrap scenario in a list for the function\n",
    "else:\n",
    "    print(f\"No pickle file at index {index_to_access}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
