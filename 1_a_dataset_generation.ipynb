{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "import sys\n",
    "# Custom library paths\n",
    "sys.path.extend(['../', './scr'])\n",
    "\n",
    "from scr.utils import set_seed\n",
    "from scr.utils import read_words\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Read and Shuffle Word List\n",
    "word_list = read_words('data/words_250000_train.txt') # , limit=10000)\n",
    "# word_list = read_words('data/250k.txt', limit=10000)\n",
    "random.shuffle(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Reading and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from scr.feature_engineering import \\\n",
    "    calculate_char_frequencies, calculate_word_frequencies\n",
    "from scr.utils import read_words, save_words_to_file\n",
    "from scr.dataset import HangmanDataset\n",
    "\n",
    "# Constants and File Paths\n",
    "MASK_PROB = 0.8\n",
    "NGRAM_N = 3\n",
    "# NUM_STRATIFIED_SAMPLES = 10\n",
    "BATCH_SIZE = 64  # Example batch size, adjust as needed\n",
    "# base_dataset_dir = Path('data/20k/')\n",
    "\n",
    "# pkls_dir = pkls_dir\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the base directory where you want to save the dataset\n",
    "base_dataset_dir = Path('./dataset/')\n",
    "# Ensure the base directory exists\n",
    "base_dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "pkls_dir = base_dataset_dir / 'pkl'\n",
    "pkls_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Splitting Dataset Function\n",
    "def split_dataset(word_list, train_ratio=0.7, val_ratio=0.15):\n",
    "    total_words = len(word_list)\n",
    "    train_size = int(total_words * train_ratio)\n",
    "    val_size = int(total_words * val_ratio)\n",
    "    random.shuffle(word_list)\n",
    "    return word_list[:train_size], word_list[train_size:train_size + val_size], \\\n",
    "        word_list[train_size + val_size:]\n",
    "\n",
    "\n",
    "# Splitting the word list\n",
    "train_words, val_words, test_words = split_dataset(word_list)\n",
    "\n",
    "# Save split datasets to files\n",
    "save_words_to_file(train_words, base_dataset_dir / 'train_words.txt')\n",
    "save_words_to_file(val_words, base_dataset_dir / 'val_words.txt')\n",
    "save_words_to_file(test_words, base_dataset_dir / 'test_words.txt')\n",
    "\n",
    "# Calculate Frequencies and Max Word Length\n",
    "word_frequencies = calculate_word_frequencies(train_words)\n",
    "char_frequency = calculate_char_frequencies(train_words)\n",
    "max_word_length = max(len(word) for word in train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.game import simulate_game_progress, play_game_with_a_word, process_word\n",
    "\n",
    "# Example word and initial state\n",
    "# Example usage\n",
    "word = \"mississippi\"\n",
    "# word = \"cat\"\n",
    "initial_states = process_word(word, mask_prob=0.5, max_variants=15)\n",
    "\n",
    "# Print generated initial states\n",
    "# print(\"Generated Initial States:\")\n",
    "# for initial_state in initial_states:\n",
    "#     # Simulate the game\n",
    "#     print(initial_state)\n",
    "    # print(f\"For initial state: {initial_state}\")\n",
    "    # won, game_progress = simulate_game_progress(\n",
    "    #     model=None,  # Assuming model is not used in this example\n",
    "    #     word=word, \n",
    "    #     initial_state=initial_state, \n",
    "    #     char_frequency={},  # Assuming char_frequency is not used in this example\n",
    "    #     max_word_length=len(word), \n",
    "    #     device=None,  # Assuming device is not used in this example\n",
    "    #     max_attempts=6, \n",
    "    #     normalize=True,\n",
    "    #     difficulty=\"medium\", \n",
    "    #     outcome_preference='win'\n",
    "    # )\n",
    "\n",
    "    # # Display game progress\n",
    "    # for step in game_progress:\n",
    "    #     print(f\"Guessed: '{step[0]}', New State: '{step[1]}', Correct: {step[2]}\")\n",
    "\n",
    "    #     # break\n",
    "\n",
    "    # # break\n",
    "\n",
    "    # # print(\"Game Result:\", \"Won\" if won else \"Lost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['___________',\n",
       " 'm_ss_ss____',\n",
       " '________pp_',\n",
       " '_ississi__i',\n",
       " '__ss_ss_pp_',\n",
       " 'mi__i__i__i',\n",
       " '_i__i__ippi',\n",
       " 'm_______pp_',\n",
       " 'm__________',\n",
       " '__ss_ss____',\n",
       " '_i__i__i__i']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STOP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sayem/Desktop/Hangman/1_a_dataset_generation.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/1_a_dataset_generation.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m STOP\n",
      "\u001b[0;31mNameError\u001b[0m: name 'STOP' is not defined"
     ]
    }
   ],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from scr.dataset import *\n",
    "from scr.game import *\n",
    "import gc\n",
    "from scr.utils import print_scenarios\n",
    "\n",
    "pkls_dir = pkls_dir\n",
    "\n",
    "# base_dataset_dir = Path('dataset/250k/')\n",
    "\n",
    "# pkls_dir = base_dataset_dir / 'pkl'\n",
    "# base_dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "# pkls_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import random\n",
    "\n",
    "def sample_scenarios(scenarios, base_sample_size, \\\n",
    "    max_samples_per_length=15, always_include_masked_state=None):\n",
    "    sampled = []\n",
    "    word_length_categories = set([len(s['word']) for s in scenarios])\n",
    "\n",
    "    for length in word_length_categories:\n",
    "        length_scenarios = [s for s in scenarios if len(s['word']) == length]\n",
    "        total_samples_for_length = 0\n",
    "\n",
    "        # Always include the fully masked state scenario if provided\n",
    "        if always_include_masked_state:\n",
    "            masked_state_scenarios = [s for s in length_scenarios \\\n",
    "                if s['initial_state'] == always_include_masked_state]\n",
    "\n",
    "            for scenario in masked_state_scenarios:\n",
    "                sampled.append(scenario)\n",
    "                total_samples_for_length += 1\n",
    "\n",
    "        # Continue with other categories\n",
    "        for category in [\"easy_win\", \"easy_lose\", \"medium_win\", \"medium_lose\", \\\n",
    "            \"hard_win\", \"hard_lose\"]:\n",
    "            cat_scenarios = [s for s in length_scenarios if s['difficulty'] \\\n",
    "                == category.split('_')[0] and s['outcome'] == category.split('_')[1]]\n",
    "\n",
    "            available_samples = max_samples_per_length - total_samples_for_length\n",
    "            if available_samples <= 0:\n",
    "                break\n",
    "\n",
    "            sample_size = min(len(cat_scenarios), base_sample_size, available_samples)\n",
    "            sampled.extend(random.sample(cat_scenarios, sample_size))\n",
    "            total_samples_for_length += sample_size\n",
    "\n",
    "    # # Debug: Check for inclusion of fully masked state scenarios in the final sample\n",
    "    # for scenario in sampled:\n",
    "    #     initial_state = scenario.get('initial_state')\n",
    "    #     if initial_state == always_include_masked_state:\n",
    "    #         print(f\"Debug: Fully masked state scenario included for word '{scenario['word']}'\")\n",
    "    #     elif initial_state is not None:\n",
    "    #         print(f\"Debug: Other initial state scenario for word '{scenario['word']}'\")\n",
    "    #     else:\n",
    "    #         print(f\"Debug: No initial state provided for word '{scenario['word']}'\")\n",
    "\n",
    "    return sampled\n",
    "    \n",
    "# Function to print scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.custom_sampler import stratified_sample_by_length_and_frequency\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "# train_words = ['cat']\n",
    "\n",
    "print(f'train words len: {len(train_words)}')\n",
    "\n",
    "NUM_STRATIFIED_SAMPLES = 1000\n",
    "# Main loop\n",
    "iteration = 0\n",
    "base_sample_size = 10  # Base number of samples per difficulty-outcome category\n",
    "\n",
    "# train_words = ['cat']\n",
    "\n",
    "\n",
    "while tqdm(train_words, miniters=1, leave=False, mininterval=2.0):\n",
    "\n",
    "    sampled_words = stratified_sample_by_length_and_frequency(train_words, \\\n",
    "        word_frequencies, \\\n",
    "        NUM_STRATIFIED_SAMPLES)\n",
    "\n",
    "\n",
    "    for word in tqdm(sampled_words, miniters=2, leave=False, mininterval=2.0): \n",
    "        # , miniters=2, leave=False, mininterval=2.0):\n",
    "        # print(word)\n",
    "        all_scenarios = []\n",
    "        # Process the word to get initial masked states\n",
    "        initial_masked_states = process_word(word, mask_prob=0.9, max_variants=10)\n",
    "\n",
    "        for initial_state in initial_masked_states:\n",
    "            \n",
    "            difficulties = [\"easy\", \"medium\", \"hard\"]\n",
    "            outcomes = [\"win\", \"lose\"]\n",
    "\n",
    "            for difficulty in difficulties:\n",
    "                for outcome in outcomes:\n",
    "                    # print(f'{word} from initial state: {initial_state}: \\\n",
    "                    # Difficulty: {difficulty}, Outcome: {outcome}')\n",
    "                    won, game_progress = simulate_game_progress(\n",
    "                                            model=None, \n",
    "                                            word=word, \n",
    "                                            initial_state=initial_state, \n",
    "                                            char_frequency=char_frequency, \n",
    "                                            max_word_length=max_word_length, \n",
    "                                            device=device, \n",
    "                                            max_attempts=6, \n",
    "                                            normalize=True, \n",
    "                                            difficulty=difficulty, \n",
    "                                            outcome_preference=outcome\n",
    "                                        )\n",
    "\n",
    "                    # all_scenarios.append({'word': word, 'difficulty': difficulty, \\\n",
    "                    #     'outcome': outcome, 'data': (won, game_progress)})\n",
    "\n",
    "                    all_scenarios.append({\n",
    "                                'word': word, \n",
    "                                'difficulty': difficulty,\n",
    "                                'outcome': outcome, \n",
    "                                'initial_state': initial_state,  # Added 'initial_state' key\n",
    "                                'data': (won, game_progress)\n",
    "                            })\n",
    "\n",
    "    # # # Sample scenarios\n",
    "    sampled_scenarios = sample_scenarios(all_scenarios, base_sample_size, \\\n",
    "        always_include_masked_state=True)\n",
    "\n",
    "    # Create a directory for the current batch\n",
    "    current_batch_dir = pkls_dir / str(iteration)\n",
    "    current_batch_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # print(current_batch_dir)\n",
    "\n",
    "    for scenario in sampled_scenarios:\n",
    "        try:\n",
    "            game_states = [scenario['data']]\n",
    "            difficulty = scenario['difficulty']\n",
    "            outcome = scenario['outcome']\n",
    "            initial_state = scenario['initial_state']  # This should be the correct scope\n",
    "            file_path = current_batch_dir / f\"{word}_from_{initial_state}_{difficulty}_{outcome}.pkl\"\n",
    "\n",
    "            # print(f\"Saving scenario for {word}: {file_path}\")\n",
    "\n",
    "            with open(file_path, 'wb') as file:\n",
    "                pickle.dump(game_states, file)\n",
    "\n",
    "            # print(f\"Saved {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving {file_path}: {e}\")\n",
    "\n",
    "    # Clear memory\n",
    "    del all_scenarios, sampled_scenarios\n",
    "\n",
    "    # Manual garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    train_words = [word for word in train_words if word not in sampled_words]\n",
    "    # print(len(train_words))\n",
    "    # print(iteration)\n",
    "    iteration += 1\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset_dir = Path('dataset/pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_list = []\n",
    "\n",
    "# Iterate over all batch directories\n",
    "for batch_dir in sorted(base_dataset_dir.iterdir(), key=lambda x: int(x.name) if x.name.isdigit() else float('inf')):\n",
    "    if batch_dir.is_dir():\n",
    "        # List all .pkl files in the current batch directory\n",
    "        pkl_files = list(batch_dir.glob(\"*.pkl\"))\n",
    "\n",
    "        for pkl_file in pkl_files:\n",
    "            with open(pkl_file, 'rb') as file:\n",
    "                game_data = pickle.load(file)\n",
    "                # Extract information from file name\n",
    "                parts = pkl_file.stem.split('_from_')\n",
    "                word_and_state = parts[0].split('_')\n",
    "                word = '_'.join(word_and_state[:-1])\n",
    "                initial_state = word_and_state[-1]\n",
    "                difficulty, outcome = parts[1].split('_')[-2:]\n",
    "\n",
    "                # Assuming game_data is a list of tuples (game_won, guesses)\n",
    "                for data in game_data:\n",
    "                    game_won, guesses = data\n",
    "                    # Create a scenario dictionary for each data tuple\n",
    "                    scenario = {\n",
    "                        'word': word,\n",
    "                        'difficulty': difficulty,\n",
    "                        'outcome': outcome,\n",
    "                        'data': (game_won, guesses)\n",
    "                    }\n",
    "                    pkl_list.append((pkl_file, scenario))  # Add scenario to the list\n",
    "\n",
    "# Accessing an individual pickle file's content by index\n",
    "index_to_access = 0  # Change this index to access different files\n",
    "if index_to_access < len(pkl_list):\n",
    "    file_path, scenario = pkl_list[index_to_access]\n",
    "    print(f\"Contents of {file_path}:\")\n",
    "    print_scenarios([scenario])  # Wrap scenario in a list for the function\n",
    "else:\n",
    "    print(f\"No pickle file at index {index_to_access}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
