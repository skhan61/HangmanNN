{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from scr.dataset import *\n",
    "from scr.game import *\n",
    "import gc\n",
    "from scr.utils import print_scenarios\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from scr.feature_engineering import \\\n",
    "    calculate_char_frequencies, calculate_word_frequencies\n",
    "from scr.utils import read_words, save_words_to_file\n",
    "\n",
    "import sys\n",
    "# Custom library paths\n",
    "sys.path.extend(['../', './scr'])\n",
    "\n",
    "from scr.utils import set_seed\n",
    "from scr.utils import read_words\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Read and Shuffle Word List\n",
    "word_list = read_words('data/words_250000_train.txt') # , limit=10000)\n",
    "# word_list = read_words('data/250k.txt', limit=10000)\n",
    "random.shuffle(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "NUM_STRATIFIED_SAMPLES = 150000\n",
    "\n",
    "# Define the base directory on the specified drive\n",
    "base_dataset_dir = Path(f\"/media/sayem/510B93E12554BBD1/dataset/{NUM_STRATIFIED_SAMPLES}\")\n",
    "\n",
    "# Ensuring the directory exists\n",
    "base_dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure the base directory exists\n",
    "base_dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "pkls_dir = base_dataset_dir / 'pkl'\n",
    "pkls_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/media/sayem/510B93E12554BBD1/dataset/150000')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Dataset Function\n",
    "def split_dataset(word_list, train_ratio=0.7, val_ratio=0.15):\n",
    "    total_words = len(word_list)\n",
    "    train_size = int(total_words * train_ratio)\n",
    "    val_size = int(total_words * val_ratio)\n",
    "    random.shuffle(word_list)\n",
    "    return word_list[:train_size], word_list[train_size:train_size + val_size], \\\n",
    "        word_list[train_size + val_size:]\n",
    "\n",
    "\n",
    "# Splitting the word list\n",
    "train_words, val_words, test_words = split_dataset(word_list)\n",
    "\n",
    "# Save split datasets to files\n",
    "save_words_to_file(train_words, base_dataset_dir / 'train_words.txt')\n",
    "save_words_to_file(val_words, base_dataset_dir / 'val_words.txt')\n",
    "save_words_to_file(test_words, base_dataset_dir / 'test_words.txt')\n",
    "\n",
    "# Calculate Frequencies and Max Word Length\n",
    "word_frequencies = calculate_word_frequencies(train_words)\n",
    "char_frequency = calculate_char_frequencies(train_words)\n",
    "max_word_length = max(len(word) for word in train_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Reading and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_repetitive_characters(word):\n",
    "    char_count = {}\n",
    "    for char in word:\n",
    "        char_count[char] = char_count.get(char, 0) + 1\n",
    "    return sum(1 for count in char_count.values() if count > 1)\n",
    "\n",
    "def adaptive_mask_prob_and_variants(word):\n",
    "    word_length = len(word)\n",
    "    unique_char_count = len(set(word))\n",
    "    repetitive_char_count = count_repetitive_characters(word)\n",
    "\n",
    "    # Lower mask probability for longer and more unique words\n",
    "    if word_length <= 5:\n",
    "        mask_prob = 0.8 if repetitive_char_count > (word_length / 2) else 0.9\n",
    "    \n",
    "    elif word_length <= 8:\n",
    "        mask_prob = 0.6 if unique_char_count > 4 else 0.7\n",
    "    \n",
    "    else:\n",
    "        mask_prob = 0.4 if unique_char_count > 6 else 0.5\n",
    "\n",
    "    # Adjust max variants based on the complexity of the word\n",
    "    if unique_char_count <= 3 or repetitive_char_count > (word_length / 2):\n",
    "        max_variants = max(5, word_length)\n",
    "    \n",
    "    elif unique_char_count <= 6:\n",
    "        max_variants = max(10, word_length)\n",
    "    \n",
    "    else:\n",
    "        max_variants = min(15, word_length)\n",
    "\n",
    "    return mask_prob, max_variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"noon\"\n",
    "mask_prob, max_variants = adaptive_mask_prob_and_variants(word)\n",
    "\n",
    "mask_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Game State Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Initial States:\n",
      "____\n",
      "For initial state: ____\n",
      "Guessed: 'o', New State: '_oo_', Correct: True\n",
      "Guessed: 'n', New State: 'noon', Correct: True\n",
      "n__n\n",
      "For initial state: n__n\n",
      "Guessed: 'l', New State: 'n__n', Correct: False\n",
      "Guessed: 'o', New State: 'noon', Correct: True\n",
      "_oo_\n",
      "For initial state: _oo_\n",
      "Guessed: 'n', New State: 'noon', Correct: True\n"
     ]
    }
   ],
   "source": [
    "from scr.game import simulate_game_progress, play_game_with_a_word, process_word\n",
    "\n",
    "# Example word and initial state\n",
    "# Example usage\n",
    "# word = \"mississippi\"\n",
    "# word = \"cat\"\n",
    "mask_prob, max_variants = adaptive_mask_prob_and_variants(word)\n",
    "\n",
    "initial_states = process_word(word, mask_prob=mask_prob, \\\n",
    "    max_variants=max_variants)\n",
    "\n",
    "# Print generated initial states\n",
    "print(\"Generated Initial States:\")\n",
    "for initial_state in initial_states:\n",
    "    # Simulate the game\n",
    "    print(initial_state)\n",
    "    print(f\"For initial state: {initial_state}\")\n",
    "    won, game_progress = simulate_game_progress(\n",
    "        model=None,  # Assuming model is not used in this example\n",
    "        word=word, \n",
    "        initial_state=initial_state, \n",
    "        char_frequency={},  # Assuming char_frequency is not used in this example\n",
    "        max_word_length=len(word), \n",
    "        device=None,  # Assuming device is not used in this example\n",
    "        max_attempts=6, \n",
    "        normalize=True,\n",
    "        difficulty=\"medium\", \n",
    "        outcome_preference='win'\n",
    "    )\n",
    "\n",
    "    # Display game progress\n",
    "    for step in game_progress:\n",
    "        print(f\"Guessed: '{step[0]}', New State: '{step[1]}', Correct: {step[2]}\")\n",
    "\n",
    "        # break\n",
    "\n",
    "    # break\n",
    "\n",
    "    # print(\"Game Result:\", \"Won\" if won else \"Lost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['____', 'n__n', '_oo_']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number words for state generation: 150001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150001 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving /media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from_a________a______easy_lose.pkl: [Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from_a________a______easy_lose.pkl'\n",
      "Error saving /media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from_a________a______medium_win.pkl: [Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from_a________a______medium_win.pkl'\n",
      "Error saving /media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from_a________a______medium_lose.pkl: [Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from_a________a______medium_lose.pkl'\n",
      "Error saving /media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from_a________a______hard_win.pkl: [Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from_a________a______hard_win.pkl'\n",
      "Error saving /media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from_a________a______hard_lose.pkl: [Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from_a________a______hard_lose.pkl'\n",
      "Error saving /media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from___m____t__t_____easy_win.pkl: [Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from___m____t__t_____easy_win.pkl'\n",
      "Error saving /media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from___m____t__t_____easy_lose.pkl: [Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from___m____t__t_____easy_lose.pkl'\n",
      "Error saving /media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from___m____t__t_____medium_win.pkl: [Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from___m____t__t_____medium_win.pkl'\n",
      "Error saving /media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from___m____t__t_____medium_lose.pkl: [Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from___m____t__t_____medium_lose.pkl'\n",
      "Error saving /media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from___m____t__t_____hard_win.pkl: [Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from___m____t__t_____hard_win.pkl'\n",
      "Error saving /media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from___m____t__t_____hard_lose.pkl: [Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from___m____t__t_____hard_lose.pkl'\n",
      "Error saving /media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from__d______r__re___easy_win.pkl: [Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from__d______r__re___easy_win.pkl'\n",
      "Error saving /media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from__d______r__re___easy_lose.pkl: [Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from__d______r__re___easy_lose.pkl'\n",
      "Error saving /media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from__d______r__re___medium_win.pkl: [Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from__d______r__re___medium_win.pkl'\n",
      "Error saving /media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from__d______r__re___medium_lose.pkl: [Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from__d______r__re___medium_lose.pkl'\n",
      "Error saving /media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from__d______r__re___hard_win.pkl: [Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from__d______r__re___hard_win.pkl'\n",
      "Error saving /media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from__d______r__re___hard_lose.pkl: [Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144837/administratress_from__d______r__re___hard_lose.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144838'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/sayem/Desktop/Hangman/1_a_dataset_generation.ipynb Cell 15\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/1_a_dataset_generation.ipynb#X20sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# Create a directory for the current strarified samples\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/1_a_dataset_generation.ipynb#X20sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m current_batch_dir \u001b[39m=\u001b[39m pkls_dir \u001b[39m/\u001b[39m \u001b[39mstr\u001b[39m(iteration)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/1_a_dataset_generation.ipynb#X20sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m current_batch_dir\u001b[39m.\u001b[39;49mmkdir(parents\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/1_a_dataset_generation.ipynb#X20sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# print(all_scenarios)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/1_a_dataset_generation.ipynb#X20sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/1_a_dataset_generation.ipynb#X20sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# print(current_batch_dir)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/1_a_dataset_generation.ipynb#X20sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mfor\u001b[39;00m scenario \u001b[39min\u001b[39;00m all_scenarios:\n",
      "File \u001b[0;32m~/anaconda3/envs/optiver/lib/python3.10/pathlib.py:1175\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[39mCreate a new directory at this given path.\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1175\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor\u001b[39m.\u001b[39;49mmkdir(\u001b[39mself\u001b[39;49m, mode)\n\u001b[1;32m   1176\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m   1177\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parents \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m:\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '/media/sayem/510B93E12554BBD1/dataset/150000/pkl/144838'"
     ]
    }
   ],
   "source": [
    "from scr.custom_sampler import stratified_sample_by_length_and_frequency\n",
    "\n",
    "# NUM_STRATIFIED_SAMPLES = 1000\n",
    "# Main loop\n",
    "iteration = 0\n",
    "\n",
    "sampled_words = stratified_sample_by_length_and_frequency(train_words, \\\n",
    "    word_frequencies, \\\n",
    "    NUM_STRATIFIED_SAMPLES)\n",
    "\n",
    "print(f'Number words for state generation: {len(sampled_words)}')\n",
    "\n",
    "for word in tqdm(sampled_words, miniters=2, leave=False, mininterval=2.0): \n",
    "    # , miniters=2, leave=False, mininterval=2.0):\n",
    "    # print(word)\n",
    "    all_scenarios = []\n",
    "    # Process the word to get initial masked states\n",
    "    # initial_masked_states = process_word(word, mask_prob=0.9, max_variants=10)\n",
    "\n",
    "    mask_prob, max_variants = adaptive_mask_prob_and_variants(word)\n",
    "\n",
    "    game_states = process_word(word, mask_prob=mask_prob, \\\n",
    "        max_variants=max_variants)\n",
    "\n",
    "    for initial_state in game_states:\n",
    "        \n",
    "        difficulties = [\"easy\", \"medium\", \"hard\"]\n",
    "        outcomes = [\"win\", \"lose\"]\n",
    "\n",
    "        for difficulty in difficulties:\n",
    "            for outcome in outcomes:\n",
    "                # print(f'{word} from initial state: {initial_state}: \\\n",
    "                # Difficulty: {difficulty}, Outcome: {outcome}')\n",
    "                won, game_progress = simulate_game_progress(\n",
    "                                        model=None, \n",
    "                                        word=word, \n",
    "                                        initial_state=initial_state, \n",
    "                                        char_frequency=char_frequency, \n",
    "                                        max_word_length=max_word_length, \n",
    "                                        device=device, \n",
    "                                        max_attempts=6, \n",
    "                                        normalize=True, \n",
    "                                        difficulty=difficulty, \n",
    "                                        outcome_preference=outcome\n",
    "                                    )\n",
    "\n",
    "                # all_scenarios.append({'word': word, 'difficulty': difficulty, \\\n",
    "                #     'outcome': outcome, 'data': (won, game_progress)})\n",
    "\n",
    "                all_scenarios.append({\n",
    "                            'word': word, \n",
    "                            'difficulty': difficulty,\n",
    "                            'outcome': outcome, \n",
    "                            'initial_state': initial_state,  # Added 'initial_state' key\n",
    "                            'data': (won, game_progress)\n",
    "                        })  # all game state\n",
    "    \n",
    "    # Create a directory for the current strarified samples\n",
    "    \n",
    "    current_batch_dir = pkls_dir / str(iteration)\n",
    "    current_batch_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # print(all_scenarios)\n",
    "\n",
    "    # print(current_batch_dir)\n",
    "\n",
    "    for scenario in all_scenarios:\n",
    "        try:\n",
    "            game_states = [scenario['data']]\n",
    "            difficulty = scenario['difficulty']\n",
    "            outcome = scenario['outcome']\n",
    "            initial_state = scenario['initial_state']  # This should be the correct scope\n",
    "            file_path = current_batch_dir / f\"{word}_from_{initial_state}_{difficulty}_{outcome}.pkl\"\n",
    "\n",
    "            # print(f\"Saving scenario for {word}: {file_path}\")\n",
    "\n",
    "            with open(file_path, 'wb') as file:\n",
    "                pickle.dump(game_states, file)\n",
    "\n",
    "            # print(f\"Saved {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving {file_path}: {e}\")\n",
    "\n",
    "    # Clear memory\n",
    "    del all_scenarios # , sampled_scenarios\n",
    "\n",
    "    # Manual garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    # train_words = [word for word in train_words if word not in sampled_words]\n",
    "    # print(len(train_words))\n",
    "    # print(iteration)\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_list = []\n",
    "\n",
    "for batch_dir in sorted(pkls_dir.iterdir(), key=lambda x: int(x.name) if x.name.isdigit() else float('inf')):\n",
    "    if batch_dir.is_dir():\n",
    "        pkl_files = list(batch_dir.glob(\"*.pkl\"))\n",
    "\n",
    "        for pkl_file in pkl_files:\n",
    "            with open(pkl_file, 'rb') as file:\n",
    "                game_data = pickle.load(file)\n",
    "\n",
    "                # Split the file name to extract word and initial state\n",
    "                parts = pkl_file.stem.split('_from_')\n",
    "                word = parts[0]  # The word is before '_from_'\n",
    "                remaining_parts = parts[1].split('_')\n",
    "                initial_state = remaining_parts[0]  # The initial state is right after '_from_'\n",
    "                difficulty, outcome = remaining_parts[1], remaining_parts[2]\n",
    "\n",
    "                for data in game_data:\n",
    "                    game_won, guesses = data\n",
    "                    scenario = {\n",
    "                        'word': word,\n",
    "                        'initial_state': initial_state,\n",
    "                        'difficulty': difficulty,\n",
    "                        'outcome': outcome,\n",
    "                        'data': (game_won, guesses)\n",
    "                    }\n",
    "                    pkl_list.append((pkl_file, scenario))\n",
    "\n",
    "index_to_access = 1000\n",
    "if index_to_access < len(pkl_list):\n",
    "    file_path, scenario = pkl_list[index_to_access]\n",
    "    print(f\"Contents of {file_path}:\")\n",
    "    print_scenarios([scenario])\n",
    "else:\n",
    "    print(f\"No pickle file at index {index_to_access}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl_list = []\n",
    "\n",
    "# # Iterate over all batch directories\n",
    "# for batch_dir in sorted(pkls_dir.iterdir(), key=lambda x: int(x.name) \\\n",
    "#     if x.name.isdigit() else float('inf')):\n",
    "#     if batch_dir.is_dir():\n",
    "#         # List all .pkl files in the current batch directory\n",
    "#         pkl_files = list(batch_dir.glob(\"*.pkl\"))\n",
    "\n",
    "#         for pkl_file in pkl_files:\n",
    "#             with open(pkl_file, 'rb') as file:\n",
    "#                 game_data = pickle.load(file)\n",
    "#                 # Extract information from file name\n",
    "#                 parts = pkl_file.stem.split('_from_')\n",
    "#                 word_and_state = parts[0].split('_')\n",
    "#                 word = '_'.join(word_and_state[:-1])\n",
    "#                 initial_state = word_and_state[-1]\n",
    "#                 difficulty, outcome = parts[1].split('_')[-2:]\n",
    "\n",
    "#                 # Assuming game_data is a list of tuples (game_won, guesses)\n",
    "#                 for data in game_data:\n",
    "#                     game_won, guesses = data\n",
    "#                     # Create a scenario dictionary for each data tuple\n",
    "#                     scenario = {\n",
    "#                         'word': word,\n",
    "#                         'difficulty': difficulty,\n",
    "#                         'outcome': outcome,\n",
    "#                         'data': (game_won, guesses)\n",
    "#                     }\n",
    "#                     pkl_list.append((pkl_file, scenario))  # Add scenario to the list\n",
    "\n",
    "# # Accessing an individual pickle file's content by index\n",
    "# index_to_access = 1000  # Change this index to access different files\n",
    "# if index_to_access < len(pkl_list):\n",
    "#     file_path, scenario = pkl_list[index_to_access]\n",
    "#     print(f\"Contents of {file_path}:\")\n",
    "#     print_scenarios([scenario])  # Wrap scenario in a list for the function\n",
    "# else:\n",
    "#     print(f\"No pickle file at index {index_to_access}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
