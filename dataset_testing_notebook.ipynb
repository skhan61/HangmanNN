{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "import sys\n",
    "# Custom library paths\n",
    "sys.path.extend(['../', './scr'])\n",
    "\n",
    "from scr.utils import set_seed\n",
    "from scr.utils import read_words\n",
    "from pathlib import Path\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from scr.feature_engineering import \\\n",
    "    calculate_char_frequencies, calculate_word_frequencies\n",
    "from scr.utils import read_words, save_words_to_file\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from scr.dataset import *\n",
    "from scr.game import *\n",
    "import gc\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from scr.utils import print_scenarios\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Read and Shuffle Word List\n",
    "word_list = read_words('data/words_250000_train.txt') # , limit=10000)\n",
    "# word_list = read_words('data/250k.txt', limit=10000)\n",
    "random.shuffle(word_list)\n",
    "\n",
    "# base_dataset_dir = Path('dataset/pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 159110 train words from dataset/2500/train_words.txt\n",
      "Loaded 34095 validation words from dataset/2500/val_words.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "NUM_STRATIFIED_SAMPLES = 2500\n",
    "\n",
    "# Define the base directory\n",
    "base_dataset_dir = Path(f'./dataset/{NUM_STRATIFIED_SAMPLES}')\n",
    "# base_dataset_dir = Path(f\"/media/sayem/510B93E12554BBD1/dataset/{NUM_STRATIFIED_SAMPLES}\")\n",
    "\n",
    "# Ensuring the base directory and 'pkl' subdirectory exist\n",
    "base_dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "pkls_dir = base_dataset_dir / 'pkl'\n",
    "pkls_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Paths to the words files\n",
    "train_words_file_path = base_dataset_dir / 'train_words.txt'\n",
    "val_words_file_path = base_dataset_dir / 'val_words.txt'\n",
    "\n",
    "# Read the words from the files\n",
    "try:\n",
    "    train_words = read_words(train_words_file_path)\n",
    "    print(f\"Loaded {len(train_words)} train words from {train_words_file_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {train_words_file_path}\")\n",
    "\n",
    "try:\n",
    "    val_words = read_words(val_words_file_path)\n",
    "    print(f\"Loaded {len(val_words)} validation words from {val_words_file_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {val_words_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('dataset/2500')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For inference\n",
    "from scr.feature_engineering import *\n",
    "\n",
    "word_frequencies = calculate_word_frequencies(word_list)\n",
    "char_frequency = calculate_char_frequencies(word_list)\n",
    "max_word_length = max(len(word) for word in word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.simple_model import SimpleLSTM\n",
    "from scr.base_model import BaseModel\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from scr.feature_engineering import *\n",
    "\n",
    "# Instantiate and test the model\n",
    "config = {\n",
    "    'embedding_dim': 200,\n",
    "    'hidden_dim': 256,\n",
    "    'num_layers': 2,\n",
    "    'vocab_size': 27,\n",
    "    'max_word_length': max_word_length,\n",
    "    'input_feature_size': 5,\n",
    "    'use_embedding': True,\n",
    "    'miss_linear_dim': 50\n",
    "}\n",
    "\n",
    "model = SimpleLSTM(config)\n",
    "optimizer = model.optimizer\n",
    "\n",
    "# Assuming 'model' is your trained model instance\n",
    "model.save_model(file_path='models/model.pth')\n",
    "\n",
    "# Assuming the saved model file is 'models/model.pth'\n",
    "model_file_path = 'models/model.pth'\n",
    "\n",
    "# Specify the device to load the model onto\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = BaseModel.load_model(SimpleLSTM, model_file_path)\n",
    "# Now `loaded_model` is an instance of `SimpleLSTM` with the state and config loaded\n",
    "\n",
    "model = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.dataset import ProcessedHangmanDataset\n",
    "\n",
    "# # Load the dataset\n",
    "processed_dataset = ProcessedHangmanDataset(pkls_dir, \\\n",
    "    char_frequency, max_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Convert PyTorch dataset to a list for train_test_split\n",
    "dataset_list = [processed_dataset[i] for i in range(len(processed_dataset))]\n",
    "\n",
    "# Perform an 80%-20% train-test split\n",
    "train_data, val_data = train_test_split(dataset_list, \\\n",
    "    test_size=0.20, random_state=42)\n",
    "\n",
    "del dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scr.model_training import train_on_data_loader\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, \\\n",
    "    collate_fn=processed_dataset.custom_collate_fn)\n",
    "\n",
    "optimizer = model.optimizer\n",
    "\n",
    "# average_loss, average_miss_penalty \\\n",
    "#     = train_on_data_loader(model, train_loader, device, optimizer)\n",
    "\n",
    "\n",
    "# # Define a probability threshold for inspecting a batch\n",
    "# inspect_probability = 0.01  # Adjust this as needed\n",
    "\n",
    "# for batch in train_loader:\n",
    "#     if batch[0] is None:\n",
    "#         continue  # Skip empty batches\n",
    "\n",
    "#     # Randomly decide whether to inspect this batch\n",
    "#     if random.random() < inspect_probability:\n",
    "#         game_states_batch, lengths_batch, missed_chars_batch, labels_batch = batch\n",
    "#         average_loss, average_miss_penalty = train_on_data_loader(model, )\n",
    "#         print(\"Inspected Batch Shapes:\")\n",
    "#         print(\"Game States Batch Shape:\", game_states_batch.shape)\n",
    "#         print(\"Lengths Batch Shape:\", lengths_batch.shape)\n",
    "#         print(\"Missed Chars Batch Shape:\", missed_chars_batch.shape)\n",
    "#         print(\"Labels Batch Shape:\", labels_batch.shape)\n",
    "#         break\n",
    "\n",
    "# average_loss, average_miss_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweeteners\n",
      "masked word:  ['sweeteners']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'torch.device' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/sayem/Desktop/Hangman/dataset_testing_notebook.ipynb Cell 10\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/dataset_testing_notebook.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscr\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_training\u001b[39;00m \u001b[39mimport\u001b[39;00m validate_hangman\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/dataset_testing_notebook.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m val_loader \u001b[39m=\u001b[39m processed_dataset\u001b[39m.\u001b[39mcreate_val_loader(val_data)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/dataset_testing_notebook.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m character_level, game_simulation  \u001b[39m=\u001b[39m validate_hangman(model, \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/dataset_testing_notebook.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     val_loader, device, max_word_length, device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/dataset_testing_notebook.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# avg_loss, avg_miss_penalty, \\\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/dataset_testing_notebook.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#     char_accuracy, word_accuracy, word_statistics\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/dataset_testing_notebook.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m character_level, game_simulation\n",
      "File \u001b[0;32m~/Desktop/Hangman/scr/model_training.py:148\u001b[0m, in \u001b[0;36mvalidate_hangman\u001b[0;34m(model, data_loader, char_frequency, max_word_length, device, max_attempts, normalize)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mfor\u001b[39;00m full_word \u001b[39min\u001b[39;00m batch_full_words:\n\u001b[1;32m    147\u001b[0m     \u001b[39mprint\u001b[39m(full_word)\n\u001b[0;32m--> 148\u001b[0m     won, final_word, attempts \u001b[39m=\u001b[39m play_game_with_a_word(model, full_word,\n\u001b[1;32m    149\u001b[0m                 char_frequency, max_word_length, device, max_attempts, normalize)\n\u001b[1;32m    151\u001b[0m     \u001b[39m# Update word-specific stats\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     word_stats[full_word] \u001b[39m=\u001b[39m {\n\u001b[1;32m    153\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwon\u001b[39m\u001b[39m\"\u001b[39m: won,\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfinal_word\u001b[39m\u001b[39m\"\u001b[39m: final_word,\n\u001b[1;32m    155\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mattempts_used\u001b[39m\u001b[39m\"\u001b[39m: attempts\n\u001b[1;32m    156\u001b[0m     }\n",
      "File \u001b[0;32m~/Desktop/Hangman/scr/game.py:141\u001b[0m, in \u001b[0;36mplay_game_with_a_word\u001b[0;34m(model, word, char_frequency, max_word_length, device, max_attempts, normalize)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m# print(f\"Starting the game. Word to guess: {' '.join(masked_word)}\")  # Display initial state\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39mwhile\u001b[39;00m game_status \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mongoing\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m attempts_remaining \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m     guessed_char \u001b[39m=\u001b[39m guess(model, word, char_frequency,\n\u001b[1;32m    142\u001b[0m                          max_word_length, device, guessed_letters)\n\u001b[1;32m    143\u001b[0m     \u001b[39m# Add to the list of guessed letters\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     guessed_letters\u001b[39m.\u001b[39mappend(guessed_char)\n",
      "File \u001b[0;32m~/Desktop/Hangman/scr/game.py:106\u001b[0m, in \u001b[0;36mguess\u001b[0;34m(model, word, char_frequency, max_word_length, device, guessed_letters)\u001b[0m\n\u001b[1;32m    102\u001b[0m cleaned_word \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(char\u001b[39m.\u001b[39mlower()\n\u001b[1;32m    103\u001b[0m                        \u001b[39mfor\u001b[39;00m char \u001b[39min\u001b[39;00m word \u001b[39mif\u001b[39;00m char\u001b[39m.\u001b[39misalpha() \u001b[39mor\u001b[39;00m char \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[39m# Predict the next character using the updated guess_character function\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m guessed_char \u001b[39m=\u001b[39m guess_character(\n\u001b[1;32m    107\u001b[0m     model, cleaned_word,\n\u001b[1;32m    108\u001b[0m     char_frequency,\n\u001b[1;32m    109\u001b[0m     max_word_length,\n\u001b[1;32m    110\u001b[0m     device,\n\u001b[1;32m    111\u001b[0m     guessed_letters  \u001b[39m# Pass the list of guessed letters\u001b[39;49;00m\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[39m# guessed_char =  get_random_character()\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \n\u001b[1;32m    116\u001b[0m \u001b[39m# Add the new guess to the guessed letters list\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m guessed_char \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m guessed_letters:\n",
      "File \u001b[0;32m~/Desktop/Hangman/scr/game.py:51\u001b[0m, in \u001b[0;36mguess_character\u001b[0;34m(model, masked_word, char_frequency, max_word_length, device, guessed_chars, max_seq_length, fallback_strategy)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39mGuess the next character in the hangman game.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m    str: The character guessed by the model or the fallback strategy.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmasked word: \u001b[39m\u001b[39m'\u001b[39m, [masked_word])\n\u001b[1;32m     50\u001b[0m batch_features, batch_missed_characters \\\n\u001b[0;32m---> 51\u001b[0m     \u001b[39m=\u001b[39m process_batch_of_games([masked_word],\n\u001b[1;32m     52\u001b[0m                              char_frequency, max_word_length, max_seq_length)\n\u001b[1;32m     54\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     56\u001b[0m sequence_lengths \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([max_seq_length]\n\u001b[1;32m     57\u001b[0m                                 \u001b[39m*\u001b[39m batch_size, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/Desktop/Hangman/scr/feature_engineering.py:113\u001b[0m, in \u001b[0;36mprocess_batch_of_games\u001b[0;34m(batch_of_games, char_frequency, max_word_length, max_seq_length)\u001b[0m\n\u001b[1;32m    110\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch_of_games)\n\u001b[1;32m    112\u001b[0m \u001b[39m# Check the number of features using the first state of the first game\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m sample_features \u001b[39m=\u001b[39m build_feature_set(\n\u001b[1;32m    114\u001b[0m     batch_of_games[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m], char_frequency, max_word_length)\n\u001b[1;32m    116\u001b[0m \u001b[39m# Assuming the last dimension holds the features\u001b[39;00m\n\u001b[1;32m    117\u001b[0m num_features \u001b[39m=\u001b[39m sample_features\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Hangman/scr/feature_engineering.py:143\u001b[0m, in \u001b[0;36mbuild_feature_set\u001b[0;34m(word, char_frequency, max_word_length, ngram_n, normalize)\u001b[0m\n\u001b[1;32m    141\u001b[0m word_length_feature \u001b[39m=\u001b[39m [word_len \u001b[39m/\u001b[39m max_word_length] \u001b[39m*\u001b[39m word_len\n\u001b[1;32m    142\u001b[0m positional_feature \u001b[39m=\u001b[39m [pos \u001b[39m/\u001b[39m max_word_length \u001b[39mfor\u001b[39;00m pos \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(word_len)]\n\u001b[0;32m--> 143\u001b[0m frequency_feature \u001b[39m=\u001b[39m [char_frequency\u001b[39m.\u001b[39mget(idx_to_char\u001b[39m.\u001b[39mget(\n\u001b[1;32m    144\u001b[0m     char_idx, \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m char_idx \u001b[39min\u001b[39;00m encoded_word]\n\u001b[1;32m    146\u001b[0m \u001b[39m# N-grams feature\u001b[39;00m\n\u001b[1;32m    147\u001b[0m ngrams \u001b[39m=\u001b[39m extract_ngrams(word, ngram_n)\n",
      "File \u001b[0;32m~/Desktop/Hangman/scr/feature_engineering.py:143\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m word_length_feature \u001b[39m=\u001b[39m [word_len \u001b[39m/\u001b[39m max_word_length] \u001b[39m*\u001b[39m word_len\n\u001b[1;32m    142\u001b[0m positional_feature \u001b[39m=\u001b[39m [pos \u001b[39m/\u001b[39m max_word_length \u001b[39mfor\u001b[39;00m pos \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(word_len)]\n\u001b[0;32m--> 143\u001b[0m frequency_feature \u001b[39m=\u001b[39m [char_frequency\u001b[39m.\u001b[39;49mget(idx_to_char\u001b[39m.\u001b[39mget(\n\u001b[1;32m    144\u001b[0m     char_idx, \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m char_idx \u001b[39min\u001b[39;00m encoded_word]\n\u001b[1;32m    146\u001b[0m \u001b[39m# N-grams feature\u001b[39;00m\n\u001b[1;32m    147\u001b[0m ngrams \u001b[39m=\u001b[39m extract_ngrams(word, ngram_n)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.device' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "from scr.model_training import validate_hangman\n",
    "\n",
    "val_loader = processed_dataset.create_val_loader(val_data)\n",
    "\n",
    "\n",
    "character_level, game_simulation  = validate_hangman(model, \\\n",
    "    val_loader, device, max_word_length, device)\n",
    "\n",
    "# avg_loss, avg_miss_penalty, \\\n",
    "#     char_accuracy, word_accuracy, word_statistics\n",
    "\n",
    "character_level, game_simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_features_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_features_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_miss_penalty = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        if batch[0] is None:\n",
    "            continue  # Skip empty batches\n",
    "\n",
    "        game_states_batch, lengths_batch, missed_chars_batch, labels_batch = batch\n",
    "        game_states_batch, lengths_batch, missed_chars_batch = \\\n",
    "            game_states_batch.to(device), lengths_batch, missed_chars_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(game_states_batch, lengths_batch, missed_chars_batch)\n",
    "        model_output_shape = outputs.shape\n",
    "        reshaped_labels = pad_and_reshape_labels(labels_batch, model_output_shape).to(device)\n",
    "\n",
    "        loss, miss_penalty = model.calculate_loss(outputs, reshaped_labels, \\\n",
    "                                                  lengths_batch, missed_chars_batch, 27)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_miss_penalty += miss_penalty.item()  # Accumulate miss penalty\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_miss_penalty = total_miss_penalty / len(data_loader)\n",
    "    return avg_loss, avg_miss_penalty  # Return average loss and miss penalty\n",
    "\n",
    "\n",
    "\n",
    "def validate_epoch(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_miss_penalty = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            if batch[0] is None:\n",
    "                continue  # Skip empty batches\n",
    "\n",
    "\n",
    "            game_states_batch, lengths_batch, missed_chars_batch, labels_batch = batch\n",
    "            game_states_batch, lengths_batch, missed_chars_batch = \\\n",
    "                game_states_batch.to(device), lengths_batch, missed_chars_batch.to(device)\n",
    "\n",
    "            outputs = model(game_states_batch, lengths_batch, missed_chars_batch)\n",
    "            model_output_shape = outputs.shape\n",
    "            reshaped_labels = pad_and_reshape_labels(labels_batch, model_output_shape).to(device)\n",
    "\n",
    "            loss, miss_penalty = model.calculate_loss(outputs, reshaped_labels, \\\n",
    "                                                      lengths_batch, missed_chars_batch, 27)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            total_miss_penalty += miss_penalty.item()  # Accumulate miss penalty\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_miss_penalty = total_miss_penalty / len(data_loader)\n",
    "    return avg_loss, avg_miss_penalty  # \n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "#     val_loss = validate_epoch(model, val_loader, device)\n",
    "    \n",
    "#     print(f\"Epoch {epoch}: Training Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "#     # You can add code to save model checkpoints if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "def train_model(model, train_data, val_data, num_epochs, optimizer, scheduler, device):\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, \\\n",
    "        collate_fn=custom_collate_fn)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, \\\n",
    "        collate_fn=custom_collate_fn)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_miss_penalty = train_epoch(model, train_loader, \\\n",
    "            optimizer, device)\n",
    "        val_loss, val_miss_penalty = validate_epoch(model, \\\n",
    "            val_loader, device)\n",
    "\n",
    "        scheduler.step(val_loss)  # Adjust LR based on validation loss\n",
    "\n",
    "        print(f\"Epoch {epoch}: Training Loss: {train_loss}, Miss Penalty: \\\n",
    "            {train_miss_penalty}, Validation Loss: {val_loss}, Validation Miss Penalty: {val_miss_penalty}\")\n",
    "        # Save model checkpoints if needed\n",
    "\n",
    "def k_fold_cross_validate(model, dataset, k, num_epochs, optimizer, scheduler_class, device):\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f\"Fold {fold}\")\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        # Initialize a new model for each fold\n",
    "        model = SimpleLSTM(config)\n",
    "        model.to(device)\n",
    "        optimizer = model.optimizer\n",
    "        scheduler = scheduler_class(optimizer)\n",
    "\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        train_model(model, train_subset, val_subset, num_epochs, optimizer, scheduler, device)\n",
    "\n",
    "\n",
    "        train_model(model, train_subset, val_subset, num_epochs, optimizer, scheduler, device)\n",
    "\n",
    "# Usage example\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "scheduler_class = torch.optim.lr_scheduler.ReduceLROnPlateau  # Example scheduler class\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "k_fold_cross_validate(model, processed_dataset, \\\n",
    "    5, num_epochs, optimizer, scheduler_class, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.dataset import ProcessedHangmanDataset\n",
    "from scr.custom_sampler import PerformanceBasedSampler\n",
    "\n",
    "\n",
    "print(f\"Total number of data points in the dataset: {len(processed_dataset)}\")\n",
    "\n",
    "# Initialize the sampler\n",
    "sampler = PerformanceBasedSampler(\n",
    "    processed_dataset, \n",
    "    performance_metrics,\n",
    "    max_word_length=max_word_length\n",
    ")\n",
    "\n",
    "print(f\"Sampler created with {len(sampler)} indices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = list(sampler)[:10]  # Get the first 10 sampled indices\n",
    "print(\"Sampled indices:\", sample_indices)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    data_point = processed_dataset[idx]\n",
    "    print(f\"Data at index {idx}: {data_point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.utils import print_scenarios\n",
    "\n",
    "def process_pkl_files(base_dir):\n",
    "    pkl_list = []\n",
    "\n",
    "    # Iterate over all batch directories\n",
    "    for batch_dir in sorted(base_dir.iterdir(), key=lambda x: int(x.name) if x.name.isdigit() else float('inf')):\n",
    "        if batch_dir.is_dir():\n",
    "            # List all .pkl files in the current batch directory\n",
    "            pkl_files = list(batch_dir.glob(\"*.pkl\"))\n",
    "\n",
    "            for pkl_file in pkl_files:\n",
    "                try:\n",
    "                    with open(pkl_file, 'rb') as file:\n",
    "                        game_data = pickle.load(file)\n",
    "                except IOError as e:\n",
    "                    print(f\"Error reading file {pkl_file}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Processing each pickle file\n",
    "                pkl_list.extend(process_pkl_file(pkl_file, game_data))\n",
    "\n",
    "    return pkl_list\n",
    "\n",
    "def process_pkl_file(pkl_file, game_data):\n",
    "    file_scenarios = []\n",
    "    for data in game_data:\n",
    "        game_won, guesses = data\n",
    "        word, initial_state, difficulty, outcome = extract_info_from_filename(pkl_file)\n",
    "        \n",
    "        # Create a scenario dictionary for each data tuple\n",
    "        scenario = {\n",
    "            'word': word,\n",
    "            'difficulty': difficulty,\n",
    "            'outcome': outcome,\n",
    "            'data': (game_won, guesses)\n",
    "        }\n",
    "        file_scenarios.append((pkl_file, scenario))  # Add scenario to the list\n",
    "\n",
    "    return file_scenarios\n",
    "\n",
    "def extract_info_from_filename(pkl_file):\n",
    "    parts = pkl_file.stem.split('_from_')\n",
    "    word_and_state = parts[0].split('_')\n",
    "    word = '_'.join(word_and_state[:-1])\n",
    "    initial_state = word_and_state[-1]\n",
    "    difficulty, outcome = parts[1].split('_')[-2:]\n",
    "    return word, initial_state, difficulty, outcome\n",
    "\n",
    "# def print_scenarios(scenarios):\n",
    "#     # Assuming this function is defined elsewhere\n",
    "#     pass\n",
    "\n",
    "# Process all pickle files\n",
    "pkl_list = process_pkl_files(base_dataset_dir)\n",
    "\n",
    "# Accessing an individual pickle file's content by index\n",
    "index_to_access = 0  # Change this index to access different files\n",
    "if index_to_access < len(pkl_list):\n",
    "    file_path, scenario = pkl_list[index_to_access]\n",
    "    print(f\"Contents of {file_path}:\")\n",
    "    print_scenarios([scenario])  # Wrap scenario in a list for the function\n",
    "else:\n",
    "    print(f\"No pickle file at index {index_to_access}\")\n",
    "\n",
    "No pickle file at index 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_list = []\n",
    "\n",
    "# Iterate over all batch directories\n",
    "for batch_dir in sorted(base_dataset_dir.iterdir(), \\\n",
    "    key=lambda x: int(x.name) if x.name.isdigit() else float('inf')):\n",
    "    if batch_dir.is_dir():\n",
    "        # List all .pkl files in the current batch directory\n",
    "        pkl_files = list(batch_dir.glob(\"*.pkl\"))\n",
    "\n",
    "        for pkl_file in pkl_files:\n",
    "            with open(pkl_file, 'rb') as file:\n",
    "                game_data = pickle.load(file)\n",
    "                # Extract information from file name\n",
    "                parts = pkl_file.stem.split('_from_')\n",
    "                word_and_statet = parts[0].split('_')\n",
    "                word = '_'.join(word_and_state[:-1])\n",
    "                initial_state = word_and_state[-1]\n",
    "                difficulty, outcome = parts[1].split('_')[-2:]\n",
    "\n",
    "                # Assuming game_data is a list of tuples (game_won, guesses)\n",
    "                for data in game_data:\n",
    "                    game_won, guesses = data\n",
    "                    # Create a scenario dictionary for each data tuple\n",
    "                    scenario = {\n",
    "                        'word': word,\n",
    "                        'difficulty': difficulty,\n",
    "                        'outcome': outcome,\n",
    "                        'data': (game_won, guesses)\n",
    "                    }\n",
    "                    pkl_list.append((pkl_file, scenario))  # Add scenario to the list\n",
    "\n",
    "# Accessing an individual pickle file's content by index\n",
    "index_to_access = 0  # Change this index to access different files\n",
    "if index_to_access < len(pkl_list):\n",
    "    file_path, scenario = pkl_list[index_to_access]\n",
    "    print(f\"Contents of {file_path}:\")\n",
    "    print_scenarios([scenario])  # Wrap scenario in a list for the function\n",
    "else:\n",
    "    print(f\"No pickle file at index {index_to_access}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
