{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "import sys\n",
    "# Custom library paths\n",
    "sys.path.extend(['../'])\n",
    "\n",
    "from scr.utils import set_seed\n",
    "from scr.utils import read_words\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scr.feature_engineering import process_single_word\n",
    "\n",
    "# def get_ngrams(word, n=2):\n",
    "#     if len(word) < n:\n",
    "#         return []  # or return a special token, e.g., ['<short>']\n",
    "#     return [word[i:i+n] for i in range(len(word)-n+1)]\n",
    "\n",
    "# get_ngrams('w', n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Reading and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.feature_engineering import process_single_word, \\\n",
    "    calculate_char_frequencies, char_to_idx, idx_to_char\n",
    "\n",
    "from scr.utils import *\n",
    "\n",
    "import random\n",
    "\n",
    "MASK_PROB = 0.8\n",
    "\n",
    "# Limit the number of words to a smaller number for debugging\n",
    "word_list = read_words('/home/sayem/Desktop/Hangman/words_250000_train.txt', limit=None)\n",
    "\n",
    "# Randomly select 1000 words\n",
    "unseen_words = random.sample(word_list, 1000)\n",
    "\n",
    "# Remove these words from the original list to create a separate test set\n",
    "word_list = [word for word in word_list if word not in unseen_words]\n",
    "\n",
    "# word_list = word_list[:10000]\n",
    "char_frequency = calculate_char_frequencies(word_list)\n",
    "max_word_length = max(len(word) for word in word_list)\n",
    "\n",
    "# Testing on a single word\n",
    "word = \"sir\"\n",
    "features, labels, missed_chars = process_single_word(word, char_frequency, \\\n",
    "    max_word_length, mask_prob=MASK_PROB, normalize=True)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for features, labels, missed characters, and original words\n",
    "all_features, all_labels, all_missed_chars, original_words = [], [], [], []\n",
    "\n",
    "for word in word_list:\n",
    "    # Process each word to get its features, label, and missed characters\n",
    "    feature_set, label, missed_chars = process_single_word(word, char_frequency, \\\n",
    "        max_word_length, mask_prob=MASK_PROB, normalize=True)\n",
    "\n",
    "    all_features.append(feature_set)\n",
    "    all_labels.append(label)\n",
    "    all_missed_chars.append(missed_chars)\n",
    "    original_words.append(word)  # Store the original word\n",
    "\n",
    "# Convert lists to tensors\n",
    "all_features_tensor = [features.squeeze(0) for features in all_features]  # Remove batch dimension\n",
    "labels_tensor = [label.squeeze(0) for label in all_labels]  # Remove batch dimension\n",
    "missed_chars_tensor = [missed_chars.squeeze(0) for missed_chars in all_missed_chars]  # Remove batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class HangmanDataset(Dataset):\n",
    "    def __init__(self, feature_tensors, label_tensors, missed_chars_tensors, original_words):\n",
    "        self.features = feature_tensors\n",
    "        self.labels = label_tensors\n",
    "        self.missed_chars = missed_chars_tensors\n",
    "        self.original_words = original_words\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx], self.missed_chars[idx], self.original_words[idx]\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_features, batch_labels, batch_missed_chars, original_words = zip(*batch)\n",
    "    \n",
    "    # Find the maximum sequence length in the batch\n",
    "    max_length = max(feature.size(0) for feature in batch_features)\n",
    "    \n",
    "    # Pad each sequence to the max_length\n",
    "    padded_features = [torch.nn.functional.pad(feature, \\\n",
    "        (0, 0, 0, max_length - feature.size(0))) for feature in batch_features]\n",
    "    padded_labels = [torch.nn.functional.pad(label, \\\n",
    "        (0, max_length - label.size(0))) for label in batch_labels]\n",
    "\n",
    "    # Convert list of tensors to tensors with an added batch dimension\n",
    "    padded_features = torch.stack(padded_features, dim=0)  # shape: [batch_size, max_seq_length, feature_size]\n",
    "    padded_labels = torch.stack(padded_labels, dim=0)     # shape: [batch_size, max_seq_length]\n",
    "    batch_missed_chars = torch.stack(batch_missed_chars, dim=0)  # shape: [batch_size, vocab_size]\n",
    "\n",
    "    # Create a tensor for lengths\n",
    "    lengths_features = torch.tensor([feature.size(0) \\\n",
    "        for feature in batch_features], dtype=torch.long)  # shape: [batch_size]\n",
    "\n",
    "    return padded_features, padded_labels, \\\n",
    "        batch_missed_chars, lengths_features, original_words\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming missed_chars_tensor is a list of tensors for missed characters\n",
    "# dataset = HangmanDataset(all_features_tensor, \\\n",
    "#     labels_tensor, missed_chars_tensor, original_words)\n",
    "\n",
    "# data_loader = DataLoader(dataset, batch_size=32, \\\n",
    "#     shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "dataset = HangmanDataset(all_features_tensor, \\\n",
    "    labels_tensor, missed_chars_tensor, original_words)\n",
    "\n",
    "dataset[140] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of iterating over the DataLoader\n",
    "# i = 0\n",
    "# for i, batch in enumerate(data_loader):\n",
    "#     inputs, labels, miss_chars, lengths, original_words = batch\n",
    "#     print(f\"Batch {i}: Inputs Shape: {inputs.shape}, Labels Shape: {labels.shape}, \\\n",
    "# Lengths: {lengths}, Miss Chars: {miss_chars}, Original Words: {original_words}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your main script or Jupyter Notebook\n",
    "from scr.rnn import RNN\n",
    "from scr.feature_engineering import process_single_word, \\\n",
    "    char_to_idx, idx_to_char, calculate_char_frequencies, \\\n",
    "        get_missed_characters\n",
    "from scr.game import simulate_game, \\\n",
    "    predict_next_character, predict_next_character_beam_search\n",
    "\n",
    "# Configuration for the RNN model\n",
    "# Configuration for the RNN model\n",
    "config = {\n",
    "    'rnn': 'LSTM',\n",
    "    'vocab_size': 27,  # Assuming 26 letters + 1 for underscore\n",
    "    'hidden_dim': 128,\n",
    "    'num_layers': 2,\n",
    "    'embedding_dim': 150,\n",
    "    'output_mid_features': 100,\n",
    "    'miss_linear_dim': 50,\n",
    "    'dropout': 0.5,\n",
    "    'use_embedding': True,\n",
    "    'lr': 0.0001,\n",
    "    'input_feature_size': 5 # Number of features excluding the embedding dimension\n",
    "}\n",
    "\n",
    "# Initialize RNN model\n",
    "model = RNN(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Prepare your dataset, train the model, etc.\n",
    "\n",
    "# Example of using predict_next_character in a game scenario\n",
    "word = \"exampleexample\"\n",
    "current_masked_word = \"_xam__e_xam__e\"\n",
    "missed_chars = get_missed_characters(word, char_to_idx)\n",
    "\n",
    "predicted_index = predict_next_character(model, current_masked_word, \\\n",
    "    missed_chars, char_frequency, max_word_length, \\\n",
    "        device=device, normalize=True)\n",
    "\n",
    "# predicted_index = predict_next_character_beam_search(model, current_masked_word, \\\n",
    "#     missed_chars, \\\n",
    "#     char_frequency, max_word_length, \\\n",
    "#     device, normalize=True, beam_width=3)\n",
    "        \n",
    "predicted_char = idx_to_char[predicted_index]\n",
    "\n",
    "predicted_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary functions\n",
    "from scr.feature_engineering import \\\n",
    "    process_single_word, get_missed_characters\n",
    "    \n",
    "from scr.game import predict_next_character, simulate_game\n",
    "from scr.rnn import RNN\n",
    "import random\n",
    "# random.seed(400)\n",
    "# Your existing code for initializing the model, etc.\n",
    "\n",
    "def play_multiple_games(model, num_games, word_list, \\\n",
    "    char_to_idx, idx_to_char, char_frequency, max_word_length, device):\n",
    "    game_results = []\n",
    "    for _ in range(num_games):\n",
    "        random_word = random.choice(word_list)\n",
    "        with torch.no_grad():\n",
    "            won, final_word, attempts_used = simulate_game(\n",
    "                model, \n",
    "                random_word, \n",
    "                char_to_idx, \n",
    "                idx_to_char, \n",
    "                char_frequency, \n",
    "                max_word_length, \n",
    "                device, \n",
    "                normalize=True, \n",
    "                max_attempts=6\n",
    "            )\n",
    "        game_results.append((won, final_word, attempts_used))\n",
    "    return game_results\n",
    "\n",
    "num_games = 1000\n",
    "results = play_multiple_games(model, num_games, \\\n",
    "    unseen_words, char_to_idx, idx_to_char, \\\n",
    "        char_frequency, max_word_length, device)\n",
    "\n",
    "# Analyzing results\n",
    "total_wins = sum(result[0] for result in results)\n",
    "\n",
    "total_wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, device=device):\n",
    "    total_actual_penalty = 0\n",
    "    total_miss_penalty = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        inputs, labels, miss_chars, lengths, _ = batch\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        miss_chars = miss_chars.to(device)\n",
    "        lengths = lengths # .to(device)\n",
    "\n",
    "        # print(f\"Batch {i}: Inputs Shape: {inputs.shape}, Labels Shape: {labels.shape}, \\\n",
    "        # Lengths: {lengths.shape}, Miss Chars: {miss_chars.shape}\")\n",
    "\n",
    "        # Run the model\n",
    "        outputs = model(inputs, lengths, miss_chars)\n",
    "        # print(f'NN output: {outputs.shape}')\n",
    "\n",
    "        # # Flatten output for loss calculation (if necessary)\n",
    "        # outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        # print(f'NN output (view): {outputs.shape}')\n",
    "\n",
    "        # labels = labels.view(-1).long()\n",
    "\n",
    "        # print(labels.shape)\n",
    "\n",
    "        # Calculate the custom loss\n",
    "        actual_penalty, miss_penalty = model.calculate_loss(outputs, \\\n",
    "            labels, lengths, miss_chars, vocab_size=27, use_cuda=True)\n",
    "\n",
    "        # print(actual_penalty)\n",
    "        # print(miss_penalty)\n",
    "\n",
    "        total_actual_penalty += actual_penalty.item()\n",
    "        total_miss_penalty += miss_penalty.item()\n",
    "        total_batches += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        actual_penalty.backward()  # Backpropagation for the actual_penalty\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_actual_penalty = total_actual_penalty / total_batches if total_batches > 0 else 0\n",
    "    avg_miss_penalty = total_miss_penalty / total_batches if total_batches > 0 else 0\n",
    "    return avg_actual_penalty, avg_miss_penalty\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def validate_one_epoch(model, val_loader, device=device, max_games_per_epoch=100):\n",
    "    model.eval()\n",
    "    total_wins = 0\n",
    "    total_games = 0\n",
    "\n",
    "    # Collect all words from the validation loader\n",
    "    all_words = []\n",
    "    for batch in val_loader:\n",
    "        batch_original_words = batch[-1]  # Adjust according to your batch structure\n",
    "        all_words.extend(batch_original_words)\n",
    "\n",
    "    # Randomly sample a set number of words for this epoch's validation\n",
    "    selected_words = random.sample(all_words, min(max_games_per_epoch, len(all_words)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for word in selected_words:\n",
    "            won, final_word, attempts_used = simulate_game(\n",
    "                model, \n",
    "                word, \n",
    "                char_to_idx, \n",
    "                idx_to_char, \n",
    "                char_frequency, \n",
    "                max_word_length, \n",
    "                device, \n",
    "                normalize=True, \n",
    "                max_attempts=6\n",
    "            )\n",
    "            total_wins += int(won)\n",
    "            total_games += 1\n",
    "\n",
    "    accuracy_percentage = (total_wins / total_games) * 100 if total_games > 0 else 0\n",
    "    return accuracy_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from scr.rnn import RNN\n",
    "# from scr.game import validate_one_epoch\n",
    "# from scr.utils import EarlyStopping # , train_one_epoch  # Assuming these are in scr.utils\n",
    "\n",
    "def objective(trial, dataset, static_config, num_epochs):\n",
    "    # Dynamic hyperparameters\n",
    "    # Get dynamic hyperparameters from the trial\n",
    "    dynamic_config = optuna_dynamic_hyperparameters(trial)\n",
    "    \n",
    "    # Merge configurations\n",
    "    config = {**static_config, **dynamic_config}\n",
    "\n",
    "    # k-Fold Cross-Validation setup\n",
    "    kfold = KFold(n_splits=5, shuffle=True)\n",
    "    val_accuracies = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        train_loader, val_loader = get_data_loaders(dataset, train_idx, val_idx)\n",
    "\n",
    "        # Model initialization and training\n",
    "        model, optimizer = initialize_model(config)\n",
    "        early_stopping = EarlyStopping(patience=10, delta=0.001)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            _, _ = train_one_epoch(model, train_loader, optimizer)\n",
    "            validation_accuracy = validate_one_epoch(model, val_loader)\n",
    "            \n",
    "            if early_stopping(validation_accuracy):\n",
    "                break\n",
    "\n",
    "        val_accuracies.append(validation_accuracy)\n",
    "\n",
    "    return sum(val_accuracies) / len(val_accuracies)\n",
    "\n",
    "# Utility functions (for cleaner code)\n",
    "def get_data_loaders(dataset, train_idx, val_idx):\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def initialize_model(config):\n",
    "    model = RNN(config)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    return model, optimizer\n",
    "\n",
    "# Dynamic hyperparameters for Optuna trials\n",
    "def optuna_dynamic_hyperparameters(trial):\n",
    "    return {\n",
    "        'lr': trial.suggest_float('lr', 1e-5, 1e-3, log=True),\n",
    "        'hidden_dim': trial.suggest_categorical('hidden_dim', [128, 256, 512]),\n",
    "        'embedding_dim': trial.suggest_categorical('embedding_dim', [32, 50, 100]),\n",
    "        'output_mid_features': trial.suggest_categorical('output_mid_features', [50, 100, 200]),\n",
    "        'miss_linear_dim': trial.suggest_categorical('miss_linear_dim', [25, 50, 100]),\n",
    "        'dropout': trial.suggest_float('dropout', 0.2, 0.5),\n",
    "        'num_layers': trial.suggest_int('num_layers', 1, 3)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming missed_chars_tensor is a list of tensors for missed characters\n",
    "dataset = HangmanDataset(all_features_tensor, \\\n",
    "    labels_tensor, missed_chars_tensor, original_words)\n",
    "\n",
    "# data_loader = DataLoader(dataset, batch_size=32, \\\n",
    "#     shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Static configuration\n",
    "static_config = {\n",
    "    'rnn': 'LSTM',\n",
    "    'vocab_size': 27,  # 26 English alphabets + 1 (e.g., for underscore)\n",
    "    'use_embedding': True,  # Typically a design choice\n",
    "    'input_feature_size': 5,  # Based on your feature engineering strategy\n",
    "}\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define num_epochs for training in each fold\n",
    "num_epochs = 1  # Adjust as needed\n",
    "\n",
    "# Create Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda trial: objective(trial, dataset, \\\n",
    "    static_config, num_epochs), n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from scr.utils import EarlyStopping\n",
    "\n",
    "def objective(trial, dataset, static_config):\n",
    "    # Hyperparameter space definition\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [128, 256, 512])\n",
    "    dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "\n",
    "    # Merge static configuration with dynamic hyperparameters\n",
    "    config = {**static_config, 'lr': lr, 'hidden_dim': hidden_dim, 'dropout': dropout, 'num_layers': num_layers}\n",
    "\n",
    "    # ... rest of the objective function (k-fold cross-validation, training, validation)\n",
    "    # Use 'config' to initialize the RNN model\n",
    "# Load or define your dataset\n",
    "dataset = HangmanDataset(...)\n",
    "# Assuming missed_chars_tensor is a list of tensors for missed characters\n",
    "dataset = HangmanDataset(all_features_tensor, \\\n",
    "    labels_tensor, missed_chars_tensor, original_words)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=32, \\\n",
    "    shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Static configuration\n",
    "static_config = {\n",
    "    'vocab_size': 27,\n",
    "    'embedding_dim': 50,\n",
    "    'output_mid_features': 100,\n",
    "    'miss_linear_dim': 50,\n",
    "    'use_embedding': True,\n",
    "    'input_feature_size': 5\n",
    "}\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter space definition\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [128, 256, 512])\n",
    "    dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "\n",
    "    # Configurations for the model\n",
    "    config = {\n",
    "        'lr': lr,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'dropout': dropout,\n",
    "        'num_layers': num_layers,\n",
    "        # Other necessary configurations\n",
    "    }\n",
    "\n",
    "    # k-Fold Cross-Validation\n",
    "    kfold = KFold(n_splits=5, shuffle=True)\n",
    "    val_accuracies = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "        val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "        # Initialize the model with the current configuration\n",
    "        model = RNN(config)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        # Early Stopping and Training Logic\n",
    "        early_stopping = EarlyStopping(patience=10, delta=0.001)\n",
    "        for epoch in range(num_epochs):  # Define num_epochs\n",
    "            avg_actual_penalty, avg_miss_penalty = train_one_epoch(model, train_loader, optimizer, device)\n",
    "            \n",
    "            # Validation step\n",
    "            validation_accuracy = validate_one_epoch(model, val_loader, device, max_games_per_epoch=100)\n",
    "            if early_stopping(validation_accuracy):\n",
    "                break\n",
    "\n",
    "        val_accuracies.append(validation_accuracy)\n",
    "\n",
    "    # Calculate average validation accuracy over all folds\n",
    "    avg_val_accuracy = sum(val_accuracies) / len(val_accuracies)\n",
    "    return avg_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# def validate_one_epoch(model, val_loader, device, max_games_per_epoch=100):\n",
    "#     model.eval()\n",
    "#     total_wins = 0\n",
    "#     total_games = 0\n",
    "\n",
    "#     # Collect all words from the validation loader\n",
    "#     all_words = []\n",
    "#     for batch in val_loader:\n",
    "#         batch_original_words = batch[-1]  # Adjust according to your batch structure\n",
    "#         all_words.extend(batch_original_words)\n",
    "\n",
    "#     # Randomly sample a set number of words for this epoch's validation\n",
    "#     selected_words = random.sample(all_words, min(max_games_per_epoch, len(all_words)))\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for word in selected_words:\n",
    "#             won, final_word, attempts_used = simulate_game(\n",
    "#                 model, \n",
    "#                 word, \n",
    "#                 char_to_idx, \n",
    "#                 idx_to_char, \n",
    "#                 char_frequency, \n",
    "#                 max_word_length, \n",
    "#                 device, \n",
    "#                 normalize=True, \n",
    "#                 max_attempts=6\n",
    "#             )\n",
    "#             total_wins += int(won)\n",
    "#             total_games += 1\n",
    "\n",
    "#     accuracy_percentage = (total_wins / total_games) * 100 if total_games > 0 else 0\n",
    "#     return accuracy_percentage\n",
    "\n",
    "\n",
    "# # Training loop\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "# num_epochs = 20\n",
    "\n",
    "# # Assuming missed_chars_tensor is a list of tensors for missed characters\n",
    "# dataset = HangmanDataset(all_features_tensor, \\\n",
    "#     labels_tensor, missed_chars_tensor, original_words)\n",
    "\n",
    "# data_loader = DataLoader(dataset, batch_size=32, \\\n",
    "#     shuffle=True, collate_fn=collate_fn)\n",
    "# # Assuming the HangmanDataset, DataLoader, and model setup are already done\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training step\n",
    "#     avg_actual_penalty, avg_miss_penalty = train_one_epoch(model, data_loader, optimizer, device)\n",
    "#     print(f\"Epoch {epoch+1}: Training - Avg Actual Penalty: {avg_actual_penalty}, Avg Miss Penalty: {avg_miss_penalty}\")\n",
    "\n",
    "#     # Validation step\n",
    "#     validation_accuracy = validate_one_epoch(model, val_loader, device, max_games_per_epoch=100)\n",
    "#     print(f\"Epoch {epoch+1}: Validation - Accuracy: {validation_accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary functions\n",
    "from scr.feature_engineering import \\\n",
    "    process_single_word, get_missed_characters\n",
    "    \n",
    "from scr.game import predict_next_character, simulate_game\n",
    "from scr.rnn import RNN\n",
    "import random\n",
    "# random.seed(400)\n",
    "# Your existing code for initializing the model, etc.\n",
    "\n",
    "def play_multiple_games(model, num_games, word_list, \\\n",
    "    char_to_idx, idx_to_char, char_frequency, max_word_length, device):\n",
    "    game_results = []\n",
    "    for _ in range(num_games):\n",
    "        random_word = random.choice(word_list)\n",
    "        with torch.no_grad():\n",
    "            won, final_word, attempts_used = simulate_game(\n",
    "                model, \n",
    "                random_word, \n",
    "                char_to_idx, \n",
    "                idx_to_char, \n",
    "                char_frequency, \n",
    "                max_word_length, \n",
    "                device, \n",
    "                normalize=True, \n",
    "                max_attempts=6\n",
    "            )\n",
    "        game_results.append((won, final_word, attempts_used))\n",
    "    return game_results\n",
    "\n",
    "num_games = 1000\n",
    "\n",
    "results = play_multiple_games(model, num_games, \\\n",
    "    unseen_words, char_to_idx, idx_to_char, \\\n",
    "        char_frequency, max_word_length, device)\n",
    "\n",
    "# Analyzing results\n",
    "total_wins = sum(result[0] for result in results)\n",
    "\n",
    "total_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# Define the training function for one epoch\n",
    "def train_one_epoch(model, data_loader, optimizer, loss_function, device):\n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    model.train()\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        inputs = batch[0]\n",
    "        labels = batch[1]\n",
    "        lengths = batch[2]\n",
    "\n",
    "        padded_inputs = pad_sequence(inputs, batch_first=True).to(device)\n",
    "        lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "        padded_labels = pad_sequence(labels, batch_first=True).to(device)\n",
    "\n",
    "        hidden = model.init_hidden(padded_inputs.size(0))\n",
    "        outputs, hidden = model(padded_inputs, lengths, hidden)\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        labels = padded_labels.view(-1).long()\n",
    "\n",
    "        loss = loss_function(outputs, labels) # TODO: forece predict? here? \n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / total_batches if total_batches > 0 else 0\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def validate_model(model, val_loader, device, num_games=2000):\n",
    "    model.eval()\n",
    "    total_wins = 0\n",
    "\n",
    "    # Collect words from the validation loader\n",
    "    words_for_validation = []\n",
    "    for _, _, _, batch_original_words in val_loader:\n",
    "        words_for_validation.extend(batch_original_words)\n",
    "\n",
    "    # Randomly select 'num_games' words for validation\n",
    "    selected_words = random.sample(words_for_validation, num_games)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for word in selected_words:\n",
    "            win_result, _, _ = simulate_game(model, word, char_to_idx, idx_to_char, \n",
    "                                             char_frequency, max_word_length, \n",
    "                                             device=device, normalize=True, \n",
    "                                             max_attempts=6)\n",
    "            total_wins += int(win_result)\n",
    "    \n",
    "    win_rate_percentage = int((total_wins / num_games) * 100)  # Convert to percentage\n",
    "\n",
    "    return win_rate_percentage if num_games > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def objective(trial, dataset, input_size, output_size, num_epochs, device):\n",
    "    \n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [64, 128, 256])\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5)\n",
    "    num_layers = 2  # Or any other integer value you wish to use\n",
    "\n",
    "    total_win_rate = 0\n",
    "\n",
    "    kf = KFold(n_splits=5)\n",
    "    fold_number = 1  # Initialize fold counter\n",
    "    trial_number = trial.number  # Get the current trial number\n",
    "\n",
    "    # Create directory for this trial's plots if it doesn't exist\n",
    "    plot_dir = Path(f'./plots/trial_{trial_number}')\n",
    "    plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for train_index, val_index in kf.split(dataset):\n",
    "\n",
    "        print(f\"Fold {fold_number}\")  # Print the current fold number\n",
    "        train_dataset = Subset(dataset, train_index)\n",
    "        val_dataset = Subset(dataset, val_index)\n",
    "\n",
    "        print(f\"Number of words in training dataset: {len(train_dataset)}\")\n",
    "        print(f\"Number of words in validation dataset: {len(val_dataset)}\")\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, \\\n",
    "            batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "        val_loader = DataLoader(val_dataset, \\\n",
    "            batch_size=256, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "        model = HangmanLSTM(input_size, hidden_size, \\\n",
    "            output_size, num_layers, dropout_rate=dropout_rate).to(device)\n",
    "            \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5)\n",
    "        \n",
    "        best_win_rate = 0\n",
    "        epochs_no_improve = 0\n",
    "        early_stop_epochs = 5\n",
    "        num_epochs = num_epochs\n",
    "\n",
    "        # Lists to store metrics for plotting\n",
    "        epoch_losses = []\n",
    "        epoch_win_rates = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            avg_loss = train_one_epoch(model, train_loader, \\\n",
    "                optimizer, loss_function, device)\n",
    "\n",
    "            epoch_losses.append(avg_loss)  # Store loss for this epoch\n",
    "\n",
    "            print(f\"Epoch {epoch}: Loss {avg_loss}\")\n",
    "            \n",
    "            win_rate = validate_model(model, val_loader, device)\n",
    "\n",
    "            epoch_win_rates.append(win_rate)  # Store win rate for this epoch\n",
    "\n",
    "            print(f\"Epoch {epoch}/{num_epochs}: Train Loss {avg_loss}, Win Rate {win_rate}\")\n",
    "\n",
    "            # Store metrics\n",
    "            epoch_losses.append(avg_loss)\n",
    "            epoch_win_rates.append(win_rate)\n",
    "\n",
    "            scheduler.step(win_rate)\n",
    "\n",
    "            # Gradient clipping\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            if win_rate > best_win_rate:\n",
    "                best_win_rate = win_rate\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve == early_stop_epochs:\n",
    "                break  # Early stopping\n",
    "        \n",
    "        \n",
    "        # Plotting after each fold\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Plot Loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, label='Loss')  # Ensure correct range\n",
    "        plt.title(f'Fold {fold_number} - Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'./plots/trial_{trial.number}/fold_{fold_number}_loss.png')\n",
    "\n",
    "        # Plot Win Rate\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(1, len(epoch_win_rates) + 1), epoch_win_rates, label='Win Rate')  # Ensure correct range\n",
    "        plt.title(f'Fold {fold_number} - Validation Win Rate')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Win Rate')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'./plots/trial_{trial.number}/fold_{fold_number}_win_rate.png')\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        fold_number += 1\n",
    "\n",
    "        total_win_rate += best_win_rate\n",
    "\n",
    "    average_win_rate = total_win_rate / kf.get_n_splits()\n",
    "\n",
    "    print(f\"Avg win rate: \", average_win_rate)\n",
    "    average_win_rate = 0\n",
    "    return average_win_rate\n",
    "\n",
    "# Define your dataset, input_size, output_size, device, num_epochs\n",
    "dataset = HangmanDataset(all_features_tensor, labels_tensor, original_words)\n",
    "\n",
    "input_size = 4  # Set your input size\n",
    "output_size = 27  # Set your output size\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Choose device\n",
    "num_epochs = 10\n",
    "n_trials = 1\n",
    "\n",
    "# Create and run the Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda trial: objective(trial, dataset, \\\n",
    "    input_size, output_size, num_epochs, device), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the best hyperparameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"Value: {trial.value}\")\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
