{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "import sys\n",
    "# Custom library paths\n",
    "sys.path.extend(['../'])\n",
    "\n",
    "from scr.utils import set_seed\n",
    "from scr.utils import read_words\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scr.feature_engineering import process_single_word\n",
    "\n",
    "# def get_ngrams(word, n=2):\n",
    "#     if len(word) < n:\n",
    "#         return []  # or return a special token, e.g., ['<short>']\n",
    "#     return [word[i:i+n] for i in range(len(word)-n+1)]\n",
    "\n",
    "# get_ngrams('w', n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Reading and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.1034,  0.0000,  0.0699, 19.0000],\n",
       "         [ 0.0000,  0.1034,  1.0000,  0.0869,  9.0000],\n",
       "         [ 0.0000,  0.1034,  2.0000,  0.0703,  9.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000, 18.0000]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scr.feature_engineering import process_single_word, \\\n",
    "    calculate_char_frequencies, char_to_idx, idx_to_char\n",
    "\n",
    "from scr.utils import *\n",
    "\n",
    "import random\n",
    "\n",
    "MASK_PROB = 0.8\n",
    "\n",
    "# Limit the number of words to a smaller number for debugging\n",
    "word_list = read_words('/home/sayem/Desktop/Hangman/words_250000_train.txt', limit=None)\n",
    "\n",
    "# Randomly select 1000 words\n",
    "unseen_words = random.sample(word_list, 1000)\n",
    "\n",
    "# Remove these words from the original list to create a separate test set\n",
    "word_list = [word for word in word_list if word not in unseen_words]\n",
    "\n",
    "# word_list = word_list[:10000]\n",
    "char_frequency = calculate_char_frequencies(word_list)\n",
    "max_word_length = max(len(word) for word in word_list)\n",
    "\n",
    "# Testing on a single word\n",
    "word = \"sir\"\n",
    "features, labels, missed_chars = process_single_word(word, char_frequency, \\\n",
    "    max_word_length, mask_prob=MASK_PROB, normalize=True)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize lists for features, labels, missed characters, and original words\n",
    "# all_features, all_labels, all_missed_chars, original_words = [], [], [], []\n",
    "\n",
    "# for word in word_list:\n",
    "#     # Process each word to get its features, label, and missed characters\n",
    "#     feature_set, label, missed_chars = process_single_word(word, char_frequency, \\\n",
    "#         max_word_length, mask_prob=MASK_PROB, normalize=True)\n",
    "\n",
    "#     all_features.append(feature_set)\n",
    "#     all_labels.append(label)\n",
    "#     all_missed_chars.append(missed_chars)\n",
    "#     original_words.append(word)  # Store the original word\n",
    "\n",
    "# # Convert lists to tensors\n",
    "# all_features_tensor = [features.squeeze(0) for features in all_features]  # Remove batch dimension\n",
    "# labels_tensor = [label.squeeze(0) for label in all_labels]  # Remove batch dimension\n",
    "# missed_chars_tensor = [missed_chars.squeeze(0) for missed_chars in all_missed_chars]  # Remove batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# class HangmanDataset(Dataset):\n",
    "#     def __init__(self, feature_tensors, label_tensors, missed_chars_tensors, original_words):\n",
    "#         self.features = feature_tensors\n",
    "#         self.labels = label_tensors\n",
    "#         self.missed_chars = missed_chars_tensors\n",
    "#         self.original_words = original_words\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.features)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.features[idx], self.labels[idx], self.missed_chars[idx], self.original_words[idx]\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     batch_features, batch_labels, batch_missed_chars, original_words = zip(*batch)\n",
    "    \n",
    "#     # Find the maximum sequence length in the batch\n",
    "#     max_length = max(feature.size(0) for feature in batch_features)\n",
    "    \n",
    "#     # Pad each sequence to the max_length\n",
    "#     padded_features = [torch.nn.functional.pad(feature, \\\n",
    "#         (0, 0, 0, max_length - feature.size(0))) for feature in batch_features]\n",
    "#     padded_labels = [torch.nn.functional.pad(label, \\\n",
    "#         (0, max_length - label.size(0))) for label in batch_labels]\n",
    "\n",
    "#     # Convert list of tensors to tensors with an added batch dimension\n",
    "#     padded_features = torch.stack(padded_features, dim=0)  # shape: [batch_size, max_seq_length, feature_size]\n",
    "#     padded_labels = torch.stack(padded_labels, dim=0)     # shape: [batch_size, max_seq_length]\n",
    "#     batch_missed_chars = torch.stack(batch_missed_chars, dim=0)  # shape: [batch_size, vocab_size]\n",
    "\n",
    "#     # Create a tensor for lengths\n",
    "#     lengths_features = torch.tensor([feature.size(0) for feature in batch_features], dtype=torch.long)  # shape: [batch_size]\n",
    "\n",
    "#     return padded_features, padded_labels, batch_missed_chars, lengths_features, original_words\n",
    "\n",
    "\n",
    "\n",
    "# # # Assuming missed_chars_tensor is a list of tensors for missed characters\n",
    "# # dataset = HangmanDataset(all_features_tensor, \\\n",
    "# #     labels_tensor, missed_chars_tensor, original_words)\n",
    "\n",
    "# # data_loader = DataLoader(dataset, batch_size=32, \\\n",
    "# #     shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# dataset = HangmanDataset(all_features_tensor, \\\n",
    "#     labels_tensor, missed_chars_tensor, original_words)\n",
    "\n",
    "# dataset[0]\n",
    "# # Example of iterating over the DataLoader\n",
    "# i = 0\n",
    "# for i, batch in enumerate(data_loader):\n",
    "#     inputs, labels, miss_chars, lengths, original_words = batch\n",
    "#     print(f\"Batch {i}: Inputs Shape: {inputs.shape}, Labels Shape: {labels.shape}, \\\n",
    "# Lengths: {lengths}, Miss Chars: {miss_chars}, Original Words: {original_words}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of iterating over the DataLoader\n",
    "# i = 0\n",
    "# for i, batch in enumerate(data_loader):\n",
    "#     inputs, labels, miss_chars, lengths, original_words = batch\n",
    "#     print(f\"Batch {i}: Inputs Shape: {inputs.shape}, Labels Shape: {labels.shape}, \\\n",
    "# Lengths: {lengths}, Miss Chars: {miss_chars}, Original Words: {original_words}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[27648, 1]' is invalid for input of size 27136",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sayem/Desktop/Hangman/old/notebook.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/old/notebook.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m current_masked_word \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_xam__e\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/old/notebook.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m missed_chars \u001b[39m=\u001b[39m get_missed_characters(word, char_to_idx)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/old/notebook.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m predicted_index \u001b[39m=\u001b[39m predict_next_character(model, current_masked_word, \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/old/notebook.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     missed_chars, char_frequency, max_word_length, \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/old/notebook.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         device\u001b[39m=\u001b[39;49mdevice, normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/old/notebook.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m predicted_char \u001b[39m=\u001b[39m idx_to_char[predicted_index]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/old/notebook.ipynb#X14sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m predicted_char\n",
      "File \u001b[0;32m~/Desktop/Hangman/old/../scr/game.py:112\u001b[0m, in \u001b[0;36mpredict_next_character\u001b[0;34m(model, current_masked_word, missed_chars, char_frequency, max_word_length, device, normalize)\u001b[0m\n\u001b[1;32m    109\u001b[0m sequence_lengths \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([feature_set\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)  \u001b[39m# .to(device)\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 112\u001b[0m     output \u001b[39m=\u001b[39m model(feature_set, sequence_lengths, missed_chars)  \u001b[39m# Call the RNN model\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[39m# Since the output is (batch size, max sequence length, output dim), we are interested in the last character\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[39m# We take the last character in the sequence which is not a padding (based on sequence length)\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     last_char_position \u001b[39m=\u001b[39m sequence_lengths\u001b[39m.\u001b[39mitem() \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/optiver/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/optiver/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Hangman/old/../scr/rnn.py:81\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, x, x_lens, miss_chars)\u001b[0m\n\u001b[1;32m     78\u001b[0m x_packed \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpack_padded_sequence(x, x_lens, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, enforce_sorted\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     80\u001b[0m \u001b[39m# RNN forward pass\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m output_packed, (hidden, cell) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x_packed)\n\u001b[1;32m     83\u001b[0m \u001b[39m# print(output_packed.shape)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \n\u001b[1;32m     85\u001b[0m \u001b[39m# Unpack the sequence\u001b[39;00m\n\u001b[1;32m     86\u001b[0m output, _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpad_packed_sequence(output_packed, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/optiver/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/optiver/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/optiver/lib/python3.10/site-packages/torch/nn/modules/rnn.py:882\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    879\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    880\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n\u001b[1;32m    881\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 882\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, batch_sizes, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional)\n\u001b[1;32m    884\u001b[0m output \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m]\n\u001b[1;32m    885\u001b[0m hidden \u001b[39m=\u001b[39m result[\u001b[39m1\u001b[39m:]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[27648, 1]' is invalid for input of size 27136"
     ]
    }
   ],
   "source": [
    "# In your main script or Jupyter Notebook\n",
    "from scr.rnn import RNN\n",
    "from scr.feature_engineering import process_single_word, \\\n",
    "    char_to_idx, idx_to_char, calculate_char_frequencies, \\\n",
    "        get_missed_characters\n",
    "from scr.game import simulate_game, predict_next_character, \\\n",
    "    predict_next_character_beam_search\n",
    "\n",
    "# Configuration for the RNN model\n",
    "# Configuration for the RNN model\n",
    "config = {\n",
    "    'rnn': 'LSTM',\n",
    "    'vocab_size': 27,  # Assuming 26 letters + 1 for underscore\n",
    "    'hidden_dim': 128,\n",
    "    'num_layers': 2,\n",
    "    'embedding_dim': 50,\n",
    "    'output_mid_features': 100,\n",
    "    'miss_linear_dim': 50,\n",
    "    'dropout': 0.5,\n",
    "    'use_embedding': True,\n",
    "    'lr': 0.001,\n",
    "    'input_feature_size': 4  # Number of features excluding the embedding dimension\n",
    "}\n",
    "\n",
    "# Initialize RNN model\n",
    "model = RNN(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Prepare your dataset, train the model, etc.\n",
    "\n",
    "# Example of using predict_next_character in a game scenario\n",
    "word = \"example\"\n",
    "current_masked_word = \"_xam__e\"\n",
    "missed_chars = get_missed_characters(word, char_to_idx)\n",
    "\n",
    "predicted_index = predict_next_character(model, current_masked_word, \\\n",
    "    missed_chars, char_frequency, max_word_length, \\\n",
    "        device=device, normalize=True)\n",
    "        \n",
    "predicted_char = idx_to_char[predicted_index]\n",
    "\n",
    "predicted_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.feature_engineering import process_single_word, \\\n",
    "    calculate_char_frequencies, char_to_idx, idx_to_char\n",
    "\n",
    "MASK_PROB = 0.8\n",
    "\n",
    "# Limit the number of words to a smaller number for debugging\n",
    "word_list = read_words('/home/sayem/Desktop/Hangman/words_250000_train.txt', limit=None)\n",
    "\n",
    "# word_list = word_list[:10000]\n",
    "char_frequency = calculate_char_frequencies(word_list)\n",
    "max_word_length = max(len(word) for word in word_list)\n",
    "\n",
    "# Testing on a single word\n",
    "word = \"sir\"\n",
    "features, labels, missed_chars = process_single_word(word, char_frequency, \\\n",
    "    max_word_length, mask_prob=MASK_PROB, normalize=True)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary functions\n",
    "from scr.feature_engineering import \\\n",
    "    process_single_word, get_missed_characters\n",
    "    \n",
    "from scr.game import predict_next_character, simulate_game\n",
    "from scr.rnn import RNN\n",
    "import random\n",
    "# random.seed(400)\n",
    "# Your existing code for initializing the model, etc.\n",
    "\n",
    "def play_multiple_games(model, num_games, word_list, \\\n",
    "    char_to_idx, idx_to_char, char_frequency, max_word_length, device):\n",
    "    game_results = []\n",
    "    for _ in range(num_games):\n",
    "        random_word = random.choice(word_list)\n",
    "        with torch.no_grad():\n",
    "            won, final_word, attempts_used = simulate_game(\n",
    "                model, \n",
    "                random_word, \n",
    "                char_to_idx, \n",
    "                idx_to_char, \n",
    "                char_frequency, \n",
    "                max_word_length, \n",
    "                device, \n",
    "                normalize=True, \n",
    "                max_attempts=6\n",
    "            )\n",
    "        game_results.append((won, final_word, attempts_used))\n",
    "    return game_results\n",
    "\n",
    "num_games = 1000\n",
    "results = play_multiple_games(model, num_games, \\\n",
    "    word_list, char_to_idx, idx_to_char, \\\n",
    "        char_frequency, max_word_length, device)\n",
    "\n",
    "# Analyzing results\n",
    "total_wins = sum(result[0] for result in results)\n",
    "\n",
    "total_wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, device):\n",
    "    total_actual_penalty = 0\n",
    "    total_miss_penalty = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        inputs, labels, miss_chars, lengths, _ = batch\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        miss_chars = miss_chars.to(device)\n",
    "        lengths = lengths # .to(device)\n",
    "\n",
    "        # print(f\"Batch {i}: Inputs Shape: {inputs.shape}, Labels Shape: {labels.shape}, \\\n",
    "        # Lengths: {lengths.shape}, Miss Chars: {miss_chars.shape}\")\n",
    "\n",
    "        # Run the model\n",
    "        outputs = model(inputs, lengths, miss_chars)\n",
    "        # print(f'NN output: {outputs.shape}')\n",
    "\n",
    "        # # Flatten output for loss calculation (if necessary)\n",
    "        # outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        # print(f'NN output (view): {outputs.shape}')\n",
    "\n",
    "        # labels = labels.view(-1).long()\n",
    "\n",
    "        # print(labels.shape)\n",
    "\n",
    "        # Calculate the custom loss\n",
    "        actual_penalty, miss_penalty = model.calculate_loss(outputs, \\\n",
    "            labels, lengths, miss_chars, vocab_size=27, use_cuda=True)\n",
    "\n",
    "        # print(actual_penalty)\n",
    "        # print(miss_penalty)\n",
    "\n",
    "        total_actual_penalty += actual_penalty.item()\n",
    "        total_miss_penalty += miss_penalty.item()\n",
    "        total_batches += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        actual_penalty.backward()  # Backpropagation for the actual_penalty\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_actual_penalty = total_actual_penalty / total_batches if total_batches > 0 else 0\n",
    "    avg_miss_penalty = total_miss_penalty / total_batches if total_batches > 0 else 0\n",
    "    return avg_actual_penalty, avg_miss_penalty\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "num_epochs = 15\n",
    "\n",
    "# Assuming missed_chars_tensor is a list of tensors for missed characters\n",
    "dataset = HangmanDataset(all_features_tensor, \\\n",
    "    labels_tensor, missed_chars_tensor, original_words)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=32, \\\n",
    "    shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_actual_penalty, avg_miss_penalty = train_one_epoch(model, data_loader, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}: Avg Actual Penalty: {avg_actual_penalty}, Avg Miss Penalty: {avg_miss_penalty}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary functions\n",
    "from scr.feature_engineering import \\\n",
    "    process_single_word, get_missed_characters\n",
    "    \n",
    "from scr.game import predict_next_character, simulate_game\n",
    "from scr.rnn import RNN\n",
    "import random\n",
    "# random.seed(400)\n",
    "# Your existing code for initializing the model, etc.\n",
    "\n",
    "def play_multiple_games(model, num_games, word_list, \\\n",
    "    char_to_idx, idx_to_char, char_frequency, max_word_length, device):\n",
    "    game_results = []\n",
    "    for _ in range(num_games):\n",
    "        random_word = random.choice(word_list)\n",
    "        with torch.no_grad():\n",
    "            won, final_word, attempts_used = simulate_game(\n",
    "                model, \n",
    "                random_word, \n",
    "                char_to_idx, \n",
    "                idx_to_char, \n",
    "                char_frequency, \n",
    "                max_word_length, \n",
    "                device, \n",
    "                normalize=True, \n",
    "                max_attempts=6\n",
    "            )\n",
    "        game_results.append((won, final_word, attempts_used))\n",
    "    return game_results\n",
    "\n",
    "num_games = 1000\n",
    "results = play_multiple_games(model, num_games, \\\n",
    "    word_list, char_to_idx, idx_to_char, \\\n",
    "        char_frequency, max_word_length, device)\n",
    "\n",
    "# Analyzing results\n",
    "total_wins = sum(result[0] for result in results)\n",
    "\n",
    "total_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# Define the training function for one epoch\n",
    "def train_one_epoch(model, data_loader, optimizer, loss_function, device):\n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    model.train()\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        inputs = batch[0]\n",
    "        labels = batch[1]\n",
    "        lengths = batch[2]\n",
    "\n",
    "        padded_inputs = pad_sequence(inputs, batch_first=True).to(device)\n",
    "        lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "        padded_labels = pad_sequence(labels, batch_first=True).to(device)\n",
    "\n",
    "        hidden = model.init_hidden(padded_inputs.size(0))\n",
    "        outputs, hidden = model(padded_inputs, lengths, hidden)\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        labels = padded_labels.view(-1).long()\n",
    "\n",
    "        loss = loss_function(outputs, labels) # TODO: forece predict? here? \n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / total_batches if total_batches > 0 else 0\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def validate_model(model, val_loader, device, num_games=2000):\n",
    "    model.eval()\n",
    "    total_wins = 0\n",
    "\n",
    "    # Collect words from the validation loader\n",
    "    words_for_validation = []\n",
    "    for _, _, _, batch_original_words in val_loader:\n",
    "        words_for_validation.extend(batch_original_words)\n",
    "\n",
    "    # Randomly select 'num_games' words for validation\n",
    "    selected_words = random.sample(words_for_validation, num_games)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for word in selected_words:\n",
    "            win_result, _, _ = simulate_game(model, word, char_to_idx, idx_to_char, \n",
    "                                             char_frequency, max_word_length, \n",
    "                                             device=device, normalize=True, \n",
    "                                             max_attempts=6)\n",
    "            total_wins += int(win_result)\n",
    "    \n",
    "    win_rate_percentage = int((total_wins / num_games) * 100)  # Convert to percentage\n",
    "\n",
    "    return win_rate_percentage if num_games > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def objective(trial, dataset, input_size, output_size, num_epochs, device):\n",
    "    \n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [64, 128, 256])\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5)\n",
    "    num_layers = 2  # Or any other integer value you wish to use\n",
    "\n",
    "    total_win_rate = 0\n",
    "\n",
    "    kf = KFold(n_splits=5)\n",
    "    fold_number = 1  # Initialize fold counter\n",
    "    trial_number = trial.number  # Get the current trial number\n",
    "\n",
    "    # Create directory for this trial's plots if it doesn't exist\n",
    "    plot_dir = Path(f'./plots/trial_{trial_number}')\n",
    "    plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for train_index, val_index in kf.split(dataset):\n",
    "\n",
    "        print(f\"Fold {fold_number}\")  # Print the current fold number\n",
    "        train_dataset = Subset(dataset, train_index)\n",
    "        val_dataset = Subset(dataset, val_index)\n",
    "\n",
    "        print(f\"Number of words in training dataset: {len(train_dataset)}\")\n",
    "        print(f\"Number of words in validation dataset: {len(val_dataset)}\")\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, \\\n",
    "            batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "        val_loader = DataLoader(val_dataset, \\\n",
    "            batch_size=256, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "        model = HangmanLSTM(input_size, hidden_size, \\\n",
    "            output_size, num_layers, dropout_rate=dropout_rate).to(device)\n",
    "            \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5)\n",
    "        \n",
    "        best_win_rate = 0\n",
    "        epochs_no_improve = 0\n",
    "        early_stop_epochs = 5\n",
    "        num_epochs = num_epochs\n",
    "\n",
    "        # Lists to store metrics for plotting\n",
    "        epoch_losses = []\n",
    "        epoch_win_rates = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            avg_loss = train_one_epoch(model, train_loader, \\\n",
    "                optimizer, loss_function, device)\n",
    "\n",
    "            epoch_losses.append(avg_loss)  # Store loss for this epoch\n",
    "\n",
    "            print(f\"Epoch {epoch}: Loss {avg_loss}\")\n",
    "            \n",
    "            win_rate = validate_model(model, val_loader, device)\n",
    "\n",
    "            epoch_win_rates.append(win_rate)  # Store win rate for this epoch\n",
    "\n",
    "            print(f\"Epoch {epoch}/{num_epochs}: Train Loss {avg_loss}, Win Rate {win_rate}\")\n",
    "\n",
    "            # Store metrics\n",
    "            epoch_losses.append(avg_loss)\n",
    "            epoch_win_rates.append(win_rate)\n",
    "\n",
    "            scheduler.step(win_rate)\n",
    "\n",
    "            # Gradient clipping\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            if win_rate > best_win_rate:\n",
    "                best_win_rate = win_rate\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve == early_stop_epochs:\n",
    "                break  # Early stopping\n",
    "        \n",
    "        \n",
    "        # Plotting after each fold\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Plot Loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, label='Loss')  # Ensure correct range\n",
    "        plt.title(f'Fold {fold_number} - Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'./plots/trial_{trial.number}/fold_{fold_number}_loss.png')\n",
    "\n",
    "        # Plot Win Rate\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(1, len(epoch_win_rates) + 1), epoch_win_rates, label='Win Rate')  # Ensure correct range\n",
    "        plt.title(f'Fold {fold_number} - Validation Win Rate')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Win Rate')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'./plots/trial_{trial.number}/fold_{fold_number}_win_rate.png')\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        fold_number += 1\n",
    "\n",
    "        total_win_rate += best_win_rate\n",
    "\n",
    "    average_win_rate = total_win_rate / kf.get_n_splits()\n",
    "\n",
    "    print(f\"Avg win rate: \", average_win_rate)\n",
    "    average_win_rate = 0\n",
    "    return average_win_rate\n",
    "\n",
    "# Define your dataset, input_size, output_size, device, num_epochs\n",
    "dataset = HangmanDataset(all_features_tensor, labels_tensor, original_words)\n",
    "\n",
    "input_size = 4  # Set your input size\n",
    "output_size = 27  # Set your output size\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Choose device\n",
    "num_epochs = 10\n",
    "n_trials = 1\n",
    "\n",
    "# Create and run the Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda trial: objective(trial, dataset, \\\n",
    "    input_size, output_size, num_epochs, device), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the best hyperparameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"Value: {trial.value}\")\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
