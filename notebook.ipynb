{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "import sys\n",
    "# Custom library paths\n",
    "sys.path.extend(['../', './scr'])\n",
    "\n",
    "from scr.utils import set_seed\n",
    "from scr.utils import read_words\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Reading and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.feature_engineering import build_feature_set, \\\n",
    "    process_single_word_inference\n",
    "from scr.utils import *\n",
    "\n",
    "import random\n",
    "\n",
    "# MASK_PROB = 0.5\n",
    "\n",
    "# Limit the number of words to a smaller number for debugging\n",
    "word_list = read_words('/home/sayem/Desktop/Hangman/words_250000_train.txt', limit=None)\n",
    "# word_list = word_list[:100]\n",
    "# # # Randomly select 1000 words\n",
    "# # unseen_words = random.sample(word_list, 1000)c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scr.feature_engineering import add_features_for_training, calculate_char_frequencies\n",
    "import random\n",
    "# # Initialize lists for features, labels, missed characters, and original words\n",
    "# all_features, all_labels, all_missed_chars, original_words = [], [], [], []\n",
    "\n",
    "char_frequency = calculate_char_frequencies(word_list)\n",
    "max_word_length = max(len(word) for word in word_list)\n",
    "MASK_PROB = 0.8\n",
    "NGRAM_N = 3\n",
    "\n",
    "feature_set, label, missed_chars = add_features_for_training(\n",
    "        word_list[0], char_frequency, max_word_length, MASK_PROB, NGRAM_N)\n",
    "\n",
    "features, labels, missed_chars_list, original_words = [], [], [], []\n",
    "\n",
    "for word in word_list:\n",
    "    # Process each word to get its features, label, and missed characters\n",
    "    feature_set, label, missed_chars = add_features_for_training(\n",
    "        word, char_frequency, max_word_length, MASK_PROB, NGRAM_N\n",
    "    )\n",
    "\n",
    "    # Add features and labels to the lists without squeezing\n",
    "    features.append(feature_set)\n",
    "    labels.append(torch.tensor(label, dtype=torch.float))\n",
    "    missed_chars_list.append(missed_chars)\n",
    "    original_words.append(word)  # Store the original word\n",
    "\n",
    "# # Convert lists to tensors\n",
    "# all_features_tensor = [features.squeeze(0) for features in all_features]  # Remove batch dimension\n",
    "# labels_tensor = [label.squeeze(0) for label in all_labels]  # Remove batch dimension\n",
    "# missed_chars_tensor = [missed_chars.squeeze(0) for missed_chars in all_missed_chars]  # Remove batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.feature_engineering import process_single_word_inference\n",
    "\n",
    "def process_inference_word(word, char_frequency, max_word_length, ngram_n=2):\n",
    "    feature_set, missed_chars = process_single_word_inference(word, \\\n",
    "        char_frequency, max_word_length, ngram_n=ngram_n)\n",
    "    return feature_set.squeeze(0), missed_chars.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "# Example usage\n",
    "inference_word = \"_p_\"\n",
    "inference_features, inference_missed_chars = \\\n",
    "    process_inference_word(inference_word, char_frequency, max_word_length, ngram_n=NGRAM_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.1034,  0.0000,  0.0000,  0.0000],\n",
       "        [16.0000,  0.1034,  0.0345,  0.2814, 16.0000],\n",
       "        [ 0.0000,  0.1034,  0.0690,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_missed_chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227300"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227300"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scr.dataset import HangmanDataset, collate_fn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "dataset = HangmanDataset(features, \\\n",
    "    labels, missed_chars_list, original_words)\n",
    "\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=32, \\\n",
    "    shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Example of iterating over the DataLoader\n",
    "i = 0\n",
    "for i, batch in enumerate(data_loader):\n",
    "    inputs, labels, miss_chars, lengths, original_words = batch\n",
    "    i += 1\n",
    "\n",
    "    # print(inputs.shape)\n",
    "    # print(labels.shape)\n",
    "    # print(missed_chars.shape)\n",
    "    # print(lengths.shape)\n",
    "    # print()\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your main script or Jupyter Notebook\n",
    "from scr.model import RNN\n",
    "from scr.feature_engineering import process_single_word_inference, \\\n",
    "    char_to_idx, idx_to_char, calculate_char_frequencies, \\\n",
    "        get_missed_characters\n",
    "from scr.game import simulate_game, \\\n",
    "    predict_next_character\n",
    "\n",
    "# Configuration for the RNN model\n",
    "# Configuration for the RNN model\n",
    "config = {\n",
    "    'rnn': 'LSTM',\n",
    "    'vocab_size': 27,  # Assuming 26 letters + 1 for underscore\n",
    "    'hidden_dim': 256,\n",
    "    'num_layers': 2,\n",
    "    'embedding_dim': 200,\n",
    "    'output_mid_features': 100,\n",
    "    'miss_linear_dim': 100,\n",
    "    'dropout': 0.7,\n",
    "    'use_embedding': True,\n",
    "    'lr': 0.00001,\n",
    "    'input_feature_size': 5 # Number of features excluding the embedding dimension\n",
    "}\n",
    "\n",
    "# Initialize RNN model\n",
    "model = RNN(config)\n",
    "model = model.to(device)\n",
    "\n",
    "model.save_model('models/model.pth') \n",
    "\n",
    "# Prepare your dataset, train the model, etc.\n",
    "\n",
    "# # Example of using predict_next_character in a game scenario\n",
    "# word = \"apple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_masked_word = \"_ppl_\"\n",
    "\n",
    "# missed_chars = get_missed_characters(word, char_to_idx)\n",
    "\n",
    "predicted_char = predict_next_character(model, \\\n",
    "    current_masked_word, \\\n",
    "    char_frequency, max_word_length, \\\n",
    "        device=device, use_initial_guess=True)\n",
    "\n",
    "# predicted_index = predict_next_character_beam_search(model, current_masked_word, \\\n",
    "#     missed_chars, \\\n",
    "#     char_frequency, max_word_length, \\\n",
    "#     device, normalize=True, beam_width=3)\n",
    "        \n",
    "# predicted_char = idx_to_char[predicted_index]\n",
    "\n",
    "predicted_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "\n",
    "# from scr.game import predict_next_character, simulate_game\n",
    "# from scr.model import RNN\n",
    "# import random\n",
    "# # random.seed(400)\n",
    "# # Your existing code for initializing the model, etc.\n",
    "\n",
    "# import random\n",
    "\n",
    "# def play_multiple_games(model, num_games, word_list, \\\n",
    "#     char_to_idx, idx_to_char, char_frequency, max_word_length, device):\n",
    "#     game_results = []\n",
    "#     sampled_words = random.sample(word_list, num_games)  # Select unique words\n",
    "\n",
    "#     for random_word in sampled_words:\n",
    "#         with torch.no_grad():\n",
    "#             won, final_word, attempts_used = simulate_game(\n",
    "#                 model, \n",
    "#                 random_word, \n",
    "#                 char_to_idx, \n",
    "#                 idx_to_char, \n",
    "#                 char_frequency, \n",
    "#                 max_word_length, \n",
    "#                 device, \n",
    "#                 normalize=True, \n",
    "#                 max_attempts=6\n",
    "#             )\n",
    "#         game_results.append((won, final_word, attempts_used))\n",
    "\n",
    "#     return game_results\n",
    "\n",
    "# # Example usage\n",
    "# num_games = 10**4\n",
    "# results = play_multiple_games(model, num_games, \\\n",
    "#     word_list, char_to_idx, idx_to_char, \\\n",
    "#         char_frequency, max_word_length, device)\n",
    "\n",
    "# # Analyzing results\n",
    "# total_wins = sum(result[0] for result in results)\n",
    "# win_rate = (total_wins / num_games) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, device=device):\n",
    "    total_actual_penalty = 0\n",
    "    total_miss_penalty = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        inputs, labels, miss_chars, lengths, original_words = batch\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        miss_chars = miss_chars.to(device)\n",
    "        lengths = lengths # .to(device)\n",
    "\n",
    "        # print(f\"Batch {i}: Inputs Shape: {inputs.shape}, Labels Shape: \\\n",
    "        # {labels.shape}, Lengths: {lengths.shape}, Miss Chars: {miss_chars.shape}\")\n",
    "\n",
    "        # # Run the model\n",
    "        outputs = model(inputs, lengths, miss_chars)\n",
    "        # print(f'NN output: {outputs.shape}')\n",
    "\n",
    "       \n",
    "        # # Calculate the custom loss\n",
    "        actual_penalty, miss_penalty = model.calculate_loss(outputs, \\\n",
    "            labels, lengths, miss_chars, vocab_size=27, use_cuda=True)\n",
    "\n",
    "        # # print(actual_penalty)\n",
    "        # # print(miss_penalty)\n",
    "\n",
    "        total_actual_penalty += actual_penalty.item()\n",
    "        total_miss_penalty += miss_penalty.item()\n",
    "        total_batches += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        actual_penalty.backward()  # Backpropagation for the actual_penalty\n",
    "        optimizer.step()\n",
    "        # break\n",
    "\n",
    "    avg_actual_penalty = total_actual_penalty / total_batches if total_batches > 0 else 0\n",
    "    avg_miss_penalty = total_miss_penalty / total_batches if total_batches > 0 else 0\n",
    "    return avg_actual_penalty, avg_miss_penalty\n",
    "\n",
    "\n",
    "import random\n",
    "import collections\n",
    "\n",
    "def validate_one_epoch(model, val_loader, char_to_idx, idx_to_char, \\\n",
    "    char_frequency, max_word_length, device=device, max_games_per_epoch=10 ** 4):\n",
    "    model.eval()\n",
    "    total_wins = 0\n",
    "    total_attempts = 0\n",
    "    total_games = 0\n",
    "\n",
    "    win_count_by_length = collections.defaultdict(int)\n",
    "    game_count_by_length = collections.defaultdict(int)\n",
    "\n",
    "    # Collect all words from the validation loader\n",
    "    all_words = []\n",
    "    for batch in val_loader:\n",
    "        batch_original_words = batch[-1]  # Adjust according to your batch structure\n",
    "        all_words.extend(batch_original_words)\n",
    "\n",
    "    # Randomly sample a set number of words for this epoch's validation\n",
    "    selected_words = random.sample(all_words, min(max_games_per_epoch, len(all_words)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for word in selected_words:\n",
    "            won, final_word, attempts_used = simulate_game(\n",
    "                model, \n",
    "                word, \n",
    "                char_to_idx, \n",
    "                idx_to_char, \n",
    "                char_frequency, \n",
    "                max_word_length, \n",
    "                device, \n",
    "                normalize=True, \n",
    "                max_attempts=6\n",
    "            )\n",
    "            total_wins += int(won)\n",
    "            total_attempts += attempts_used\n",
    "            total_games += 1\n",
    "\n",
    "            word_length = len(word)\n",
    "            win_count_by_length[word_length] += int(won)\n",
    "            game_count_by_length[word_length] += 1\n",
    "\n",
    "    win_rate = (total_wins / total_games) * 100 if total_games > 0 else 0\n",
    "    average_attempts = total_attempts / total_games if total_games > 0 else 0\n",
    "    win_rate_by_length = {length: (win_count_by_length[length] / game_count_by_length[length]) \n",
    "                          for length in game_count_by_length}\n",
    "\n",
    "    return win_rate, average_attempts, win_rate_by_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "# num_epochs = 20\n",
    "\n",
    "# avg_actual_penalty, avg_miss_penalty = train_one_epoch(model, data_loader, optimizer, \\\n",
    "#         device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Training loop\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "num_epochs = 25\n",
    "\n",
    "# Data Loaders\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training step\n",
    "#     avg_actual_penalty, avg_miss_penalty = train_one_epoch(model, train_loader, optimizer, \\\n",
    "#         device)\n",
    "#     print(f\"Epoch {epoch+1}: Training - Avg Actual Penalty: {avg_actual_penalty}, Avg Miss Penalty: {avg_miss_penalty}\")\n",
    "\n",
    "#     # Validation step\n",
    "#     win_rate, average_attempts, win_rate_by_length = validate_one_epoch(model, \\\n",
    "#         val_loader, char_to_idx, idx_to_char, \\\n",
    "#     char_frequency, max_word_length, device)\n",
    "#     print(f\"Epoch {epoch+1}: Validation - Accuracy: {win_rate}%, win_rate_by_length: {average_attempts}, win_rate_by_length: {win_rate_by_length}\")\n",
    "\n",
    "#     print()\n",
    "\n",
    "# win_rate, average_attempts, win_rate_by_length = validate_one_epoch(model, val_loader, \\\n",
    "#     char_to_idx, idx_to_char, char_frequency, max_word_length, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from scr.model import RNN\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from scr.utils import *\n",
    "from scr.early_stopping import EarlyStopping\n",
    "\n",
    "def objective(trial, dataset, static_config, num_epochs):\n",
    "    dynamic_config = optuna_dynamic_hyperparameters(trial)\n",
    "    config = {**static_config, **dynamic_config}\n",
    "    # print(f\"Trial {trial.number}: Configuration - {config}\")\n",
    "\n",
    "    kfold = KFold(n_splits=5, shuffle=True)\n",
    "    aggregated_metrics = []\n",
    "\n",
    "    best_objective_value = float('-inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        # print(f\"Starting Fold {fold + 1}\")\n",
    "        train_loader, val_loader = get_data_loaders(dataset, train_idx, val_idx)\n",
    "        model, optimizer = initialize_model(config)\n",
    "\n",
    "        scheduler = StepLR(optimizer, step_size=trial.suggest_int(\"step_size\", 5, 20), \\\n",
    "            gamma=trial.suggest_float(\"gamma\", 0.1, 0.5))\n",
    "        \n",
    "        early_stopping = EarlyStopping(patience=10, delta=0.001)  # Early stopping instance\n",
    "\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            _, _ = train_one_epoch(model, train_loader, optimizer)\n",
    "            scheduler.step()\n",
    "\n",
    "            win_rate, average_attempts, win_rate_by_length = validate_one_epoch(model, \\\n",
    "                val_loader, char_to_idx, idx_to_char, char_frequency, max_word_length, device)\n",
    "\n",
    "            aggregated_metrics.append((win_rate, average_attempts, win_rate_by_length))\n",
    "\n",
    "            objective_value = calculate_objective_value(win_rate, average_attempts, win_rate_by_length)\n",
    "            \n",
    "            trial.report(objective_value, epoch)\n",
    "\n",
    "            if objective_value > best_objective_value:\n",
    "                best_objective_value = objective_value\n",
    "                best_model_state = model.state_dict()  # Update best model state\n",
    "\n",
    "            # Check early stopping\n",
    "            if early_stopping(objective_value):\n",
    "                break  # Stop the current fold if early stopping criteria are met\n",
    "\n",
    "            # Pruning check\n",
    "            if trial.should_prune():\n",
    "                return process_aggregated_metrics(aggregated_metrics)\n",
    "\n",
    "    # Save the best model at the end of the trial\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        model.save_model()  # Save the model using your custom method\n",
    "\n",
    "    final_objective_value = process_aggregated_metrics(aggregated_metrics)\n",
    "    \n",
    "    return final_objective_value\n",
    "\n",
    "\n",
    "def calculate_objective_value(win_rate_percentage, avg_attempts_per_game, \\\n",
    "    win_rate_by_word_length):\n",
    "    # Define weights for each metric\n",
    "    weight_for_win_rate = 0.7  # Higher weight for win rate\n",
    "    weight_for_average_attempts = 0.3  # Lower weight for average attempts\n",
    "\n",
    "    # Normalize win rate (converting percentage to a range of 0 to 1)\n",
    "    normalized_win_rate = win_rate_percentage / 100\n",
    "\n",
    "    # Normalize average attempts (assuming max attempts per game is known)\n",
    "    max_attempts_per_game = 6  # Example: maximum number of attempts allowed\n",
    "    normalized_avg_attempts = avg_attempts_per_game / max_attempts_per_game\n",
    "\n",
    "    # Normalize win rate by word length\n",
    "    normalized_win_rate_by_length = normalize_win_rate_by_length(win_rate_by_word_length)\n",
    "\n",
    "    # Combine metrics into a single objective value\n",
    "    objective_value = (weight_for_win_rate * normalized_win_rate) - \\\n",
    "                      (weight_for_average_attempts * normalized_avg_attempts) + \\\n",
    "                      normalized_win_rate_by_length\n",
    "    return objective_value\n",
    "\n",
    "def normalize_win_rate_by_length(win_rate_by_length_dict):\n",
    "    # Example logic for normalization\n",
    "    max_possible_win_rate = 1  # Maximum win rate (100% as a decimal)\n",
    "    normalized_scores = {length: win_rate / max_possible_win_rate for \\\n",
    "        length, win_rate in win_rate_by_length_dict.items()}\n",
    "    # Average the normalized win rates across all word lengths\n",
    "    average_normalized_score = sum(normalized_scores.values()) / len(normalized_scores)\n",
    "    return average_normalized_score\n",
    "\n",
    "def process_aggregated_metrics(aggregated_metrics):\n",
    "    # Initialize sums for each metric\n",
    "    sum_win_rate = 0\n",
    "    sum_average_attempts = 0\n",
    "    sum_win_rate_by_length = {}\n",
    "\n",
    "    # Process each set of metrics\n",
    "    for win_rate, average_attempts, win_rate_by_length in aggregated_metrics:\n",
    "        sum_win_rate += win_rate\n",
    "        sum_average_attempts += average_attempts\n",
    "        for length, rate in win_rate_by_length.items():\n",
    "            sum_win_rate_by_length[length] = sum_win_rate_by_length.get(length, 0) + rate\n",
    "\n",
    "    # Calculate averages\n",
    "    num_entries = len(aggregated_metrics)\n",
    "    avg_win_rate = sum_win_rate / num_entries\n",
    "    avg_average_attempts = sum_average_attempts / num_entries\n",
    "    avg_win_rate_by_length = {length: rate / num_entries for length, rate in sum_win_rate_by_length.items()}\n",
    "\n",
    "    # Now calculate the final objective value using these averages\n",
    "    final_objective_value = calculate_objective_value(avg_win_rate, \\\n",
    "        avg_average_attempts, avg_win_rate_by_length)\n",
    "    return final_objective_value\n",
    "\n",
    "\n",
    "# Utility functions (for cleaner code)\n",
    "def get_data_loaders(dataset, train_idx, val_idx):\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, \\\n",
    "        shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, \\\n",
    "        shuffle=False, collate_fn=collate_fn)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def initialize_model(config):\n",
    "    model = RNN(config)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    return model, optimizer\n",
    "\n",
    "# Utility functions\n",
    "def optuna_dynamic_hyperparameters(trial):\n",
    "    # Define and return dynamic hyperparameters based on the trial\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [128, 256, 512])\n",
    "    embedding_dim = trial.suggest_categorical('embedding_dim', [50, 100, 150])\n",
    "    output_mid_features = trial.suggest_categorical('output_mid_features', [50, 100, 200])\n",
    "    miss_linear_dim = trial.suggest_categorical('miss_linear_dim', [50, 100, 150])\n",
    "    dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "\n",
    "    return {\n",
    "        'lr': lr,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'embedding_dim': embedding_dim,\n",
    "        'output_mid_features': output_mid_features,\n",
    "        'miss_linear_dim': miss_linear_dim,\n",
    "        'dropout': dropout,\n",
    "        'num_layers': num_layers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-16 14:59:52,687] A new study created in memory with name: no-name-b5a017de-bc10-45f8-866b-b4c58de5b628\n",
      "[I 2023-11-16 17:18:20,986] Trial 0 finished with value: -0.26674618565818786 and parameters: {'lr': 0.00017445130534251728, 'hidden_dim': 128, 'embedding_dim': 100, 'output_mid_features': 50, 'miss_linear_dim': 100, 'dropout': 0.23334078392375454, 'num_layers': 3, 'step_size': 7, 'gamma': 0.16042801409919455}. Best is trial 0 with value: -0.26674618565818786.\n",
      "[I 2023-11-16 19:39:59,921] Trial 1 finished with value: -0.25860093296700803 and parameters: {'lr': 1.3683399679827584e-05, 'hidden_dim': 512, 'embedding_dim': 50, 'output_mid_features': 100, 'miss_linear_dim': 150, 'dropout': 0.32398449126972173, 'num_layers': 2, 'step_size': 12, 'gamma': 0.4037849211832595}. Best is trial 1 with value: -0.25860093296700803.\n",
      "[I 2023-11-16 22:53:30,961] Trial 2 finished with value: -0.26295785259206866 and parameters: {'lr': 2.2110380911531494e-05, 'hidden_dim': 512, 'embedding_dim': 150, 'output_mid_features': 100, 'miss_linear_dim': 50, 'dropout': 0.35792539552039526, 'num_layers': 3, 'step_size': 10, 'gamma': 0.4716124301113982}. Best is trial 1 with value: -0.25860093296700803.\n",
      "[I 2023-11-17 00:23:16,607] Trial 3 finished with value: -0.2617888932944841 and parameters: {'lr': 0.0008049971276778388, 'hidden_dim': 256, 'embedding_dim': 50, 'output_mid_features': 50, 'miss_linear_dim': 50, 'dropout': 0.27079273824529676, 'num_layers': 1, 'step_size': 13, 'gamma': 0.2579323802231869}. Best is trial 1 with value: -0.25860093296700803.\n",
      "[I 2023-11-17 02:14:16,069] Trial 4 finished with value: -0.26303165450007726 and parameters: {'lr': 2.9460783912149277e-05, 'hidden_dim': 128, 'embedding_dim': 100, 'output_mid_features': 100, 'miss_linear_dim': 50, 'dropout': 0.4106907713111455, 'num_layers': 2, 'step_size': 13, 'gamma': 0.23766465918001517}. Best is trial 1 with value: -0.25860093296700803.\n",
      "[I 2023-11-17 02:16:28,855] Trial 5 finished with value: -0.2661845676903731 and parameters: {'lr': 0.0003246954602041393, 'hidden_dim': 512, 'embedding_dim': 50, 'output_mid_features': 100, 'miss_linear_dim': 150, 'dropout': 0.3418469285169399, 'num_layers': 3, 'step_size': 7, 'gamma': 0.3322701898597382}. Best is trial 1 with value: -0.25860093296700803.\n",
      "[I 2023-11-17 02:17:38,223] Trial 6 finished with value: -0.2716372844855698 and parameters: {'lr': 1.2623511540680878e-05, 'hidden_dim': 128, 'embedding_dim': 100, 'output_mid_features': 100, 'miss_linear_dim': 150, 'dropout': 0.3638386791151357, 'num_layers': 1, 'step_size': 16, 'gamma': 0.48934668983711216}. Best is trial 1 with value: -0.25860093296700803.\n",
      "[I 2023-11-17 03:54:46,757] Trial 7 finished with value: -0.2619856247748265 and parameters: {'lr': 1.3794059109476859e-05, 'hidden_dim': 512, 'embedding_dim': 150, 'output_mid_features': 100, 'miss_linear_dim': 50, 'dropout': 0.3763499502656833, 'num_layers': 1, 'step_size': 13, 'gamma': 0.4582630743800198}. Best is trial 1 with value: -0.25860093296700803.\n",
      "[I 2023-11-17 03:56:08,084] Trial 8 finished with value: -0.29555884206349964 and parameters: {'lr': 0.0006776370286183873, 'hidden_dim': 128, 'embedding_dim': 50, 'output_mid_features': 200, 'miss_linear_dim': 50, 'dropout': 0.3560229695825034, 'num_layers': 2, 'step_size': 5, 'gamma': 0.28793224100199166}. Best is trial 1 with value: -0.25860093296700803.\n",
      "[I 2023-11-17 03:57:13,282] Trial 9 finished with value: -0.27814226511314244 and parameters: {'lr': 2.1810957567994742e-05, 'hidden_dim': 128, 'embedding_dim': 150, 'output_mid_features': 200, 'miss_linear_dim': 150, 'dropout': 0.45356566227508743, 'num_layers': 1, 'step_size': 19, 'gamma': 0.10825494733614108}. Best is trial 1 with value: -0.25860093296700803.\n",
      "[I 2023-11-17 03:58:36,901] Trial 10 finished with value: -0.2787375625326 and parameters: {'lr': 5.737387417771179e-05, 'hidden_dim': 256, 'embedding_dim': 50, 'output_mid_features': 50, 'miss_linear_dim': 100, 'dropout': 0.4972746058855896, 'num_layers': 2, 'step_size': 17, 'gamma': 0.39403313279010677}. Best is trial 1 with value: -0.25860093296700803.\n",
      "[I 2023-11-17 04:01:02,012] Trial 11 finished with value: -0.25656562480061196 and parameters: {'lr': 0.00095353278935304, 'hidden_dim': 256, 'embedding_dim': 50, 'output_mid_features': 50, 'miss_linear_dim': 150, 'dropout': 0.2817129153138427, 'num_layers': 1, 'step_size': 11, 'gamma': 0.36587639067228106}. Best is trial 11 with value: -0.25656562480061196.\n",
      "[I 2023-11-17 04:02:29,876] Trial 12 finished with value: -0.2695756939813438 and parameters: {'lr': 8.558486083153871e-05, 'hidden_dim': 256, 'embedding_dim': 50, 'output_mid_features': 50, 'miss_linear_dim': 150, 'dropout': 0.2938637551333714, 'num_layers': 2, 'step_size': 10, 'gamma': 0.36943107235374417}. Best is trial 11 with value: -0.25656562480061196.\n",
      "[I 2023-11-17 05:48:00,549] Trial 13 finished with value: -0.26100880193722276 and parameters: {'lr': 0.0001781175434116562, 'hidden_dim': 256, 'embedding_dim': 50, 'output_mid_features': 50, 'miss_linear_dim': 150, 'dropout': 0.20809104893954322, 'num_layers': 1, 'step_size': 11, 'gamma': 0.4134965271647858}. Best is trial 11 with value: -0.25656562480061196.\n",
      "[I 2023-11-17 05:49:27,591] Trial 14 finished with value: -0.27239811780154954 and parameters: {'lr': 4.1518989595697106e-05, 'hidden_dim': 512, 'embedding_dim': 50, 'output_mid_features': 200, 'miss_linear_dim': 150, 'dropout': 0.2941263505718762, 'num_layers': 2, 'step_size': 15, 'gamma': 0.34188180334685764}. Best is trial 11 with value: -0.25656562480061196.\n",
      "[I 2023-11-17 07:57:18,660] Trial 15 finished with value: -0.2628475153342909 and parameters: {'lr': 7.08141689378063e-05, 'hidden_dim': 256, 'embedding_dim': 50, 'output_mid_features': 100, 'miss_linear_dim': 150, 'dropout': 0.26407616708564097, 'num_layers': 2, 'step_size': 8, 'gamma': 0.41966260919665455}. Best is trial 11 with value: -0.25656562480061196.\n",
      "[I 2023-11-17 07:59:38,653] Trial 16 finished with value: -0.25200344544870795 and parameters: {'lr': 1.0187940470581784e-05, 'hidden_dim': 512, 'embedding_dim': 50, 'output_mid_features': 50, 'miss_linear_dim': 100, 'dropout': 0.30445129196460996, 'num_layers': 1, 'step_size': 9, 'gamma': 0.33676963790615766}. Best is trial 16 with value: -0.25200344544870795.\n",
      "[I 2023-11-17 08:02:08,655] Trial 17 finished with value: -0.26546701110012455 and parameters: {'lr': 0.0001440935231588517, 'hidden_dim': 512, 'embedding_dim': 50, 'output_mid_features': 50, 'miss_linear_dim': 100, 'dropout': 0.3195802218480139, 'num_layers': 1, 'step_size': 9, 'gamma': 0.318538244693987}. Best is trial 16 with value: -0.25200344544870795.\n",
      "[I 2023-11-17 08:03:21,018] Trial 18 finished with value: -0.2733037666901175 and parameters: {'lr': 0.0004181747127273326, 'hidden_dim': 256, 'embedding_dim': 100, 'output_mid_features': 50, 'miss_linear_dim': 100, 'dropout': 0.2501851485340332, 'num_layers': 1, 'step_size': 5, 'gamma': 0.355575019390143}. Best is trial 16 with value: -0.25200344544870795.\n",
      "[I 2023-11-17 08:07:51,495] Trial 19 finished with value: -0.2592224638039725 and parameters: {'lr': 0.00011345699570716355, 'hidden_dim': 256, 'embedding_dim': 150, 'output_mid_features': 50, 'miss_linear_dim': 100, 'dropout': 0.3005407138408419, 'num_layers': 1, 'step_size': 15, 'gamma': 0.29463897606685496}. Best is trial 16 with value: -0.25200344544870795.\n",
      "[I 2023-11-17 08:09:04,107] Trial 20 finished with value: -0.26894941887292656 and parameters: {'lr': 0.0009554195945838521, 'hidden_dim': 512, 'embedding_dim': 50, 'output_mid_features': 50, 'miss_linear_dim': 100, 'dropout': 0.2081077017125793, 'num_layers': 1, 'step_size': 20, 'gamma': 0.37071510485106124}. Best is trial 16 with value: -0.25200344544870795.\n",
      "[I 2023-11-17 08:10:34,465] Trial 21 finished with value: -0.27213856392766317 and parameters: {'lr': 1.3985121594972915e-05, 'hidden_dim': 512, 'embedding_dim': 50, 'output_mid_features': 50, 'miss_linear_dim': 150, 'dropout': 0.32685459362513647, 'num_layers': 2, 'step_size': 11, 'gamma': 0.4347298407397936}. Best is trial 16 with value: -0.25200344544870795.\n",
      "[I 2023-11-17 08:12:31,153] Trial 22 finished with value: -0.2682483573904459 and parameters: {'lr': 1.1514487222704365e-05, 'hidden_dim': 512, 'embedding_dim': 50, 'output_mid_features': 100, 'miss_linear_dim': 150, 'dropout': 0.3113876085756824, 'num_layers': 3, 'step_size': 12, 'gamma': 0.3899893965174772}. Best is trial 16 with value: -0.25200344544870795.\n",
      "[I 2023-11-17 08:14:51,056] Trial 23 finished with value: -0.26830640196461447 and parameters: {'lr': 1.0094028738336599e-05, 'hidden_dim': 512, 'embedding_dim': 50, 'output_mid_features': 200, 'miss_linear_dim': 150, 'dropout': 0.27829980308229924, 'num_layers': 1, 'step_size': 9, 'gamma': 0.4416655736479676}. Best is trial 16 with value: -0.25200344544870795.\n",
      "[I 2023-11-17 10:24:25,733] Trial 24 finished with value: -0.26593909066666355 and parameters: {'lr': 3.954346259104914e-05, 'hidden_dim': 512, 'embedding_dim': 50, 'output_mid_features': 50, 'miss_linear_dim': 100, 'dropout': 0.3279967135638548, 'num_layers': 2, 'step_size': 12, 'gamma': 0.3877104121372656}. Best is trial 16 with value: -0.25200344544870795.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define your models directory path\n",
    "models_dir = Path('models')\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Assuming missed_chars_tensor is a list of tensors for missed characters\n",
    "# dataset = HangmanDataset(features, \\\n",
    "#     labels, missed_chars, original_words)\n",
    "\n",
    "# Static configuration\n",
    "static_config = {\n",
    "    'rnn': 'LSTM',\n",
    "    'vocab_size': 27,  # 26 English alphabets + 1 (e.g., for underscore)\n",
    "    'use_embedding': True,  # Typically a design choice\n",
    "    'input_feature_size': 5,  # Based on your feature engineering strategy\n",
    "    'models': str(models_dir)\n",
    "}\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define num_epochs for training in each fold\n",
    "num_epochs = 20 # Adjust as needed\n",
    "N_TRIAL = 25\n",
    "\n",
    "# ====================================================== #\n",
    "# Define the direction for the objective: 'maximize' or 'minimize'\n",
    "direction = 'maximize'  # Adjust based on your objective function\n",
    "\n",
    "study = optuna.create_study(direction=direction)\n",
    "study.optimize(lambda trial: objective(trial, dataset, \\\n",
    "    static_config, num_epochs), n_trials=N_TRIAL)\n",
    "# ====================================================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "Value: -0.25200344544870795\n",
      "Params: \n",
      "    lr: 1.0187940470581784e-05\n",
      "    hidden_dim: 512\n",
      "    embedding_dim: 50\n",
      "    output_mid_features: 50\n",
      "    miss_linear_dim: 100\n",
      "    dropout: 0.30445129196460996\n",
      "    num_layers: 1\n",
      "    step_size: 9\n",
      "    gamma: 0.33676963790615766\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "# Output the best hyperparameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"Value: {trial.value}\")\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial number: 16\n"
     ]
    }
   ],
   "source": [
    "best_trial = study.best_trial\n",
    "print(f\"Best trial number: {best_trial.number}\")\n",
    "\n",
    "# Assuming model saving includes the trial number in the filename\n",
    "best_model_filename = 'models/trial_1/LSTM.pth' # f\"models/trial_{best_trial.number+1}/LSTM.pth\"\n",
    "best_model = RNN.load_model(RNN, best_model_filename, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dictionaries to form a single configuration\n",
    "config = {**static_config, **best_params}\n",
    "\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Assuming static_config and best_params are defined dictionaries\n",
    "config = {**static_config, **best_params}\n",
    "\n",
    "# Define the folder path using pathlib\n",
    "folder_path = Path(\"models/trial_1\")  # Replace with your actual folder path\n",
    "\n",
    "# Ensure the folder exists\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the file path for the YAML file\n",
    "file_path = folder_path / \"config.yaml\"\n",
    "\n",
    "# Write the configuration to the YAML file\n",
    "with open(file_path, 'w') as file:\n",
    "    yaml.dump(config, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(28, 50)\n",
       "  (linear1_out): Linear(in_features=1124, out_features=50, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (linear2_out): Linear(in_features=50, out_features=27, bias=True)\n",
       "  (miss_linear): Linear(in_features=27, out_features=100, bias=True)\n",
       "  (rnn): LSTM(54, 512, num_layers=2, batch_first=True, dropout=0.3279967135638548, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from scr.model import RNN  # Assuming RNN is your model class\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the folder path and filenames\n",
    "folder_path = Path(\"models/trial_1/\")\n",
    "model_filename = folder_path / \"LSTM.pth\"\n",
    "\n",
    "# Load the saved file, which contains more than just the state dictionary\n",
    "checkpoint = torch.load(model_filename, map_location=torch.device('cpu'))\n",
    "\n",
    "# Extract the state dictionary for the model\n",
    "model_state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# Initialize the RNN model (ensure config is loaded or defined)\n",
    "# Assuming you have the configuration dictionary for initializing RNN\n",
    "config = checkpoint['config']  # Or load config from another source if needed\n",
    "model = RNN(config)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(model_state_dict)\n",
    "model = model.to(torch.device('cpu'))\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Now your model is loaded with the saved parameters and ready for inference or further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dictionaries to form a single configuration\n",
    "config = {**static_config, **best_params}\n",
    "\n",
    "# Initialize the RNN model with the combined configuration\n",
    "model = RNN(config)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# model.load_model(config['LSTM'], 2, 256, trial_number=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing on unknown data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from scr.game import predict_next_character, simulate_game\n",
    "from scr.model import RNN\n",
    "import random\n",
    "# random.seed(400)\n",
    "# Your existing code for initializing the model, etc.\n",
    "\n",
    "import random\n",
    "\n",
    "def play_multiple_games(model, num_games, word_list, \\\n",
    "    char_to_idx, idx_to_char, char_frequency, max_word_length, device):\n",
    "    game_results = []\n",
    "    \n",
    "    sampled_words = random.sample(word_list, num_games)  # Select unique words\n",
    "\n",
    "    for random_word in sampled_words:\n",
    "        with torch.no_grad():\n",
    "            won, final_word, attempts_used = simulate_game(\n",
    "                model, \n",
    "                random_word, \n",
    "                char_to_idx, \n",
    "                idx_to_char, \n",
    "                char_frequency, \n",
    "                max_word_length, \n",
    "                device, \n",
    "                normalize=True, \n",
    "                max_attempts=6\n",
    "            )\n",
    "        game_results.append((won, final_word, attempts_used))\n",
    "\n",
    "    return game_results\n",
    "\n",
    "# Example usage\n",
    "num_games = 10**3\n",
    "results = play_multiple_games(model, num_games, \\\n",
    "    word_list, char_to_idx, idx_to_char, \\\n",
    "        char_frequency, max_word_length, device)\n",
    "\n",
    "# Analyzing results\n",
    "total_wins = sum(result[0] for result in results)\n",
    "win_rate = (total_wins / num_games) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
