{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a62bd7e6",
   "metadata": {
    "papermill": {
     "duration": 0.002135,
     "end_time": "2023-11-24T22:23:42.367043",
     "exception": false,
     "start_time": "2023-11-24T22:23:42.364908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "323a72c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:23:42.378753Z",
     "iopub.status.busy": "2023-11-24T22:23:42.378580Z",
     "iopub.status.idle": "2023-11-24T22:23:43.346735Z",
     "shell.execute_reply": "2023-11-24T22:23:43.346282Z"
    },
    "papermill": {
     "duration": 0.971674,
     "end_time": "2023-11-24T22:23:43.347691",
     "exception": false,
     "start_time": "2023-11-24T22:23:42.376017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from scr.dataset import *\n",
    "# from scr.game import *\n",
    "from scr.feature_engineering import *\n",
    "# from scr.plot_utils import *\n",
    "import gc\n",
    "from scr.utils import print_scenarios\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from scr.utils import read_words, save_words_to_file\n",
    "\n",
    "import sys\n",
    "# Custom library paths\n",
    "sys.path.extend(['../', './scr'])\n",
    "\n",
    "from scr.utils import set_seed\n",
    "from scr.utils import read_words\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')  # for tensor core\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Read and Shuffle Word List\n",
    "word_list = read_words('data/words_250000_train.txt') # , limit=10000)\n",
    "# word_list = read_words('data/250k.txt', limit=10000)\n",
    "\n",
    "random.shuffle(word_list)\n",
    "\n",
    "# Calculate Frequencies and Max Word Length\n",
    "word_frequencies = calculate_word_frequencies(word_list)\n",
    "char_frequency = calculate_char_frequencies(word_list)\n",
    "max_word_length = max(len(word) for word in word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e2750e",
   "metadata": {},
   "source": [
    "##### Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8b7063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories '/media/sayem/510B93E12554BBD1/dataset/5000/parquets' have been recreated.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "NUM_STRATIFIED_SAMPLES = 5000 # This will be overwritten by Papermill\n",
    "\n",
    "# Define the base directory and the paths for training and validation parquet files\n",
    "base_dataset_dir = Path(\"/media/sayem/510B93E12554BBD1/dataset/\")\n",
    "\n",
    "stratified_samples_dir = base_dataset_dir / str(NUM_STRATIFIED_SAMPLES)\n",
    "\n",
    "parquet_path = stratified_samples_dir / 'parquets'\n",
    "# parquet_valid_path = stratified_samples_dir / 'valid_parquets'\n",
    "\n",
    "# Function to delete and recreate a directory\n",
    "def recreate_directory(path):\n",
    "    if path.exists():\n",
    "        shutil.rmtree(path)  # Delete the directory and its contents\n",
    "    path.mkdir(parents=True)  # Create the directory\n",
    "\n",
    "# Recreate the train and valid directories\n",
    "recreate_directory(parquet_path)\n",
    "# recreate_directory(parquet_valid_path)\n",
    "\n",
    "# print(f\"Directories '{parquet_train_path}' and '{parquet_valid_path}' have been recreated.\")\n",
    "\n",
    "print(f\"Directories '{parquet_path}' have been recreated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8cc199b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:23:43.568919Z",
     "iopub.status.busy": "2023-11-24T22:23:43.568821Z",
     "iopub.status.idle": "2023-11-24T22:23:43.571482Z",
     "shell.execute_reply": "2023-11-24T22:23:43.571208Z"
    },
    "papermill": {
     "duration": 0.007294,
     "end_time": "2023-11-24T22:23:43.572518",
     "exception": false,
     "start_time": "2023-11-24T22:23:43.565224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227300"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc97d62d",
   "metadata": {},
   "source": [
    "##### Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef6ad69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing words saved in /media/sayem/510B93E12554BBD1/dataset/5000/testing_words.txt\n"
     ]
    }
   ],
   "source": [
    "# Define the total number of words and the number of test samples\n",
    "from scr.custom_sampler import *\n",
    "NUM_TEST_SAMPLES = 10_000\n",
    "\n",
    "# Assuming 'word_list' contains the 250,000 words\n",
    "# First, separate 10,000 words for the final testing set\n",
    "testing_words = stratified_sample_by_length_and_uniqueness(\n",
    "    word_list, \n",
    "    NUM_TEST_SAMPLES\n",
    ")\n",
    "\n",
    "# Define the file path for saving the testing words\n",
    "testing_words_file_path = stratified_samples_dir / \"testing_words.txt\"\n",
    "\n",
    "# Save the testing words to a file\n",
    "with open(testing_words_file_path, 'w') as file:\n",
    "    for word in testing_words:\n",
    "        file.write(word + '\\n')\n",
    "\n",
    "print(f\"Testing words saved in {testing_words_file_path}\")\n",
    "\n",
    "# Now, remove these testing samples from the original word list\n",
    "remaining_words = [word for word in word_list if word not in testing_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469a87ce",
   "metadata": {},
   "source": [
    "##### Stratified Sample Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41e486f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:23:43.579494Z",
     "iopub.status.busy": "2023-11-24T22:23:43.579377Z",
     "iopub.status.idle": "2023-11-24T22:23:43.640436Z",
     "shell.execute_reply": "2023-11-24T22:23:43.640019Z"
    },
    "papermill": {
     "duration": 0.065848,
     "end_time": "2023-11-24T22:23:43.641509",
     "exception": false,
     "start_time": "2023-11-24T22:23:43.575661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Statrified samples: 5000\n",
      "5039\n"
     ]
    }
   ],
   "source": [
    "## we are taking starified samples from train_words\n",
    "\n",
    "from scr.custom_sampler import \\\n",
    "    stratified_sample_by_length_and_frequency, \\\n",
    "        stratified_sample_by_length, stratified_sample_by_length_and_uniqueness\n",
    "\n",
    "print(f'Number of Statrified samples: {NUM_STRATIFIED_SAMPLES}')\n",
    "\n",
    "# sampled_words_by_length_and_frequency \\\n",
    "#     = stratified_sample_by_length_and_frequency(train_words, \\\n",
    "#     word_frequencies, \\\n",
    "#     NUM_STRATIFIED_SAMPLES)\n",
    "\n",
    "sampled_words_by_length = stratified_sample_by_length_and_uniqueness(remaining_words, \\\n",
    "    NUM_STRATIFIED_SAMPLES)\n",
    "\n",
    "print(len(sampled_words_by_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0d326f",
   "metadata": {
    "papermill": {
     "duration": 0.004373,
     "end_time": "2023-11-24T22:23:44.300659",
     "exception": false,
     "start_time": "2023-11-24T22:23:44.296286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Intial State Simulation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae53fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.game import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1415477e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'allMasked': '____________',\n",
       " 'early': '____o_o_____',\n",
       " 'quarterRevealed': '__t_o_o_t___',\n",
       " 'midRevealed': '_yt_o_o_ti__',\n",
       " 'midLateRevealed': '_ytho_o_tiz_',\n",
       " 'lateRevealed': 'mytho_oetize',\n",
       " 'nearEnd': 'mytho_oetize'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word = \"mississippi\"\n",
    "word = \"mythopoetize\"\n",
    "# word = \"cat\"\n",
    "\n",
    "initial_states = process_word_for_six_states(word)\n",
    "\n",
    "initial_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c3da54c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:23:44.317155Z",
     "iopub.status.busy": "2023-11-24T22:23:44.317069Z",
     "iopub.status.idle": "2023-11-24T22:23:44.319452Z",
     "shell.execute_reply": "2023-11-24T22:23:44.319138Z"
    },
    "papermill": {
     "duration": 0.005907,
     "end_time": "2023-11-24T22:23:44.320079",
     "exception": false,
     "start_time": "2023-11-24T22:23:44.314172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5cffa5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:23:44.326727Z",
     "iopub.status.busy": "2023-11-24T22:23:44.326580Z",
     "iopub.status.idle": "2023-11-24T22:23:44.329649Z",
     "shell.execute_reply": "2023-11-24T22:23:44.329220Z"
    },
    "papermill": {
     "duration": 0.007091,
     "end_time": "2023-11-24T22:23:44.330204",
     "exception": false,
     "start_time": "2023-11-24T22:23:44.323113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'allMasked': '____________',\n",
       " 'early': '____o_o_____',\n",
       " 'quarterRevealed': '__t_o_o_t___',\n",
       " 'midRevealed': '_yt_o_o_ti__',\n",
       " 'midLateRevealed': '_ytho_o_tiz_',\n",
       " 'lateRevealed': 'mytho_oetize',\n",
       " 'nearEnd': 'mytho_oetize'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01da2152",
   "metadata": {
    "papermill": {
     "duration": 0.002642,
     "end_time": "2023-11-24T22:23:44.335321",
     "exception": false,
     "start_time": "2023-11-24T22:23:44.332679",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Dataset Generation: Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3dfd089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Initial States:  {'allMasked': '___________', 'early': '__ss_ss____', 'quarterRevealed': '__ss_ss____', 'midRevealed': '__ss_ss_pp_', 'midLateRevealed': 'm_ss_ss_pp_', 'lateRevealed': 'm_ss_ss_pp_', 'nearEnd': 'm_ss_ss_pp_'}\n"
     ]
    }
   ],
   "source": [
    "word = \"mississippi\"\n",
    "\n",
    "initial_states = process_word_for_six_states(word)\n",
    "\n",
    "# print(initial_states)\n",
    "# Print generated initial states\n",
    "print(\"Generated Initial States: \", initial_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f65e7334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Initial States:\n",
      "___________\n",
      "1: For initial state: ___________\n",
      "Guessed: 'm', New State: 'm__________', Correct: True\n",
      "Guessed: 's', New State: 'm_ss_ss____', Correct: True\n",
      "Guessed: 'i', New State: 'mississi__i', Correct: True\n",
      "Guessed: 'o', New State: 'mississi__i', Correct: False\n",
      "Guessed: 'p', New State: 'mississippi', Correct: True\n",
      "__ss_ss____\n",
      "2: For initial state: __ss_ss____\n",
      "Guessed: 'n', New State: '__ss_ss____', Correct: False\n",
      "Guessed: 'p', New State: '__ss_ss_pp_', Correct: True\n",
      "Guessed: 'm', New State: 'm_ss_ss_pp_', Correct: True\n",
      "Guessed: 'i', New State: 'mississippi', Correct: True\n",
      "__ss_ss____\n",
      "3: For initial state: __ss_ss____\n",
      "Guessed: 'n', New State: '__ss_ss____', Correct: False\n",
      "Guessed: 'c', New State: '__ss_ss____', Correct: False\n",
      "Guessed: 'y', New State: '__ss_ss____', Correct: False\n",
      "Guessed: 'z', New State: '__ss_ss____', Correct: False\n",
      "Guessed: 'k', New State: '__ss_ss____', Correct: False\n",
      "Guessed: 'm', New State: 'm_ss_ss____', Correct: True\n",
      "Guessed: 'p', New State: 'm_ss_ss_pp_', Correct: True\n",
      "Guessed: 'b', New State: 'm_ss_ss_pp_', Correct: False\n",
      "m_ss_ss____\n",
      "4: For initial state: m_ss_ss____\n",
      "Guessed: 'i', New State: 'mississi__i', Correct: True\n",
      "Guessed: 'z', New State: 'mississi__i', Correct: False\n",
      "Guessed: 'k', New State: 'mississi__i', Correct: False\n",
      "Guessed: 'p', New State: 'mississippi', Correct: True\n",
      "mississi__i\n",
      "5: For initial state: mississi__i\n",
      "Guessed: 'p', New State: 'mississippi', Correct: True\n",
      "mississi__i\n",
      "6: For initial state: mississi__i\n",
      "Guessed: 'p', New State: 'mississippi', Correct: True\n",
      "mississi__i\n",
      "7: For initial state: mississi__i\n",
      "Guessed: 'd', New State: 'mississi__i', Correct: False\n",
      "Guessed: 'p', New State: 'mississippi', Correct: True\n"
     ]
    }
   ],
   "source": [
    "from scr.game import simulate_game_progress, \\\n",
    "    play_game_with_a_word, process_word\n",
    "\n",
    "# Example word and initial state\n",
    "# Example usage\n",
    "word = \"mississippi\"\n",
    "# word = \"cat\"\n",
    "\n",
    "initial_states = process_word_for_six_states(word)\n",
    "\n",
    "# print(initial_states)\n",
    "# Print generated initial states\n",
    "print(\"Generated Initial States:\")\n",
    "i = 1 \n",
    "for state_name, initial_state in initial_states.items():\n",
    "    # Simulate the game\n",
    "    print(initial_state)\n",
    "    print(f\"{i}: For initial state: {initial_state}\")\n",
    "    won, game_progress = simulate_game_progress(\n",
    "        model=None,  # Assuming model is not used in this example\n",
    "        word=word, \n",
    "        initial_state=initial_state, \n",
    "        char_frequency={},  # Assuming char_frequency is not used in this example\n",
    "        max_word_length=len(word), \n",
    "        device=None,  # Assuming device is not used in this example\n",
    "        max_attempts=6, \n",
    "        normalize=True,\n",
    "        difficulty=\"medium\", \n",
    "        outcome_preference='win'\n",
    "    )\n",
    "\n",
    "    # Display game progress\n",
    "    for step in game_progress:\n",
    "        print(f\"Guessed: '{step[0]}', New State: '{step[1]}', Correct: {step[2]}\")\n",
    "\n",
    "        # break\n",
    "\n",
    "    i+=1\n",
    "\n",
    "    # break\n",
    "\n",
    "    # print(\"Game Result:\", \"Won\" if won else \"Lost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8955b2cc",
   "metadata": {},
   "source": [
    "##### Writing Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe2eb7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming the function 'process_word_for_six_states' is defined elsewhere\n",
    "from scr.game import simulate_game_progress, process_word_for_six_states\n",
    "\n",
    "def process_batch_to_parquet(batch, file_path, start_game_counter):\n",
    "    game_counter = start_game_counter\n",
    "    data_for_parquet = []\n",
    "\n",
    "    for game_data in batch:\n",
    "        word, state_name, initial_state, difficulty, outcome, won, game_progress = game_data\n",
    "        if not game_progress:\n",
    "            continue\n",
    "\n",
    "        final_state = game_progress[-1][1]\n",
    "        guessed_states = [initial_state] + [state for _, state, _ in game_progress]\n",
    "        guessed_letters = [letter for letter, _, _ in game_progress]\n",
    "\n",
    "        data_for_parquet.append({\n",
    "            'game_id': game_counter,\n",
    "            'word': word,\n",
    "            'initial_state': initial_state,\n",
    "            'final_state': final_state,\n",
    "            'guessed_states': ','.join(guessed_states),\n",
    "            'guessed_letters': ','.join(guessed_letters),\n",
    "            'game_state': state_name,\n",
    "            'difficulty': difficulty,\n",
    "            'outcome': outcome,\n",
    "            'word_length': len(word),\n",
    "            'won': won\n",
    "        })\n",
    "\n",
    "        game_counter += 1\n",
    "\n",
    "    df = pd.DataFrame(data_for_parquet)\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_to_dataset(table, root_path=file_path, compression='snappy')\n",
    "\n",
    "    return game_counter\n",
    "\n",
    "def generate_batch_for_word(word):\n",
    "    batch = []\n",
    "    initial_states = process_word_for_six_states(word)\n",
    "    games_generated = 0  # Counter for games generated\n",
    "\n",
    "    for state_name, initial_state in initial_states.items():\n",
    "        for difficulty in [\"easy\", \"medium\", \"hard\"]:\n",
    "            for outcome in [\"win\", \"lose\"]:\n",
    "                won, game_progress = simulate_game_progress(\n",
    "                    model=None, word=word, initial_state=initial_state,\n",
    "                    char_frequency={}, max_word_length=len(word),\n",
    "                    device=None, max_attempts=6, normalize=True,\n",
    "                    difficulty=difficulty, outcome_preference=outcome\n",
    "                )\n",
    "                batch.append((word, state_name, initial_state, \\\n",
    "                    difficulty, outcome, won, game_progress))\n",
    "                games_generated += 1  # Increment game counter\n",
    "\n",
    "    return batch, games_generated\n",
    "\n",
    "def main_execution(words, parquet_path):\n",
    "    game_counter = 0\n",
    "    games_per_word = len(initial_states) * 3 * 2  # 7 states, 3 difficulties, 2 outcomes\n",
    "\n",
    "    for word in tqdm(words, desc=\"Processing Words\"):\n",
    "        batch, games_generated = generate_batch_for_word(word)\n",
    "\n",
    "        # Update game count\n",
    "        game_counter += games_generated\n",
    "\n",
    "        # Process and save batch\n",
    "        process_batch_to_parquet(batch, parquet_path, game_counter)\n",
    "\n",
    "    print(f\"Total games processed: {game_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8153b488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Words:   0%|          | 0/5039 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Words: 100%|██████████| 5039/5039 [00:16<00:00, 298.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total games processed: 211638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# Execute the main function\n",
    "main_execution(sampled_words_by_length, parquet_path)\n",
    "# ======================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612692c7",
   "metadata": {},
   "source": [
    "##### Checking the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe078175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of game sequences across all files: 211602\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Find all Parquet files in the directory\n",
    "parquet_files = list(parquet_path.glob('*.parquet'))\n",
    "\n",
    "if parquet_files:\n",
    "    total_game_sequences = 0\n",
    "\n",
    "    # Iterate over each file and sum the number of game sequences\n",
    "    for file in parquet_files:\n",
    "        df = pd.read_parquet(file)\n",
    "        total_game_sequences += len(df)\n",
    "\n",
    "    print(f\"Total number of game sequences across all files: {total_game_sequences}\")\n",
    "else:\n",
    "    print(\"No Parquet files found in the specified directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6bea88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of games in the dataset: 211602\n",
      "Null values in each column:\n",
      "game_id            0\n",
      "word               0\n",
      "initial_state      0\n",
      "final_state        0\n",
      "guessed_states     0\n",
      "guessed_letters    0\n",
      "game_state         0\n",
      "difficulty         0\n",
      "outcome            0\n",
      "word_length        0\n",
      "won                0\n",
      "dtype: int64\n",
      "\n",
      "Summary statistics:\n",
      "             game_id    word_length\n",
      "count  211602.000000  211602.000000\n",
      "mean   105842.724232       9.411064\n",
      "std     61084.765488       3.100986\n",
      "min        42.000000       1.000000\n",
      "25%     52942.250000       7.000000\n",
      "50%    105842.500000       9.000000\n",
      "75%    158742.750000      11.000000\n",
      "max    211679.000000      29.000000\n",
      "\n",
      "Number of unique words: 5039\n",
      "\n",
      "Outcome distribution:\n",
      "outcome\n",
      "win     105801\n",
      "lose    105801\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Difficulty distribution:\n",
      "difficulty\n",
      "easy      70534\n",
      "medium    70534\n",
      "hard      70534\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Word Length Distribution:\n",
      "word_length\n",
      "9     28602\n",
      "8     28140\n",
      "10    24906\n",
      "7     23982\n",
      "11    21042\n",
      "6     18102\n",
      "12    16884\n",
      "13    11970\n",
      "5     10458\n",
      "14     8148\n",
      "15     4914\n",
      "4      4872\n",
      "16     2982\n",
      "3      2046\n",
      "17     1638\n",
      "18      924\n",
      "19      504\n",
      "21      336\n",
      "20      336\n",
      "2       282\n",
      "22      210\n",
      "23      126\n",
      "24       84\n",
      "25       42\n",
      "29       42\n",
      "1        30\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Use glob to find all Parquet files in the folder\n",
    "parquet_files = parquet_path.glob('*.parquet')\n",
    "\n",
    "# Read and concatenate all Parquet files into a single DataFrame\n",
    "df = pd.concat([pd.read_parquet(file) for file in \\\n",
    "    parquet_files], ignore_index=True)\n",
    "\n",
    "# # Display the first few rows of the DataFrame\n",
    "# print(df.head())\n",
    "\n",
    "# Get the total number of rows (games) in the DataFrame\n",
    "total_games = len(df)\n",
    "print(f\"Total number of games in the dataset: {total_games}\")\n",
    "\n",
    "# Additional checks and summary statistics\n",
    "print(\"Null values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Count the number of unique words or game states\n",
    "unique_words = df['word'].nunique()\n",
    "print(f\"\\nNumber of unique words: {unique_words}\")\n",
    "\n",
    "# Inspect the distribution of game outcomes, difficulties, etc.\n",
    "print(\"\\nOutcome distribution:\")\n",
    "print(df['outcome'].value_counts())\n",
    "\n",
    "print(\"\\nDifficulty distribution:\")\n",
    "print(df['difficulty'].value_counts())\n",
    "\n",
    "# Word length distribution\n",
    "print(\"\\nWord Length Distribution:\")\n",
    "print(df['word_length'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5a2b185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>word</th>\n",
       "      <th>initial_state</th>\n",
       "      <th>final_state</th>\n",
       "      <th>guessed_states</th>\n",
       "      <th>guessed_letters</th>\n",
       "      <th>game_state</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>outcome</th>\n",
       "      <th>word_length</th>\n",
       "      <th>won</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>193578</td>\n",
       "      <td>unproportionably</td>\n",
       "      <td>________________</td>\n",
       "      <td>unproportionably</td>\n",
       "      <td>________________,______________l_,____________...</td>\n",
       "      <td>l,a,r,n,b,p,z,i,y,t,o,u</td>\n",
       "      <td>allMasked</td>\n",
       "      <td>easy</td>\n",
       "      <td>win</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>193579</td>\n",
       "      <td>unproportionably</td>\n",
       "      <td>________________</td>\n",
       "      <td>unproportionably</td>\n",
       "      <td>________________,______________l_,__p__p______...</td>\n",
       "      <td>l,p,y,i,t,n,q,u,r,o,b,a</td>\n",
       "      <td>allMasked</td>\n",
       "      <td>easy</td>\n",
       "      <td>lose</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>193580</td>\n",
       "      <td>unproportionably</td>\n",
       "      <td>________________</td>\n",
       "      <td>unp_opo___on__ly</td>\n",
       "      <td>________________,u_______________,u___________...</td>\n",
       "      <td>u,g,y,z,n,p,q,l,d,o,j,c</td>\n",
       "      <td>allMasked</td>\n",
       "      <td>medium</td>\n",
       "      <td>win</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>193581</td>\n",
       "      <td>unproportionably</td>\n",
       "      <td>________________</td>\n",
       "      <td>un_r___rti_na__y</td>\n",
       "      <td>________________,_______________y,_________i__...</td>\n",
       "      <td>y,i,e,f,a,d,n,t,k,r,u,s,h</td>\n",
       "      <td>allMasked</td>\n",
       "      <td>medium</td>\n",
       "      <td>lose</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>193582</td>\n",
       "      <td>unproportionably</td>\n",
       "      <td>________________</td>\n",
       "      <td>____o_o__io_a_ly</td>\n",
       "      <td>________________,_________i______,_________i__...</td>\n",
       "      <td>i,y,l,s,k,o,v,z,h,a,g</td>\n",
       "      <td>allMasked</td>\n",
       "      <td>hard</td>\n",
       "      <td>win</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211597</th>\n",
       "      <td>115831</td>\n",
       "      <td>prediscourage</td>\n",
       "      <td>pred_scourage</td>\n",
       "      <td>prediscourage</td>\n",
       "      <td>pred_scourage,prediscourage</td>\n",
       "      <td>i</td>\n",
       "      <td>nearEnd</td>\n",
       "      <td>easy</td>\n",
       "      <td>lose</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211598</th>\n",
       "      <td>115832</td>\n",
       "      <td>prediscourage</td>\n",
       "      <td>pred_scourage</td>\n",
       "      <td>prediscourage</td>\n",
       "      <td>pred_scourage,pred_scourage,prediscourage</td>\n",
       "      <td>x,i</td>\n",
       "      <td>nearEnd</td>\n",
       "      <td>medium</td>\n",
       "      <td>win</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211599</th>\n",
       "      <td>115833</td>\n",
       "      <td>prediscourage</td>\n",
       "      <td>pred_scourage</td>\n",
       "      <td>prediscourage</td>\n",
       "      <td>pred_scourage,prediscourage</td>\n",
       "      <td>i</td>\n",
       "      <td>nearEnd</td>\n",
       "      <td>medium</td>\n",
       "      <td>lose</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211600</th>\n",
       "      <td>115834</td>\n",
       "      <td>prediscourage</td>\n",
       "      <td>pred_scourage</td>\n",
       "      <td>pred_scourage</td>\n",
       "      <td>pred_scourage,pred_scourage,pred_scourage,pred...</td>\n",
       "      <td>j,q,t,y,k,l</td>\n",
       "      <td>nearEnd</td>\n",
       "      <td>hard</td>\n",
       "      <td>win</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211601</th>\n",
       "      <td>115835</td>\n",
       "      <td>prediscourage</td>\n",
       "      <td>pred_scourage</td>\n",
       "      <td>prediscourage</td>\n",
       "      <td>pred_scourage,prediscourage</td>\n",
       "      <td>i</td>\n",
       "      <td>nearEnd</td>\n",
       "      <td>hard</td>\n",
       "      <td>lose</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211602 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        game_id              word     initial_state       final_state  \\\n",
       "0        193578  unproportionably  ________________  unproportionably   \n",
       "1        193579  unproportionably  ________________  unproportionably   \n",
       "2        193580  unproportionably  ________________  unp_opo___on__ly   \n",
       "3        193581  unproportionably  ________________  un_r___rti_na__y   \n",
       "4        193582  unproportionably  ________________  ____o_o__io_a_ly   \n",
       "...         ...               ...               ...               ...   \n",
       "211597   115831     prediscourage     pred_scourage     prediscourage   \n",
       "211598   115832     prediscourage     pred_scourage     prediscourage   \n",
       "211599   115833     prediscourage     pred_scourage     prediscourage   \n",
       "211600   115834     prediscourage     pred_scourage     pred_scourage   \n",
       "211601   115835     prediscourage     pred_scourage     prediscourage   \n",
       "\n",
       "                                           guessed_states  \\\n",
       "0       ________________,______________l_,____________...   \n",
       "1       ________________,______________l_,__p__p______...   \n",
       "2       ________________,u_______________,u___________...   \n",
       "3       ________________,_______________y,_________i__...   \n",
       "4       ________________,_________i______,_________i__...   \n",
       "...                                                   ...   \n",
       "211597                        pred_scourage,prediscourage   \n",
       "211598          pred_scourage,pred_scourage,prediscourage   \n",
       "211599                        pred_scourage,prediscourage   \n",
       "211600  pred_scourage,pred_scourage,pred_scourage,pred...   \n",
       "211601                        pred_scourage,prediscourage   \n",
       "\n",
       "                  guessed_letters game_state difficulty outcome  word_length  \\\n",
       "0         l,a,r,n,b,p,z,i,y,t,o,u  allMasked       easy     win           16   \n",
       "1         l,p,y,i,t,n,q,u,r,o,b,a  allMasked       easy    lose           16   \n",
       "2         u,g,y,z,n,p,q,l,d,o,j,c  allMasked     medium     win           16   \n",
       "3       y,i,e,f,a,d,n,t,k,r,u,s,h  allMasked     medium    lose           16   \n",
       "4           i,y,l,s,k,o,v,z,h,a,g  allMasked       hard     win           16   \n",
       "...                           ...        ...        ...     ...          ...   \n",
       "211597                          i    nearEnd       easy    lose           13   \n",
       "211598                        x,i    nearEnd     medium     win           13   \n",
       "211599                          i    nearEnd     medium    lose           13   \n",
       "211600                j,q,t,y,k,l    nearEnd       hard     win           13   \n",
       "211601                          i    nearEnd       hard    lose           13   \n",
       "\n",
       "          won  \n",
       "0        True  \n",
       "1        True  \n",
       "2       False  \n",
       "3       False  \n",
       "4       False  \n",
       "...       ...  \n",
       "211597   True  \n",
       "211598   True  \n",
       "211599   True  \n",
       "211600  False  \n",
       "211601   True  \n",
       "\n",
       "[211602 rows x 11 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # Replace this with the path to your Parquet file\n",
    "# parquet_file_path = 'path/to/your/HangmanData.parquet'\n",
    "\n",
    "# Read the Parquet file\n",
    "df = pd.read_parquet(parquet_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7878ebe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of games in the dataset: 211602\n",
      "game_id            0\n",
      "word               0\n",
      "initial_state      0\n",
      "final_state        0\n",
      "guessed_states     0\n",
      "guessed_letters    0\n",
      "game_state         0\n",
      "difficulty         0\n",
      "outcome            0\n",
      "word_length        0\n",
      "won                0\n",
      "dtype: int64\n",
      "             game_id    word_length\n",
      "count  211602.000000  211602.000000\n",
      "mean   105842.724232       9.411064\n",
      "std     61084.765488       3.100986\n",
      "min        42.000000       1.000000\n",
      "25%     52942.250000       7.000000\n",
      "50%    105842.500000       9.000000\n",
      "75%    158742.750000      11.000000\n",
      "max    211679.000000      29.000000\n",
      "Number of unique words: 5039\n",
      "outcome\n",
      "win     105801\n",
      "lose    105801\n",
      "Name: count, dtype: int64\n",
      "difficulty\n",
      "easy      70534\n",
      "medium    70534\n",
      "hard      70534\n",
      "Name: count, dtype: int64\n",
      "won\n",
      "True     140814\n",
      "False     70788\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of rows (games) in the DataFrame\n",
    "total_games = len(df)\n",
    "\n",
    "print(f\"Total number of games in the dataset: {total_games}\")\n",
    "\n",
    "# Additional checks you might want to perform:\n",
    "# - Check for any null values or anomalies in the data\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# - Get a summary of the DataFrame\n",
    "print(df.describe())\n",
    "\n",
    "# - Count the number of unique words or game states\n",
    "unique_words = df['word'].nunique()\n",
    "print(f\"Number of unique words: {unique_words}\")\n",
    "\n",
    "# - Inspect the distribution of game outcomes, difficulties, etc.\n",
    "print(df['outcome'].value_counts())\n",
    "print(df['difficulty'].value_counts())\n",
    "print(df['won'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a7795d",
   "metadata": {},
   "source": [
    "##### Dataset Creation and train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cbe602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets directly from the saved parquet files\n",
    "hangman_dataset = HangmanDataset(parquet_path)\n",
    "# valid_dataset = HangmanDataset(parquet_valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1de27f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_dataset, valid_dataset = hangman_dataset.split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cce7ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in original dataset: 211602\n",
      "Samples in training dataset: 169273\n",
      "Samples in validation dataset: 42329\n",
      "Training dataset sequence length distribution: {13: 3523, 14: 2030, 12: 6014, 11: 8786, 15: 1017, 10: 12079, 8: 22252, 7: 25064, 9: 16881, 5: 10049, 4: 11620, 2: 25194, 3: 14766, 6: 9357, 17: 167, 16: 400, 18: 47, 19: 19, 21: 4, 20: 4}\n",
      "Validation dataset sequence length distribution: {13: 881, 14: 508, 12: 1504, 11: 2197, 15: 255, 10: 3020, 8: 5564, 7: 6266, 9: 4221, 5: 2513, 4: 2906, 2: 6299, 3: 3692, 6: 2340, 17: 42, 16: 101, 18: 12, 19: 5, 21: 1, 20: 2}\n"
     ]
    }
   ],
   "source": [
    "def check_seq_len_distribution(dataset):\n",
    "    distribution = {}\n",
    "    for seq_len, indices in dataset.seq_len_index.items():\n",
    "        distribution[seq_len] = len(indices)\n",
    "    return distribution\n",
    "\n",
    "def check_seq_len_distribution(dataset):\n",
    "    distribution = {}\n",
    "    for seq_len, indices in dataset.seq_len_index.items():\n",
    "        distribution[seq_len] = len(indices)\n",
    "    return distribution\n",
    "\n",
    "# Check proportions\n",
    "print(f\"Total samples in original dataset: {len(hangman_dataset)}\")\n",
    "print(f\"Samples in training dataset: {len(train_dataset)}\")\n",
    "print(f\"Samples in validation dataset: {len(valid_dataset)}\")\n",
    "\n",
    "# Verify that the sum of train and validation samples equals the total samples\n",
    "assert len(train_dataset) + len(valid_dataset) == len(hangman_dataset), \"Mismatch in total sample count\"\n",
    "\n",
    "# Check sequence length distribution in each dataset\n",
    "train_distribution = check_seq_len_distribution(train_dataset)\n",
    "valid_distribution = check_seq_len_distribution(valid_dataset)\n",
    "\n",
    "print(\"Training dataset sequence length distribution:\", train_distribution)\n",
    "print(\"Validation dataset sequence length distribution:\", valid_distribution)\n",
    "\n",
    "# Ensure each sequence length is represented in both datasets\n",
    "assert set(train_distribution.keys()) == set(valid_distribution.keys()), \"Mismatch in sequence length representation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "048f836a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sequence lengths in training dataset: 20\n",
      "Unique sequence lengths in validation dataset: 20\n"
     ]
    }
   ],
   "source": [
    "def count_unique_seq_lens(dataset):\n",
    "    # Count the unique sequence lengths in the dataset\n",
    "    unique_seq_lens = len(dataset.seq_len_index)\n",
    "    return unique_seq_lens\n",
    "\n",
    "train_unique_seq_lens = count_unique_seq_lens(train_dataset)\n",
    "valid_unique_seq_lens = count_unique_seq_lens(valid_dataset)\n",
    "\n",
    "print(f\"Unique sequence lengths in training dataset: {train_unique_seq_lens}\")\n",
    "print(f\"Unique sequence lengths in validation dataset: {valid_unique_seq_lens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12e88f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'game_id': 193578,\n",
       " 'word': 'unproportionably',\n",
       " 'initial_state': ['________________'],\n",
       " 'final_state': 'unproportionably',\n",
       " 'guessed_states': ['________________',\n",
       "  '______________l_',\n",
       "  '____________a_l_',\n",
       "  '___r___r____a_l_',\n",
       "  '_n_r___r___na_l_',\n",
       "  '_n_r___r___nabl_',\n",
       "  '_npr_p_r___nabl_',\n",
       "  '_npr_p_r___nabl_',\n",
       "  '_npr_p_r_i_nabl_',\n",
       "  '_npr_p_r_i_nably',\n",
       "  '_npr_p_rti_nably',\n",
       "  '_nproportionably',\n",
       "  'unproportionably'],\n",
       " 'guessed_letters': ['l',\n",
       "  'a',\n",
       "  'r',\n",
       "  'n',\n",
       "  'b',\n",
       "  'p',\n",
       "  'z',\n",
       "  'i',\n",
       "  'y',\n",
       "  't',\n",
       "  'o',\n",
       "  'u'],\n",
       " 'game_state': 'allMasked',\n",
       " 'difficulty': 'easy',\n",
       " 'outcome': 'win',\n",
       " 'word_length': 16,\n",
       " 'won': True}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hangman_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5765946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final State: salicaceae\n",
      "Guessed States: ['__l_______', '__li______', 's_li______', 's_li___e_e', 's_lic_ce_e', 'salicaceae']\n",
      "Guessed Letters: ['l', 'i', 's', 'e', 'c', 'a']\n",
      "Game Won: True\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define the function simulate_game_progress here\n",
    "\n",
    "# Simulate a game with the specified parameters\n",
    "word = 'salicaceae'\n",
    "initial_state = '__________'  # Adjusted from ['__________'] to '__________' to match expected input type\n",
    "game_won, game_progress = simulate_game_progress(None, word, initial_state, None, None, None, difficulty='easy', outcome_preference='win')\n",
    "\n",
    "# Extract the final state from the last entry in game_progress, if available\n",
    "final_state = game_progress[-1][1] if game_progress else initial_state\n",
    "\n",
    "# Extract guessed states and guessed letters from game_progress\n",
    "guessed_states = [state for _, state, _ in game_progress]\n",
    "guessed_letters = [guess for guess, _, _ in game_progress]\n",
    "\n",
    "# Determine if the game was won based on the final state\n",
    "won = final_state == word\n",
    "\n",
    "# Output the results for comparison\n",
    "print(f\"Final State: {final_state}\")\n",
    "print(f\"Guessed States: {guessed_states}\")\n",
    "print(f\"Guessed Letters: {guessed_letters}\")\n",
    "print(f\"Game Won: {won}\")\n",
    "\n",
    "# Compare the results with the expected outcomes\n",
    "# Note: Direct comparison might not be feasible due to the randomness in guessed letters and states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b27d93d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/sayem/Desktop/Hangman/1_a_dataset_generation.ipynb",
   "output_path": "/home/sayem/Desktop/Hangman/1_a_dataset_generation.ipynb",
   "parameters": {},
   "start_time": "2023-11-24T22:23:41.671735",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
