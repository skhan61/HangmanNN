{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a62bd7e6",
   "metadata": {
    "papermill": {
     "duration": 0.002135,
     "end_time": "2023-11-24T22:23:42.367043",
     "exception": false,
     "start_time": "2023-11-24T22:23:42.364908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a72c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:23:42.378753Z",
     "iopub.status.busy": "2023-11-24T22:23:42.378580Z",
     "iopub.status.idle": "2023-11-24T22:23:43.346735Z",
     "shell.execute_reply": "2023-11-24T22:23:43.346282Z"
    },
    "papermill": {
     "duration": 0.971674,
     "end_time": "2023-11-24T22:23:43.347691",
     "exception": false,
     "start_time": "2023-11-24T22:23:42.376017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from scr.dataset import *\n",
    "# from scr.game import *\n",
    "from scr.feature_engineering import *\n",
    "# from scr.plot_utils import *\n",
    "import gc\n",
    "from scr.utils import print_scenarios\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from scr.utils import read_words, save_words_to_file\n",
    "\n",
    "import sys\n",
    "# Custom library paths\n",
    "sys.path.extend(['../', './scr'])\n",
    "\n",
    "from scr.utils import set_seed\n",
    "from scr.utils import read_words\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Read and Shuffle Word List\n",
    "word_list = read_words('data/words_250000_train.txt') # , limit=10000)\n",
    "# word_list = read_words('data/250k.txt', limit=10000)\n",
    "\n",
    "random.shuffle(word_list)\n",
    "\n",
    "# Calculate Frequencies and Max Word Length\n",
    "word_frequencies = calculate_word_frequencies(word_list)\n",
    "char_frequency = calculate_char_frequencies(word_list)\n",
    "max_word_length = max(len(word) for word in word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e2750e",
   "metadata": {},
   "source": [
    "##### Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b7063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "NUM_STRATIFIED_SAMPLES = 500 # This will be overwritten by Papermill\n",
    "\n",
    "# Define the base directory and the paths for training and validation parquet files\n",
    "base_dataset_dir = Path(\"/media/sayem/510B93E12554BBD1/dataset/\")\n",
    "\n",
    "stratified_samples_dir = base_dataset_dir / str(NUM_STRATIFIED_SAMPLES)\n",
    "\n",
    "parquet_train_path = stratified_samples_dir / 'train_parquets'\n",
    "parquet_valid_path = stratified_samples_dir / 'valid_parquets'\n",
    "\n",
    "# Function to delete and recreate a directory\n",
    "def recreate_directory(path):\n",
    "    if path.exists():\n",
    "        shutil.rmtree(path)  # Delete the directory and its contents\n",
    "    path.mkdir(parents=True)  # Create the directory\n",
    "\n",
    "# Recreate the train and valid directories\n",
    "recreate_directory(parquet_train_path)\n",
    "recreate_directory(parquet_valid_path)\n",
    "\n",
    "print(f\"Directories '{parquet_train_path}' and '{parquet_valid_path}' have been recreated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cc199b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:23:43.568919Z",
     "iopub.status.busy": "2023-11-24T22:23:43.568821Z",
     "iopub.status.idle": "2023-11-24T22:23:43.571482Z",
     "shell.execute_reply": "2023-11-24T22:23:43.571208Z"
    },
    "papermill": {
     "duration": 0.007294,
     "end_time": "2023-11-24T22:23:43.572518",
     "exception": false,
     "start_time": "2023-11-24T22:23:43.565224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc97d62d",
   "metadata": {},
   "source": [
    "##### Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ad69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the total number of words and the number of test samples\n",
    "from scr.custom_sampler import *\n",
    "NUM_TEST_SAMPLES = 10_000\n",
    "\n",
    "# Assuming 'word_list' contains the 250,000 words\n",
    "# First, separate 10,000 words for the final testing set\n",
    "testing_words = stratified_sample_by_length_and_uniqueness(\n",
    "    word_list, \n",
    "    NUM_TEST_SAMPLES\n",
    ")\n",
    "\n",
    "# Define the file path for saving the testing words\n",
    "testing_words_file_path = stratified_samples_dir / \"testing_words.txt\"\n",
    "\n",
    "# Save the testing words to a file\n",
    "with open(testing_words_file_path, 'w') as file:\n",
    "    for word in testing_words:\n",
    "        file.write(word + '\\n')\n",
    "\n",
    "print(f\"Testing words saved in {testing_words_file_path}\")\n",
    "\n",
    "# Now, remove these testing samples from the original word list\n",
    "remaining_words = [word for word in word_list if word not in testing_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469a87ce",
   "metadata": {},
   "source": [
    "##### Stratified Sample Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e486f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:23:43.579494Z",
     "iopub.status.busy": "2023-11-24T22:23:43.579377Z",
     "iopub.status.idle": "2023-11-24T22:23:43.640436Z",
     "shell.execute_reply": "2023-11-24T22:23:43.640019Z"
    },
    "papermill": {
     "duration": 0.065848,
     "end_time": "2023-11-24T22:23:43.641509",
     "exception": false,
     "start_time": "2023-11-24T22:23:43.575661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## we are taking starified samples from train_words\n",
    "\n",
    "from scr.custom_sampler import \\\n",
    "    stratified_sample_by_length_and_frequency, \\\n",
    "        stratified_sample_by_length, stratified_sample_by_length_and_uniqueness\n",
    "\n",
    "print(f'Number of Statrified samples: {NUM_STRATIFIED_SAMPLES}')\n",
    "\n",
    "# sampled_words_by_length_and_frequency \\\n",
    "#     = stratified_sample_by_length_and_frequency(train_words, \\\n",
    "#     word_frequencies, \\\n",
    "#     NUM_STRATIFIED_SAMPLES)\n",
    "\n",
    "sampled_words_by_length = stratified_sample_by_length_and_uniqueness(remaining_words, \\\n",
    "    NUM_STRATIFIED_SAMPLES)\n",
    "\n",
    "print(len(sampled_words_by_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0d326f",
   "metadata": {
    "papermill": {
     "duration": 0.004373,
     "end_time": "2023-11-24T22:23:44.300659",
     "exception": false,
     "start_time": "2023-11-24T22:23:44.296286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Intial State Simulation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae53fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.game import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word = \"mississippi\"\n",
    "word = \"mythopoetize\"\n",
    "# word = \"cat\"\n",
    "\n",
    "initial_states = process_word_for_six_states(word)\n",
    "\n",
    "initial_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3da54c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:23:44.317155Z",
     "iopub.status.busy": "2023-11-24T22:23:44.317069Z",
     "iopub.status.idle": "2023-11-24T22:23:44.319452Z",
     "shell.execute_reply": "2023-11-24T22:23:44.319138Z"
    },
    "papermill": {
     "duration": 0.005907,
     "end_time": "2023-11-24T22:23:44.320079",
     "exception": false,
     "start_time": "2023-11-24T22:23:44.314172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(initial_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cffa5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:23:44.326727Z",
     "iopub.status.busy": "2023-11-24T22:23:44.326580Z",
     "iopub.status.idle": "2023-11-24T22:23:44.329649Z",
     "shell.execute_reply": "2023-11-24T22:23:44.329220Z"
    },
    "papermill": {
     "duration": 0.007091,
     "end_time": "2023-11-24T22:23:44.330204",
     "exception": false,
     "start_time": "2023-11-24T22:23:44.323113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01da2152",
   "metadata": {
    "papermill": {
     "duration": 0.002642,
     "end_time": "2023-11-24T22:23:44.335321",
     "exception": false,
     "start_time": "2023-11-24T22:23:44.332679",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Dataset Generation: Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dfd089",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"mississippi\"\n",
    "\n",
    "initial_states = process_word_for_six_states(word)\n",
    "\n",
    "# print(initial_states)\n",
    "# Print generated initial states\n",
    "print(\"Generated Initial States: \", initial_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e7334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.game import simulate_game_progress, \\\n",
    "    play_game_with_a_word, process_word\n",
    "\n",
    "# Example word and initial state\n",
    "# Example usage\n",
    "word = \"mississippi\"\n",
    "# word = \"cat\"\n",
    "\n",
    "initial_states = process_word_for_six_states(word)\n",
    "\n",
    "# print(initial_states)\n",
    "# Print generated initial states\n",
    "print(\"Generated Initial States:\")\n",
    "for state_name, initial_state in initial_states.items():\n",
    "    # Simulate the game\n",
    "    print(initial_state)\n",
    "    print(f\"For initial state: {initial_state}\")\n",
    "    won, game_progress = simulate_game_progress(\n",
    "        model=None,  # Assuming model is not used in this example\n",
    "        word=word, \n",
    "        initial_state=initial_state, \n",
    "        char_frequency={},  # Assuming char_frequency is not used in this example\n",
    "        max_word_length=len(word), \n",
    "        device=None,  # Assuming device is not used in this example\n",
    "        max_attempts=6, \n",
    "        normalize=True,\n",
    "        difficulty=\"medium\", \n",
    "        outcome_preference='win'\n",
    "    )\n",
    "\n",
    "    # Display game progress\n",
    "    for step in game_progress:\n",
    "        print(f\"Guessed: '{step[0]}', New State: '{step[1]}', Correct: {step[2]}\")\n",
    "\n",
    "        # break\n",
    "\n",
    "    # break\n",
    "\n",
    "    # print(\"Game Result:\", \"Won\" if won else \"Lost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8955b2cc",
   "metadata": {},
   "source": [
    "##### Writing Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2eb7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming the function 'process_word_for_six_states' is defined elsewhere\n",
    "from scr.game import simulate_game_progress, process_word_for_six_states\n",
    "\n",
    "def process_batch_to_parquet(batch, file_path, start_game_counter):\n",
    "    game_counter = start_game_counter\n",
    "    data_for_parquet = []\n",
    "\n",
    "    for game_data in batch:\n",
    "        word, state_name, initial_state, difficulty, outcome, won, game_progress = game_data\n",
    "        if not game_progress:\n",
    "            continue\n",
    "\n",
    "        final_state = game_progress[-1][1]\n",
    "        guessed_states = [initial_state] + [state for _, state, _ in game_progress]\n",
    "        guessed_letters = [letter for letter, _, _ in game_progress]\n",
    "\n",
    "        data_for_parquet.append({\n",
    "            'game_id': game_counter,\n",
    "            'word': word,\n",
    "            'initial_state': initial_state,\n",
    "            'final_state': final_state,\n",
    "            'guessed_states': ','.join(guessed_states[:-1]),\n",
    "            'guessed_letters': ','.join(guessed_letters),\n",
    "            'game_state': state_name,\n",
    "            'difficulty': difficulty,\n",
    "            'outcome': outcome,\n",
    "            'word_length': len(word),\n",
    "            'won': won\n",
    "        })\n",
    "\n",
    "        game_counter += 1\n",
    "\n",
    "    df = pd.DataFrame(data_for_parquet)\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_to_dataset(table, root_path=file_path, compression='snappy')\n",
    "\n",
    "    return game_counter\n",
    "\n",
    "def generate_batch_for_word(word):\n",
    "    batch = []\n",
    "    initial_states = process_word_for_six_states(word)\n",
    "    games_generated = 0  # Counter for games generated\n",
    "\n",
    "    for state_name, initial_state in initial_states.items():\n",
    "        for difficulty in [\"easy\", \"medium\", \"hard\"]:\n",
    "            for outcome in [\"win\", \"lose\"]:\n",
    "                won, game_progress = simulate_game_progress(\n",
    "                    model=None, word=word, initial_state=initial_state,\n",
    "                    char_frequency={}, max_word_length=len(word),\n",
    "                    device=None, max_attempts=6, normalize=True,\n",
    "                    difficulty=difficulty, outcome_preference=outcome\n",
    "                )\n",
    "                batch.append((word, state_name, initial_state, difficulty, outcome, won, game_progress))\n",
    "                games_generated += 1  # Increment game counter\n",
    "\n",
    "    return batch, games_generated\n",
    "\n",
    "\n",
    "def main_execution(words, parquet_train_path, parquet_valid_path, test_size=0.20):\n",
    "    train_game_counter = 0\n",
    "    valid_game_counter = 0\n",
    "    games_per_word = 6 * 3 * 2  # 6 states, 3 difficulties, 2 outcomes\n",
    "\n",
    "    train_words, valid_words = train_test_split(words, test_size=test_size)\n",
    "\n",
    "    # Process each word set\n",
    "    for word_set, path in [(train_words, parquet_train_path), (valid_words, parquet_valid_path)]:\n",
    "        total_words_processed = 0\n",
    "        total_games_generated = 0\n",
    "\n",
    "        for word in tqdm(word_set, desc=\"Processing Words\"):\n",
    "            batch, games_generated = generate_batch_for_word(word)\n",
    "\n",
    "            # Update word count and game count\n",
    "            total_words_processed += 1\n",
    "            total_games_generated += games_generated\n",
    "\n",
    "            # Update game counters\n",
    "            if word_set is train_words:\n",
    "                train_game_counter += games_generated\n",
    "                process_batch_to_parquet(batch, path, train_game_counter)\n",
    "            else:\n",
    "                valid_game_counter += games_generated\n",
    "                process_batch_to_parquet(batch, path, valid_game_counter)\n",
    "\n",
    "    print(f\"Final Total games processed in training set: {train_game_counter}\")\n",
    "    print(f\"Final Total games processed in validation set: {valid_game_counter}\")\n",
    "\n",
    "# Execute the main function\n",
    "main_execution(sampled_words_by_length, parquet_train_path, parquet_valid_path, test_size=0.20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a7795d",
   "metadata": {},
   "source": [
    "##### Checking Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f8ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Assuming parquet_train_path is already defined as a Path object\n",
    "# If not, define it here\n",
    "# parquet_train_path = Path('path_to_your_train_parquet_directory')\n",
    "\n",
    "# Use glob to find all Parquet files in the folder\n",
    "parquet_files = parquet_train_path.glob('*.parquet')\n",
    "\n",
    "# Count the number of files\n",
    "file_count = sum(1 for _ in parquet_files)\n",
    "\n",
    "print(f\"Number of Parquet files: {file_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb677b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Find all Parquet files in the directory\n",
    "parquet_files = list(parquet_train_path.glob('*.parquet'))\n",
    "\n",
    "if parquet_files:\n",
    "    total_game_sequences = 0\n",
    "\n",
    "    # Iterate over each file and sum the number of game sequences\n",
    "    for file in parquet_files:\n",
    "        df = pd.read_parquet(file)\n",
    "        total_game_sequences += len(df)\n",
    "\n",
    "    print(f\"Total number of game sequences across all files: {total_game_sequences}\")\n",
    "else:\n",
    "    print(\"No Parquet files found in the specified directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ae9653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Use glob to find all Parquet files in the folder\n",
    "parquet_files = parquet_train_path.glob('*.parquet')\n",
    "\n",
    "# Read and concatenate all Parquet files into a single DataFrame\n",
    "df = pd.concat([pd.read_parquet(file) for file in parquet_files], ignore_index=True)\n",
    "\n",
    "# # Display the first few rows of the DataFrame\n",
    "# print(df.head())\n",
    "\n",
    "# Get the total number of rows (games) in the DataFrame\n",
    "total_games = len(df)\n",
    "print(f\"Total number of games in the dataset: {total_games}\")\n",
    "\n",
    "# Additional checks and summary statistics\n",
    "print(\"Null values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Count the number of unique words or game states\n",
    "unique_words = df['word'].nunique()\n",
    "print(f\"\\nNumber of unique words: {unique_words}\")\n",
    "\n",
    "# Inspect the distribution of game outcomes, difficulties, etc.\n",
    "print(\"\\nOutcome distribution:\")\n",
    "print(df['outcome'].value_counts())\n",
    "\n",
    "print(\"\\nDifficulty distribution:\")\n",
    "print(df['difficulty'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1359cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # Replace this with the path to your Parquet file\n",
    "# parquet_file_path = 'path/to/your/HangmanData.parquet'\n",
    "\n",
    "# Read the Parquet file\n",
    "df = pd.read_parquet(parquet_train_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a8c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of rows (games) in the DataFrame\n",
    "total_games = len(df)\n",
    "\n",
    "print(f\"Total number of games in the dataset: {total_games}\")\n",
    "\n",
    "# Additional checks you might want to perform:\n",
    "# - Check for any null values or anomalies in the data\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# - Get a summary of the DataFrame\n",
    "print(df.describe())\n",
    "\n",
    "# - Count the number of unique words or game states\n",
    "unique_words = df['word'].nunique()\n",
    "print(f\"Number of unique words: {unique_words}\")\n",
    "\n",
    "# - Inspect the distribution of game outcomes, difficulties, etc.\n",
    "print(df['outcome'].value_counts())\n",
    "print(df['difficulty'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd332aa",
   "metadata": {},
   "source": [
    "##### Checking the Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # Replace this with the path to your Parquet file\n",
    "# parquet_file_path = 'path/to/your/HangmanData.parquet'\n",
    "\n",
    "# Read the Parquet file\n",
    "df = pd.read_parquet(parquet_valid_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08449596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of rows (games) in the DataFrame\n",
    "total_games = len(df)\n",
    "\n",
    "print(f\"Total number of games in the dataset: {total_games}\")\n",
    "\n",
    "# Additional checks you might want to perform:\n",
    "# - Check for any null values or anomalies in the data\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# - Get a summary of the DataFrame\n",
    "print(df.describe())\n",
    "\n",
    "# - Count the number of unique words or game states\n",
    "unique_words = df['word'].nunique()\n",
    "print(f\"Number of unique words: {unique_words}\")\n",
    "\n",
    "# - Inspect the distribution of game outcomes, difficulties, etc.\n",
    "print(df['outcome'].value_counts())\n",
    "print(df['difficulty'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1057e0",
   "metadata": {},
   "source": [
    "##### Reading Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets directly from the saved parquet files\n",
    "train_dataset = HangmanDataset(parquet_train_path)\n",
    "valid_dataset = HangmanDataset(parquet_valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7696bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b5fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, \n",
    "                          collate_fn=custom_collate_fn, \n",
    "                          shuffle=True, \n",
    "                          num_workers=os.cpu_count() or 1,  # Adjust based on your system\n",
    "                          prefetch_factor=2)  # Adjust based on your needs\n",
    "                          \n",
    "val_loader = DataLoader(valid_dataset, batch_size=512, \n",
    "                          collate_fn=custom_collate_fn, \n",
    "                          shuffle=False, \n",
    "                          num_workers=os.cpu_count() or 1,  # Adjust based on your system\n",
    "                          prefetch_factor=2)  # Adjust based on your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456485fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68aa2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/sayem/Desktop/Hangman/1_a_dataset_generation.ipynb",
   "output_path": "/home/sayem/Desktop/Hangman/1_a_dataset_generation.ipynb",
   "parameters": {},
   "start_time": "2023-11-24T22:23:41.671735",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
