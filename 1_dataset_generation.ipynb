{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a62bd7e6",
   "metadata": {
    "papermill": {
     "duration": 0.002135,
     "end_time": "2023-11-24T22:23:42.367043",
     "exception": false,
     "start_time": "2023-11-24T22:23:42.364908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "323a72c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:23:42.378753Z",
     "iopub.status.busy": "2023-11-24T22:23:42.378580Z",
     "iopub.status.idle": "2023-11-24T22:23:43.346735Z",
     "shell.execute_reply": "2023-11-24T22:23:43.346282Z"
    },
    "papermill": {
     "duration": 0.971674,
     "end_time": "2023-11-24T22:23:43.347691",
     "exception": false,
     "start_time": "2023-11-24T22:23:42.376017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from scr.dataset import *\n",
    "# from scr.game import *\n",
    "from scr.feature_engineering import *\n",
    "# from scr.plot_utils import *\n",
    "import gc\n",
    "from scr.utils import print_scenarios\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from scr.utils import read_words, save_words_to_file\n",
    "\n",
    "import sys\n",
    "# Custom library paths\n",
    "sys.path.extend(['../', './scr'])\n",
    "\n",
    "from scr.utils import set_seed\n",
    "from scr.utils import read_words\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Read and Shuffle Word List\n",
    "word_list = read_words('data/words_250000_train.txt') # , limit=10000)\n",
    "# word_list = read_words('data/250k.txt', limit=10000)\n",
    "\n",
    "random.shuffle(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ef82d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# import json\n",
    "\n",
    "# # Sample Hangman game data\n",
    "# games_data = [\n",
    "#     {\n",
    "#         'guessed_states': ['___b____e', 'a_aba___e', 'anaba__ne', 'anaba_ine'],\n",
    "#         'guessed_letters': ['a', 'n', 'i', 's'],\n",
    "#         'word': 'anabasine',\n",
    "#         'initial_state': '___b_____',\n",
    "#         'attributes': {'game_state': 'early', 'difficulty': 'easy', 'outcome': 'win', \n",
    "#                           'word_length': 9}\n",
    "#     },\n",
    "#     {\n",
    "#         'guessed_states': ['__c______', '_a_a_____'],\n",
    "#         'guessed_letters': ['c', 'a'],\n",
    "#         'word': 'cacophony',\n",
    "#         'initial_state': '__c______',\n",
    "#         'attributes': {'game_state': 'mid', 'difficulty': 'hard', 'outcome': 'lose', \n",
    "#                             'word_length': 9}\n",
    "#     }\n",
    "    \n",
    "#     # ... more games\n",
    "# ]\n",
    "\n",
    "# # Creating the HDF5 file\n",
    "# with h5py.File('HangmanData.h5', 'w') as f:\n",
    "#     for i, game in enumerate(games_data):\n",
    "#         # Create a group for each game\n",
    "#         game_group = f.create_group(f'game_{i+1}')\n",
    "\n",
    "#         # Serialize and store each part of the game data\n",
    "#         for key, value in game.items():\n",
    "#             if isinstance(value, dict):\n",
    "#                 # If the value is a dictionary, serialize it to JSON\n",
    "#                 value = json.dumps(value)\n",
    "#             game_group.create_dataset(key, data=value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e2750e",
   "metadata": {},
   "source": [
    "##### Data reading and Params Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a52a0793",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#### Papermil if need\n",
    " \n",
    "NUM_STRATIFIED_SAMPLES = 250_000 # This will be overwritten by Papermill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "945ee9c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:23:43.360693Z",
     "iopub.status.busy": "2023-11-24T22:23:43.360496Z",
     "iopub.status.idle": "2023-11-24T22:23:43.362839Z",
     "shell.execute_reply": "2023-11-24T22:23:43.362546Z"
    },
    "papermill": {
     "duration": 0.012935,
     "end_time": "2023-11-24T22:23:43.363813",
     "exception": false,
     "start_time": "2023-11-24T22:23:43.350878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/sayem/510B93E12554BBD1/dataset/250000\n"
     ]
    }
   ],
   "source": [
    "# Define the base directory\n",
    "base_dataset_dir = Path(\"/media/sayem/510B93E12554BBD1/dataset/\")\n",
    "\n",
    "# Create a subdirectory for the stratified samples\n",
    "stratified_samples_dir = base_dataset_dir / str(NUM_STRATIFIED_SAMPLES)\n",
    "stratified_samples_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(stratified_samples_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d927a799",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:23:43.391740Z",
     "iopub.status.busy": "2023-11-24T22:23:43.391624Z",
     "iopub.status.idle": "2023-11-24T22:23:43.560403Z",
     "shell.execute_reply": "2023-11-24T22:23:43.559962Z"
    },
    "papermill": {
     "duration": 0.173511,
     "end_time": "2023-11-24T22:23:43.561690",
     "exception": false,
     "start_time": "2023-11-24T22:23:43.388179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting Dataset Function\n",
    "import random\n",
    "\n",
    "def split_dataset(word_list, train_ratio=0.8):\n",
    "    total_words = len(word_list)\n",
    "    train_size = int(total_words * train_ratio)\n",
    "    random.shuffle(word_list)\n",
    "    return word_list[:train_size], word_list[train_size:]\n",
    "\n",
    "# Splitting the word list\n",
    "train_words, test_words = split_dataset(word_list)\n",
    "\n",
    "# Save split datasets to files\n",
    "save_words_to_file(train_words, stratified_samples_dir / 'train_words.txt')\n",
    "save_words_to_file(test_words, stratified_samples_dir / 'test_words.txt')\n",
    "\n",
    "# Calculate Frequencies and Max Word Length\n",
    "word_frequencies = calculate_word_frequencies(train_words)\n",
    "char_frequency = calculate_char_frequencies(train_words)\n",
    "max_word_length = max(len(word) for word in train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8cc199b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:23:43.568919Z",
     "iopub.status.busy": "2023-11-24T22:23:43.568821Z",
     "iopub.status.idle": "2023-11-24T22:23:43.571482Z",
     "shell.execute_reply": "2023-11-24T22:23:43.571208Z"
    },
    "papermill": {
     "duration": 0.007294,
     "end_time": "2023-11-24T22:23:43.572518",
     "exception": false,
     "start_time": "2023-11-24T22:23:43.565224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181840"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469a87ce",
   "metadata": {},
   "source": [
    "##### Stratified Sample Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e486f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:23:43.579494Z",
     "iopub.status.busy": "2023-11-24T22:23:43.579377Z",
     "iopub.status.idle": "2023-11-24T22:23:43.640436Z",
     "shell.execute_reply": "2023-11-24T22:23:43.640019Z"
    },
    "papermill": {
     "duration": 0.065848,
     "end_time": "2023-11-24T22:23:43.641509",
     "exception": false,
     "start_time": "2023-11-24T22:23:43.575661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Statrified samples: 250000\n",
      "227300\n"
     ]
    }
   ],
   "source": [
    "## we are taking starified samples from train_words\n",
    "\n",
    "from scr.custom_sampler import \\\n",
    "    stratified_sample_by_length_and_frequency, \\\n",
    "        stratified_sample_by_length, stratified_sample_by_length_and_uniqueness\n",
    "\n",
    "print(f'Number of Statrified samples: {NUM_STRATIFIED_SAMPLES}')\n",
    "\n",
    "# sampled_words_by_length_and_frequency \\\n",
    "#     = stratified_sample_by_length_and_frequency(train_words, \\\n",
    "#     word_frequencies, \\\n",
    "#     NUM_STRATIFIED_SAMPLES)\n",
    "\n",
    "sampled_words_by_length = stratified_sample_by_length_and_uniqueness(word_list, \\\n",
    "    NUM_STRATIFIED_SAMPLES)\n",
    "\n",
    "\n",
    "print(len(sampled_words_by_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0d326f",
   "metadata": {
    "papermill": {
     "duration": 0.004373,
     "end_time": "2023-11-24T22:23:44.300659",
     "exception": false,
     "start_time": "2023-11-24T22:23:44.296286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Intial State Simulation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae53fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.game import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1415477e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'allMasked': '____________',\n",
       " 'early': '____o_o_____',\n",
       " 'quarterRevealed': '__t_o_o_t___',\n",
       " 'midRevealed': 'myt_o_o_t___',\n",
       " 'midLateRevealed': 'mytho_oet__e',\n",
       " 'lateRevealed': 'mythopoet_ze',\n",
       " 'nearEnd': 'mythopoet_ze'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word = \"mississippi\"\n",
    "word = \"mythopoetize\"\n",
    "# word = \"cat\"\n",
    "\n",
    "initial_states = process_word_for_six_states(word)\n",
    "\n",
    "initial_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c3da54c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:23:44.317155Z",
     "iopub.status.busy": "2023-11-24T22:23:44.317069Z",
     "iopub.status.idle": "2023-11-24T22:23:44.319452Z",
     "shell.execute_reply": "2023-11-24T22:23:44.319138Z"
    },
    "papermill": {
     "duration": 0.005907,
     "end_time": "2023-11-24T22:23:44.320079",
     "exception": false,
     "start_time": "2023-11-24T22:23:44.314172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5cffa5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T22:23:44.326727Z",
     "iopub.status.busy": "2023-11-24T22:23:44.326580Z",
     "iopub.status.idle": "2023-11-24T22:23:44.329649Z",
     "shell.execute_reply": "2023-11-24T22:23:44.329220Z"
    },
    "papermill": {
     "duration": 0.007091,
     "end_time": "2023-11-24T22:23:44.330204",
     "exception": false,
     "start_time": "2023-11-24T22:23:44.323113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'allMasked': '____________',\n",
       " 'early': '____o_o_____',\n",
       " 'quarterRevealed': '__t_o_o_t___',\n",
       " 'midRevealed': 'myt_o_o_t___',\n",
       " 'midLateRevealed': 'mytho_oet__e',\n",
       " 'lateRevealed': 'mythopoet_ze',\n",
       " 'nearEnd': 'mythopoet_ze'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01da2152",
   "metadata": {
    "papermill": {
     "duration": 0.002642,
     "end_time": "2023-11-24T22:23:44.335321",
     "exception": false,
     "start_time": "2023-11-24T22:23:44.332679",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Dataset Generation: Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3dfd089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Initial States:  {'allMasked': '___________', 'early': '__ss_ss____', 'quarterRevealed': '__ss_ss____', 'midRevealed': '_ississi__i', 'midLateRevealed': '_ississippi', 'lateRevealed': '_ississippi', 'nearEnd': '_ississippi'}\n"
     ]
    }
   ],
   "source": [
    "word = \"mississippi\"\n",
    "\n",
    "initial_states = process_word_for_six_states(word)\n",
    "\n",
    "# print(initial_states)\n",
    "# Print generated initial states\n",
    "print(\"Generated Initial States: \", initial_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f65e7334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Initial States:\n",
      "___________\n",
      "For initial state: ___________\n",
      "Guessed: 'c', New State: '___________', Correct: False\n",
      "Guessed: 'd', New State: '___________', Correct: False\n",
      "Guessed: 'o', New State: '___________', Correct: False\n",
      "Guessed: 'g', New State: '___________', Correct: False\n",
      "Guessed: 'i', New State: '_i__i__i__i', Correct: True\n",
      "Guessed: 'j', New State: '_i__i__i__i', Correct: False\n",
      "Guessed: 'q', New State: '_i__i__i__i', Correct: False\n",
      "________pp_\n",
      "For initial state: ________pp_\n",
      "Guessed: 's', New State: '__ss_ss_pp_', Correct: True\n",
      "Guessed: 'w', New State: '__ss_ss_pp_', Correct: False\n",
      "Guessed: 'l', New State: '__ss_ss_pp_', Correct: False\n",
      "Guessed: 'i', New State: '_ississippi', Correct: True\n",
      "Guessed: 'm', New State: 'mississippi', Correct: True\n",
      "________pp_\n",
      "For initial state: ________pp_\n",
      "Guessed: 'm', New State: 'm_______pp_', Correct: True\n",
      "Guessed: 'l', New State: 'm_______pp_', Correct: False\n",
      "Guessed: 'x', New State: 'm_______pp_', Correct: False\n",
      "Guessed: 'z', New State: 'm_______pp_', Correct: False\n",
      "Guessed: 'i', New State: 'mi__i__ippi', Correct: True\n",
      "Guessed: 'q', New State: 'mi__i__ippi', Correct: False\n",
      "Guessed: 's', New State: 'mississippi', Correct: True\n",
      "__ss_ss_pp_\n",
      "For initial state: __ss_ss_pp_\n",
      "Guessed: 't', New State: '__ss_ss_pp_', Correct: False\n",
      "Guessed: 'm', New State: 'm_ss_ss_pp_', Correct: True\n",
      "Guessed: 'i', New State: 'mississippi', Correct: True\n",
      "_ississippi\n",
      "For initial state: _ississippi\n",
      "Guessed: 'u', New State: '_ississippi', Correct: False\n",
      "Guessed: 'm', New State: 'mississippi', Correct: True\n",
      "_ississippi\n",
      "For initial state: _ississippi\n",
      "Guessed: 'm', New State: 'mississippi', Correct: True\n",
      "_ississippi\n",
      "For initial state: _ississippi\n",
      "Guessed: 'm', New State: 'mississippi', Correct: True\n"
     ]
    }
   ],
   "source": [
    "from scr.game import simulate_game_progress, \\\n",
    "    play_game_with_a_word, process_word\n",
    "\n",
    "# Example word and initial state\n",
    "# Example usage\n",
    "word = \"mississippi\"\n",
    "# word = \"cat\"\n",
    "\n",
    "initial_states = process_word_for_six_states(word)\n",
    "\n",
    "# print(initial_states)\n",
    "# Print generated initial states\n",
    "print(\"Generated Initial States:\")\n",
    "for state_name, initial_state in initial_states.items():\n",
    "    # Simulate the game\n",
    "    print(initial_state)\n",
    "    print(f\"For initial state: {initial_state}\")\n",
    "    won, game_progress = simulate_game_progress(\n",
    "        model=None,  # Assuming model is not used in this example\n",
    "        word=word, \n",
    "        initial_state=initial_state, \n",
    "        char_frequency={},  # Assuming char_frequency is not used in this example\n",
    "        max_word_length=len(word), \n",
    "        device=None,  # Assuming device is not used in this example\n",
    "        max_attempts=6, \n",
    "        normalize=True,\n",
    "        difficulty=\"medium\", \n",
    "        outcome_preference='win'\n",
    "    )\n",
    "\n",
    "    # Display game progress\n",
    "    for step in game_progress:\n",
    "        print(f\"Guessed: '{step[0]}', New State: '{step[1]}', Correct: {step[2]}\")\n",
    "\n",
    "        # break\n",
    "\n",
    "    # break\n",
    "\n",
    "    # print(\"Game Result:\", \"Won\" if won else \"Lost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7482ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/228 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 228/228 [03:49<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total games processed: 9546084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to process a batch of games\n",
    "def process_batch_to_parquet(batch, file_path, start_game_counter):\n",
    "    game_counter = start_game_counter\n",
    "    data_for_parquet = []\n",
    "\n",
    "    for game_data in batch:\n",
    "        word, state_name, initial_state, difficulty, outcome, won, game_progress = game_data\n",
    "\n",
    "        # Check if game_progress is not empty\n",
    "        if not game_progress:\n",
    "            continue  # Skip to the next game if game_progress is empty\n",
    "\n",
    "        final_state = game_progress[-1][1]  # Adjust this based on your game_progress structure\n",
    "\n",
    "        # Preparing data for storage\n",
    "        guessed_states = [initial_state] + [state for _, state, _ in game_progress]\n",
    "        guessed_letters = [letter for letter, _, _ in game_progress]\n",
    "\n",
    "        # Add to data list for parquet\n",
    "        data_for_parquet.append({\n",
    "            'game_id': game_counter,\n",
    "            'word': word,\n",
    "            'initial_state': initial_state,\n",
    "            'final_state': final_state,\n",
    "            'guessed_states': ','.join(guessed_states[:-1]),\n",
    "            'guessed_letters': ','.join(guessed_letters),\n",
    "            'game_state': state_name,\n",
    "            'difficulty': difficulty,\n",
    "            'outcome': outcome,\n",
    "            'word_length': len(word),\n",
    "            'won': won\n",
    "        })\n",
    "\n",
    "        game_counter += 1\n",
    "\n",
    "    # Create a DataFrame and write to Parquet\n",
    "    df = pd.DataFrame(data_for_parquet)\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, file_path, compression='snappy')\n",
    "\n",
    "    return game_counter\n",
    "\n",
    "# Adjust the rest of the code to generate and process batches\n",
    "\n",
    "# Function to generate a batch of game data\n",
    "def generate_batch(start_index, end_index, sampled_words, game_states_func):\n",
    "    batch = []\n",
    "    for i in range(start_index, end_index):\n",
    "        word = sampled_words[i]\n",
    "        game_states = game_states_func(word)  # Your function to get game states\n",
    "        for state_name, initial_state in game_states.items():\n",
    "            for difficulty in [\"easy\", \"medium\", \"hard\"]:\n",
    "                for outcome in [\"win\", \"lose\"]:\n",
    "                    won, game_progress = simulate_game_progress(\n",
    "                        model=None, word=word, initial_state=initial_state,\n",
    "                        char_frequency=char_frequency, max_word_length=max_word_length,\n",
    "                        device=device, max_attempts=6, normalize=True,\n",
    "                        difficulty=difficulty, outcome_preference=outcome\n",
    "                    )\n",
    "                    batch.append((word, state_name, initial_state, \\\n",
    "                        difficulty, outcome, won, game_progress))\n",
    "    return batch\n",
    "\n",
    "\n",
    "# Main execution\n",
    "batch_size = 1000  # Adjust based on your requirements\n",
    "parquet_file_path = stratified_samples_dir / \"HangmanData.parquet\"\n",
    "\n",
    "\n",
    "# ######################### Uncomment if necessary #####################################\n",
    "\n",
    "game_counter = 1\n",
    "total_words = len(sampled_words_by_length)\n",
    "\n",
    "for start_index in tqdm(range(0, total_words, batch_size)):\n",
    "    end_index = min(start_index + batch_size, total_words)\n",
    "    batch = generate_batch(start_index, end_index, \\\n",
    "        sampled_words_by_length, process_word_for_six_states)\n",
    "    game_counter = process_batch_to_parquet(batch, parquet_file_path, game_counter)\n",
    "\n",
    "print(f\"Total games processed: {game_counter - 1}\")\n",
    "\n",
    "# ######################### Uncomment if necessary #####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d1359cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>word</th>\n",
       "      <th>initial_state</th>\n",
       "      <th>final_state</th>\n",
       "      <th>guessed_states</th>\n",
       "      <th>guessed_letters</th>\n",
       "      <th>game_state</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>outcome</th>\n",
       "      <th>word_length</th>\n",
       "      <th>won</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9533833</td>\n",
       "      <td>antivivisectionists</td>\n",
       "      <td>___________________</td>\n",
       "      <td>antivivisectionists</td>\n",
       "      <td>___________________,_____________o_____,______...</td>\n",
       "      <td>o,s,t,c,v,n,i,e,a</td>\n",
       "      <td>allMasked</td>\n",
       "      <td>easy</td>\n",
       "      <td>win</td>\n",
       "      <td>19</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9533834</td>\n",
       "      <td>antivivisectionists</td>\n",
       "      <td>___________________</td>\n",
       "      <td>antivivisectionists</td>\n",
       "      <td>___________________,_n____________n____,_n____...</td>\n",
       "      <td>n,e,l,a,s,t,h,v,i,z,m,c,p,o</td>\n",
       "      <td>allMasked</td>\n",
       "      <td>easy</td>\n",
       "      <td>lose</td>\n",
       "      <td>19</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9533835</td>\n",
       "      <td>antivivisectionists</td>\n",
       "      <td>___________________</td>\n",
       "      <td>__t_v_v_s__t____sts</td>\n",
       "      <td>___________________,___________________,______...</td>\n",
       "      <td>y,g,s,h,t,f,v,u,r</td>\n",
       "      <td>allMasked</td>\n",
       "      <td>medium</td>\n",
       "      <td>win</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9533836</td>\n",
       "      <td>antivivisectionists</td>\n",
       "      <td>___________________</td>\n",
       "      <td>antivivise_tionists</td>\n",
       "      <td>___________________,___________________,______...</td>\n",
       "      <td>r,b,e,p,v,i,n,s,a,t,x,j,o,m</td>\n",
       "      <td>allMasked</td>\n",
       "      <td>medium</td>\n",
       "      <td>lose</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9533837</td>\n",
       "      <td>antivivisectionists</td>\n",
       "      <td>___________________</td>\n",
       "      <td>__ti_i_i__cti__i_t_</td>\n",
       "      <td>___________________,___________________,__t___...</td>\n",
       "      <td>g,t,y,i,c,w,b,m,z</td>\n",
       "      <td>allMasked</td>\n",
       "      <td>hard</td>\n",
       "      <td>win</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   game_id                 word        initial_state          final_state  \\\n",
       "0  9533833  antivivisectionists  ___________________  antivivisectionists   \n",
       "1  9533834  antivivisectionists  ___________________  antivivisectionists   \n",
       "2  9533835  antivivisectionists  ___________________  __t_v_v_s__t____sts   \n",
       "3  9533836  antivivisectionists  ___________________  antivivise_tionists   \n",
       "4  9533837  antivivisectionists  ___________________  __ti_i_i__cti__i_t_   \n",
       "\n",
       "                                      guessed_states  \\\n",
       "0  ___________________,_____________o_____,______...   \n",
       "1  ___________________,_n____________n____,_n____...   \n",
       "2  ___________________,___________________,______...   \n",
       "3  ___________________,___________________,______...   \n",
       "4  ___________________,___________________,__t___...   \n",
       "\n",
       "               guessed_letters game_state difficulty outcome  word_length  \\\n",
       "0            o,s,t,c,v,n,i,e,a  allMasked       easy     win           19   \n",
       "1  n,e,l,a,s,t,h,v,i,z,m,c,p,o  allMasked       easy    lose           19   \n",
       "2            y,g,s,h,t,f,v,u,r  allMasked     medium     win           19   \n",
       "3  r,b,e,p,v,i,n,s,a,t,x,j,o,m  allMasked     medium    lose           19   \n",
       "4            g,t,y,i,c,w,b,m,z  allMasked       hard     win           19   \n",
       "\n",
       "     won  \n",
       "0   True  \n",
       "1   True  \n",
       "2  False  \n",
       "3  False  \n",
       "4  False  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # Replace this with the path to your Parquet file\n",
    "# parquet_file_path = 'path/to/your/HangmanData.parquet'\n",
    "\n",
    "# Read the Parquet file\n",
    "df = pd.read_parquet(parquet_file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3a8c199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of games in the dataset: 12252\n",
      "game_id            0\n",
      "word               0\n",
      "initial_state      0\n",
      "final_state        0\n",
      "guessed_states     0\n",
      "guessed_letters    0\n",
      "game_state         0\n",
      "difficulty         0\n",
      "outcome            0\n",
      "word_length        0\n",
      "won                0\n",
      "dtype: int64\n",
      "            game_id   word_length\n",
      "count  1.225200e+04  12252.000000\n",
      "mean   9.539958e+06     16.471596\n",
      "std    3.536992e+03      6.614032\n",
      "min    9.533833e+06      1.000000\n",
      "25%    9.536896e+06     15.000000\n",
      "50%    9.539958e+06     19.000000\n",
      "75%    9.543021e+06     21.000000\n",
      "max    9.546084e+06     29.000000\n",
      "Number of unique words: 300\n",
      "outcome\n",
      "win     6126\n",
      "lose    6126\n",
      "Name: count, dtype: int64\n",
      "difficulty\n",
      "easy      4084\n",
      "medium    4084\n",
      "hard      4084\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of rows (games) in the DataFrame\n",
    "total_games = len(df)\n",
    "\n",
    "print(f\"Total number of games in the dataset: {total_games}\")\n",
    "\n",
    "# Additional checks you might want to perform:\n",
    "# - Check for any null values or anomalies in the data\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# - Get a summary of the DataFrame\n",
    "print(df.describe())\n",
    "\n",
    "# - Count the number of unique words or game states\n",
    "unique_words = df['word'].nunique()\n",
    "print(f\"Number of unique words: {unique_words}\")\n",
    "\n",
    "# - Inspect the distribution of game outcomes, difficulties, etc.\n",
    "print(df['outcome'].value_counts())\n",
    "print(df['difficulty'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1057e0",
   "metadata": {},
   "source": [
    "##### Reading Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a076a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'game_id': 9533701,\n",
       " 'word': 'cachecache',\n",
       " 'initial_state': ['__________'],\n",
       " 'final_state': 'cachecache',\n",
       " 'guessed_states': ['__________', '___h____h_', '___he___he', '_a_he_a_he'],\n",
       " 'guessed_letters': ['h', 'e', 'a', 'c'],\n",
       " 'game_state': 'allMasked',\n",
       " 'difficulty': 'easy',\n",
       " 'outcome': 'win',\n",
       " 'word_length': 10,\n",
       " 'won': False}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from scr.dataset import HangmanDataset # , custom_collate_fn\n",
    "\n",
    "from scr.feature_engineering import process_batch_of_games\n",
    "\n",
    "# Assuming HangmanDataset is defined as provided\n",
    "hangman_dataset = HangmanDataset(parquet_file_path)  # Replace with your Parquet file path\n",
    "\n",
    "hangman_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a575e3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'game_id': 9546084,\n",
       " 'word': 'hydroxydesoxycorticosterone',\n",
       " 'initial_state': ['hy_roxy_esoxycorticosterone'],\n",
       " 'final_state': 'hydroxydesoxycorticosterone',\n",
       " 'guessed_states': ['hy_roxy_esoxycorticosterone',\n",
       "  'hy_roxy_esoxycorticosterone',\n",
       "  'hy_roxy_esoxycorticosterone',\n",
       "  'hy_roxy_esoxycorticosterone',\n",
       "  'hy_roxy_esoxycorticosterone'],\n",
       " 'guessed_letters': ['b', 'q', 'z', 'm', 'd'],\n",
       " 'game_state': 'nearEnd',\n",
       " 'difficulty': 'hard',\n",
       " 'outcome': 'lose',\n",
       " 'word_length': 27,\n",
       " 'won': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hangman_dataset[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc7f82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12384"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hangman_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93bdbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from scr.dataset import HangmanDataset # , custom_collate_fn\n",
    "\n",
    "# from scr.feature_engineering import process_batch_of_games\n",
    "\n",
    "# # Assuming HangmanDataset is defined as provided\n",
    "# hangman_dataset = HangmanDataset(parquet_file_path)  # Replace with your Parquet file path\n",
    "\n",
    "# dataloader = DataLoader(hangman_dataset, batch_size=32, \\\n",
    "#     collate_fn=custom_collate_fn, shuffle=True)\n",
    "\n",
    "# for batch in dataloader:\n",
    "#     # Now, directly use the keys of the batch dictionary\n",
    "#     states = batch['guessed_states']\n",
    "#     guesses = batch['guessed_letters']\n",
    "#     max_seq_length = batch['max_seq_len']\n",
    "#     original_seq_lengths =  batch['original_seq_lengths']\n",
    "\n",
    "#     states, guesses, max_seq_length, original_seq_lengths = batch\n",
    "\n",
    "#     assert len(states) == len(guesses)\n",
    "\n",
    "#     print(len(states))\n",
    "#     print(len(guesses))\n",
    "#     print(max_seq_length)\n",
    "#     print(original_seq_lengths)\n",
    "\n",
    "#     batch_states, batch_missed_chars = process_batch_of_games(states, guesses,\n",
    "#                            char_frequency, max_word_length, max_seq_length)\n",
    "\n",
    "#     print(batch_states.shape)\n",
    "\n",
    "#     print(batch_missed_chars.shape)\n",
    "\n",
    "#     # return batch_states, batch_missed_chars, max_seq_length, original_seq_lengths \n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e9dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word = 'sassa_ras_s'\n",
    "# fets = build_feature_set(word, char_frequency, \\\n",
    "#     max_word_length, ngram_n=3, normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/sayem/Desktop/Hangman/1_a_dataset_generation.ipynb",
   "output_path": "/home/sayem/Desktop/Hangman/1_a_dataset_generation.ipynb",
   "parameters": {},
   "start_time": "2023-11-24T22:23:41.671735",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
