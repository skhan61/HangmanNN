{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "import sys\n",
    "# Custom library paths\n",
    "sys.path.extend(['../', './scr'])\n",
    "\n",
    "from scr.utils import set_seed\n",
    "from scr.utils import read_words\n",
    "from pathlib import Path\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from scr.feature_engineering import \\\n",
    "    calculate_char_frequencies, calculate_word_frequencies\n",
    "from scr.utils import read_words, save_words_to_file\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from scr.dataset import *\n",
    "from scr.game import *\n",
    "from scr.plot_utils import *\n",
    "\n",
    "import gc\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from scr.utils import print_scenarios\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Read and Shuffle Word List\n",
    "word_list = read_words('data/words_250000_train.txt') # , limit=10000)\n",
    "# word_list = read_words('data/250k.txt', limit=10000)\n",
    "random.shuffle(word_list)\n",
    "\n",
    "# base_dataset_dir = Path('dataset/pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 181840 train words from dataset/15000/train_words.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "NUM_STRATIFIED_SAMPLES = 15000\n",
    "# # Define the base directory\n",
    "base_dataset_dir = Path(f'./dataset/{NUM_STRATIFIED_SAMPLES}')\n",
    "# base_dataset_dir = Path(f\"/media/sayem/510B93E12554BBD1/dataset/{NUM_STRATIFIED_SAMPLES}\")\n",
    "\n",
    "# Ensuring the base directory and 'pkl' subdirectory exist\n",
    "base_dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "pkls_dir = base_dataset_dir / 'pkl'\n",
    "pkls_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Paths to the words files\n",
    "train_words_file_path = base_dataset_dir / 'train_words.txt'\n",
    "test_words_file_path = base_dataset_dir / 'test_words.txt'\n",
    "\n",
    "# Read the words from the files\n",
    "try:\n",
    "    train_words = read_words(train_words_file_path)\n",
    "    print(f\"Loaded {len(train_words)} train words from {train_words_file_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {train_words_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('dataset/15000')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For inference\n",
    "from scr.feature_engineering import *\n",
    "\n",
    "word_frequencies = calculate_word_frequencies(word_list)\n",
    "char_frequency = calculate_char_frequencies(word_list)\n",
    "max_word_length = max(len(word) for word in word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.simple_model import SimpleLSTM\n",
    "from scr.base_model import BaseModel\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from scr.feature_engineering import *\n",
    "\n",
    "# max_word_length = 29 # TODO will remove later\n",
    "# Instantiate and test the model\n",
    "config = {\n",
    "    'embedding_dim': 200,\n",
    "    'hidden_dim': 256,\n",
    "    'num_layers': 2,\n",
    "    'vocab_size': 27,\n",
    "    'max_word_length': max_word_length,\n",
    "    'input_feature_size': 5,\n",
    "    'use_embedding': True,\n",
    "    'miss_linear_dim': 50\n",
    "}\n",
    "\n",
    "model = SimpleLSTM(config)\n",
    "optimizer = model.optimizer\n",
    "\n",
    "# Assuming 'model' is your trained model instance\n",
    "model.save_model(file_path='models/model.pth')\n",
    "\n",
    "# Assuming the saved model file is 'models/model.pth'\n",
    "model_file_path = 'models/model.pth'\n",
    "\n",
    "# Specify the device to load the model onto\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = BaseModel.load_model(SimpleLSTM, model_file_path)\n",
    "# Now `loaded_model` is an instance of `SimpleLSTM` with the state and config loaded\n",
    "\n",
    "# model = loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Loading and train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('dataset/15000/pkl')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkls_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.dataset import ProcessedHangmanDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from scr.model_training import *\n",
    "\n",
    "# # Load the dataset\n",
    "processed_dataset = ProcessedHangmanDataset(pkls_dir, \\\n",
    "    char_frequency, max_word_length, files_limit=None)\n",
    "    \n",
    "print(len(processed_dataset)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/sayem/Desktop/Hangman/2_training_notebook.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sayem/Desktop/Hangman/2_training_notebook.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m processed_dataset[\u001b[39m100\u001b[39;49m]\n",
      "File \u001b[0;32m~/Desktop/Hangman/scr/dataset.py:143\u001b[0m, in \u001b[0;36mProcessedHangmanDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m    142\u001b[0m     \u001b[39mif\u001b[39;00m idx \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata):\n\u001b[0;32m--> 143\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIndex out of range\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m     game_states, labels, additional_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[idx]\n\u001b[1;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m game_states, labels, additional_info\n",
      "\u001b[0;31mIndexError\u001b[0m: Index out of range"
     ]
    }
   ],
   "source": [
    "processed_dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PyTorch dataset to a list for train_test_split\n",
    "dataset_list = [processed_dataset[i] for i in range(len(processed_dataset))]\n",
    "\n",
    "# Perform an 80%-20% train-test split\n",
    "train_data, val_data = train_test_split(dataset_list, \\\n",
    "    test_size=0.2, random_state=42)\n",
    "\n",
    "val_loader = processed_dataset.create_val_loader(val_data)\n",
    "\n",
    "del dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Data Balance Sanity check\n",
    "# from scr.dataset import *\n",
    "\n",
    "# analyze_dataset_sanity(processed_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Data Balance Sanity check\n",
    "# from scr.dataset import *\n",
    "\n",
    "# analyze_word_length_balance(processed_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Untrained Model Performence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Call the validation function\n",
    "# validation_results = validate_hangman(model, val_loader, \\\n",
    "#     char_frequency, max_word_length, device, max_attempts=6, normalize=True, \\\n",
    "#         max_games_per_epoch=1000)\n",
    "\n",
    "# # Access the results\n",
    "# character_level_results = validation_results[\"character_level\"]\n",
    "# game_simulation_results = validation_results[\"game_simulation\"]\n",
    "\n",
    "# from scr.plot_utils import *\n",
    "\n",
    "# save_path = Path(f\"plots/{NUM_STRATIFIED_SAMPLES}_untrained_model_performence_word_stats_plot.png\")\n",
    "# plot_word_stats(game_simulation_results[\"length_stats\"], save_path)\n",
    "\n",
    "# # # Print results for character-level validation\n",
    "# # print(\"Character Level Validation:\")\n",
    "# print(f\"Average Loss: {character_level_results['avg_loss']}\")\n",
    "# print(f\"Miss Penalty: {character_level_results['avg_miss_penalty']}\")\n",
    "# # print(f\"Character Accuracy: {character_level_results['char_accuracy']}\")\n",
    "# # print(f\"Word Accuracy: {character_level_results['word_accuracy']}\")\n",
    "# # print(\"Word Statistics:\", character_level_results[\"word_stats\"])  # Updated key\n",
    "\n",
    "# # # Print results for game simulation\n",
    "# # print(\"\\nGame Simulation:\")\n",
    "# print(f\"Win Rate: {game_simulation_results['win_rate']}\")\n",
    "# print(f\"Average Attempts: {game_simulation_results['average_attempts']}\")\n",
    "# print(f\"Total Games: {game_simulation_results['total_games']}\")\n",
    "# print(f\"Total Wins: {game_simulation_results['total_wins']}\")\n",
    "# print(f\"Total Losses: {game_simulation_results['total_losses']}\")\n",
    "# # print(\"Word Stats:\", game_simulation_results[\"game_stats\"])\n",
    "# # print(\"Word Length Stats:\", game_simulation_results[\"length_stats\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Total Games: {game_simulation_results['total_games']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_simulation_results[\"length_stats\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_performance_dict = game_simulation_results[\"length_stats\"]\n",
    "\n",
    "# init_performance_dict = {\n",
    "#     length: {\"wins\": 1, \"losses\": 1, \"total_attempts\": 2, \"games\": 2}\n",
    "#     for length in range(1, max_word_length + 1)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initialize_performance_dict(dataset, default_games=10):\n",
    "#     \"\"\"\n",
    "#     Initializes the performance dictionary based on the word lengths in the dataset.\n",
    "\n",
    "#     :param dataset: Dataset object.\n",
    "#     :param default_games: Total default games for each word length.\n",
    "#     :return: A dictionary with initial performance metrics for each word length in the dataset.\n",
    "#     \"\"\"\n",
    "#     unique_word_lengths = set()\n",
    "#     for _, _, additional_info in dataset:\n",
    "#         word_length = int(additional_info['word_length'])\n",
    "#         unique_word_lengths.add(word_length)\n",
    "\n",
    "#     performance_dict = {}\n",
    "#     default_wins = 1\n",
    "#     default_losses = 1\n",
    "#     remaining_games = default_games - default_wins - default_losses\n",
    "\n",
    "#     for length in unique_word_lengths:\n",
    "#         performance_dict[length] = {\n",
    "#             \"wins\": default_wins + remaining_games // 2,\n",
    "#             \"losses\": default_losses + remaining_games // 2,\n",
    "#             \"total_attempts\": default_games,\n",
    "#             \"games\": default_games\n",
    "#         }\n",
    "\n",
    "#     return performance_dict\n",
    "\n",
    "# # Example usage\n",
    "# init_performance_dict = initialize_performance_dict(processed_dataset, default_games=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_performance_dict = game_simulation_results[\"length_stats\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256 # hyperparams\n",
    "\n",
    "# custom_sampler = PerformanceBasedSampler(train_data, batch_size, init_performance_dict)\n",
    "train_loader = DataLoader(train_data, \\\n",
    "    batch_size=batch_size, \\\n",
    "        collate_fn=processed_dataset.custom_collate_fn)\n",
    "\n",
    "for batch in train_loader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 256 # hyperparams\n",
    "\n",
    "# custom_sampler = PerformanceBasedSampler(train_data, batch_size, init_performance_dict)\n",
    "# train_loader = DataLoader(train_data, batch_sampler=custom_sampler, \\\n",
    "#     collate_fn=processed_dataset.custom_collate_fn)\n",
    "\n",
    "# for batch in train_loader:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_performance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from collections import Counter\n",
    "\n",
    "# def plot_word_length_distribution(batch, batch_idx):\n",
    "#     # Extracting word lengths from the second tensor in the batch\n",
    "#     word_lengths = batch[1].tolist()  # Assuming batch[1] contains word lengths\n",
    "#     counter = Counter(word_lengths)\n",
    "#     plt.bar(counter.keys(), counter.values())\n",
    "#     plt.title(f\"Word Length Distribution in Batch {batch_idx}\")\n",
    "#     plt.xlabel(\"Word Length\")\n",
    "#     plt.ylabel(\"Count\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # Inspect the first batch\n",
    "# for i, batch in enumerate(train_loader):\n",
    "#     plot_word_length_distribution(batch, i)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def train_model(model, train_data, init_performance_dict, \\\n",
    "    val_loader, num_epochs, optimizer, batch_size, scheduler=None, device=device):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    n_epochs_stop = 10  # Number of epochs to stop after no improvement\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Update the sampler with the latest performance dictionary\n",
    "        custom_sampler = PerformanceBasedSampler(train_data, batch_size, \\\n",
    "            init_performance_dict)\n",
    "            \n",
    "        train_loader = DataLoader(train_data, batch_sampler=custom_sampler, \\\n",
    "            collate_fn=processed_dataset.custom_collate_fn)\n",
    "\n",
    "        # train_loader = DataLoader(train_data, batch_size=batch_size, \\\n",
    "        #     collate_fn=processed_dataset.custom_collate_fn)\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        train_loss, train_miss_penalty = train_on_data_loader(model, \\\n",
    "            train_loader, device, optimizer)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Training Loss: {train_loss}, \\\n",
    "Miss Penalty: {train_miss_penalty}\")\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            validation_results = validate_hangman(model, val_loader, char_frequency, \\\n",
    "                max_word_length, device)\n",
    "            val_loss = validation_results[\"character_level\"]['avg_loss']\n",
    "            # Update performance dict for the next epoch\n",
    "            init_performance_dict = validation_results[\"game_simulation\"][\"length_stats\"]\n",
    "\n",
    "        # Scheduler step\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == n_epochs_stop:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch {epoch}: val loss: {val_loss}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "# # Assuming model, init_train_data, val_loader, num_epochs, optimizer, batch_size are already defined\n",
    "# init_performance_dict = {length: {\"wins\": 0, \"losses\": 0, \"total_attempts\": 0, \"games\": 0} \\\n",
    "#     for length in range(1, max_word_length + 1)}\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', \\\n",
    "    patience=5, factor=0.1, verbose=True)\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "train_model(model, train_data, init_performance_dict, \\\n",
    "    val_loader, num_epochs, optimizer, batch_size, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# def train_model(model, train_data, val_data, num_epochs, optimizer, scheduler, device):\n",
    "#     train_loader = DataLoader(train_data, batch_size=batch_size, \\\n",
    "#         collate_fn=custom_collate_fn)\n",
    "#     val_loader = DataLoader(val_data, batch_size=batch_size, \\\n",
    "#         collate_fn=custom_collate_fn)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         train_loss, train_miss_penalty = train_epoch(model, train_loader, \\\n",
    "#             optimizer, device)\n",
    "#         val_loss, val_miss_penalty = validate_epoch(model, \\\n",
    "#             val_loader, device)\n",
    "\n",
    "#         scheduler.step(val_loss)  # Adjust LR based on validation loss\n",
    "\n",
    "#         print(f\"Epoch {epoch}: Training Loss: {train_loss}, Miss Penalty: \\\n",
    "#             {train_miss_penalty}, Validation Loss: {val_loss}, Validation Miss Penalty: {val_miss_penalty}\")\n",
    "#         # Save model checkpoints if needed\n",
    "\n",
    "# def k_fold_cross_validate(model, dataset, k, num_epochs, optimizer, scheduler_class, device):\n",
    "#     kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "#     for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "#         print(f\"Fold {fold}\")\n",
    "#         train_subset = Subset(dataset, train_idx)\n",
    "#         val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "#         # Initialize a new model for each fold\n",
    "#         model = SimpleLSTM(config)\n",
    "#         model.to(device)\n",
    "#         optimizer = model.optimizer\n",
    "#         scheduler = scheduler_class(optimizer)\n",
    "\n",
    "#         train_subset = Subset(dataset, train_idx)\n",
    "#         val_subset = Subset(dataset, val_idx)\n",
    "#         train_model(model, train_subset, val_subset, num_epochs, optimizer, scheduler, device)\n",
    "\n",
    "\n",
    "#         train_model(model, train_subset, val_subset, num_epochs, optimizer, scheduler, device)\n",
    "\n",
    "# # Usage example\n",
    "# num_epochs = 10\n",
    "# batch_size = 64\n",
    "# scheduler_class = torch.optim.lr_scheduler.ReduceLROnPlateau  # Example scheduler class\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# k_fold_cross_validate(model, processed_dataset, \\\n",
    "#     5, num_epochs, optimizer, scheduler_class, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trained Model Performence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = processed_dataset.create_val_loader(val_data)\n",
    "\n",
    "\n",
    "# Call the validation function\n",
    "validation_results = validate_hangman(model, val_loader, \\\n",
    "    char_frequency, max_word_length, device)\n",
    "\n",
    "# Access the results\n",
    "character_level_results = validation_results[\"character_level\"]\n",
    "game_simulation_results = validation_results[\"game_simulation\"]\n",
    "\n",
    "from scr.plot_utils import *\n",
    "\n",
    "save_path = Path(f\"plots/{NUM_STRATIFIED_SAMPLES}_trained_model_performence_word_stats_plot.png\")\n",
    "plot_word_stats(game_simulation_results[\"length_stats\"], save_path)\n",
    "\n",
    "# # Print results for character-level validation\n",
    "# print(\"Character Level Validation:\")\n",
    "print(f\"Average Loss: {character_level_results['avg_loss']}\")\n",
    "print(f\"Miss Penalty: {character_level_results['avg_miss_penalty']}\")\n",
    "# print(f\"Character Accuracy: {character_level_results['char_accuracy']}\")\n",
    "# print(f\"Word Accuracy: {character_level_results['word_accuracy']}\")\n",
    "# print(\"Word Statistics:\", character_level_results[\"word_stats\"])  # Updated key\n",
    "\n",
    "# # Print results for game simulation\n",
    "# print(\"\\nGame Simulation:\")\n",
    "print(f\"Win Rate: {game_simulation_results['win_rate']}\")\n",
    "print(f\"Average Attempts: {game_simulation_results['average_attempts']}\")\n",
    "print(f\"Total Games: {game_simulation_results['total_games']}\")\n",
    "print(f\"Total Wins: {game_simulation_results['total_wins']}\")\n",
    "print(f\"Total Losses: {game_simulation_results['total_losses']}\")\n",
    "# print(\"Word Stats:\", game_simulation_results[\"game_stats\"])\n",
    "# print(\"Word Length Stats:\", game_simulation_results[\"length_stats\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Word Stats:\", game_simulation_results[\"game_stats\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_features_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_features_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_miss_penalty = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        if batch[0] is None:\n",
    "            continue  # Skip empty batches\n",
    "\n",
    "        game_states_batch, lengths_batch, missed_chars_batch, labels_batch = batch\n",
    "        game_states_batch, lengths_batch, missed_chars_batch = \\\n",
    "            game_states_batch.to(device), lengths_batch, missed_chars_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(game_states_batch, lengths_batch, missed_chars_batch)\n",
    "        model_output_shape = outputs.shape\n",
    "        reshaped_labels = pad_and_reshape_labels(labels_batch, model_output_shape).to(device)\n",
    "\n",
    "        loss, miss_penalty = model.calculate_loss(outputs, reshaped_labels, \\\n",
    "                                                  lengths_batch, missed_chars_batch, 27)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_miss_penalty += miss_penalty.item()  # Accumulate miss penalty\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_miss_penalty = total_miss_penalty / len(data_loader)\n",
    "    return avg_loss, avg_miss_penalty  # Return average loss and miss penalty\n",
    "\n",
    "\n",
    "\n",
    "def validate_epoch(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_miss_penalty = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            if batch[0] is None:\n",
    "                continue  # Skip empty batches\n",
    "\n",
    "\n",
    "            game_states_batch, lengths_batch, missed_chars_batch, labels_batch = batch\n",
    "            game_states_batch, lengths_batch, missed_chars_batch = \\\n",
    "                game_states_batch.to(device), lengths_batch, missed_chars_batch.to(device)\n",
    "\n",
    "            outputs = model(game_states_batch, lengths_batch, missed_chars_batch)\n",
    "            model_output_shape = outputs.shape\n",
    "            reshaped_labels = pad_and_reshape_labels(labels_batch, model_output_shape).to(device)\n",
    "\n",
    "            loss, miss_penalty = model.calculate_loss(outputs, reshaped_labels, \\\n",
    "                                                      lengths_batch, missed_chars_batch, 27)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            total_miss_penalty += miss_penalty.item()  # Accumulate miss penalty\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_miss_penalty = total_miss_penalty / len(data_loader)\n",
    "    return avg_loss, avg_miss_penalty  # \n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "#     val_loss = validate_epoch(model, val_loader, device)\n",
    "    \n",
    "#     print(f\"Epoch {epoch}: Training Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "#     # You can add code to save model checkpoints if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "def train_model(model, train_data, val_data, num_epochs, optimizer, scheduler, device):\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, \\\n",
    "        collate_fn=custom_collate_fn)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, \\\n",
    "        collate_fn=custom_collate_fn)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_miss_penalty = train_epoch(model, train_loader, \\\n",
    "            optimizer, device)\n",
    "        val_loss, val_miss_penalty = validate_epoch(model, \\\n",
    "            val_loader, device)\n",
    "\n",
    "        scheduler.step(val_loss)  # Adjust LR based on validation loss\n",
    "\n",
    "        print(f\"Epoch {epoch}: Training Loss: {train_loss}, Miss Penalty: \\\n",
    "            {train_miss_penalty}, Validation Loss: {val_loss}, Validation Miss Penalty: {val_miss_penalty}\")\n",
    "        # Save model checkpoints if needed\n",
    "\n",
    "def k_fold_cross_validate(model, dataset, k, num_epochs, optimizer, scheduler_class, device):\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f\"Fold {fold}\")\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        # Initialize a new model for each fold\n",
    "        model = SimpleLSTM(config)\n",
    "        model.to(device)\n",
    "        optimizer = model.optimizer\n",
    "        scheduler = scheduler_class(optimizer)\n",
    "\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        train_model(model, train_subset, val_subset, num_epochs, optimizer, scheduler, device)\n",
    "\n",
    "\n",
    "        train_model(model, train_subset, val_subset, num_epochs, optimizer, scheduler, device)\n",
    "\n",
    "# Usage example\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "scheduler_class = torch.optim.lr_scheduler.ReduceLROnPlateau  # Example scheduler class\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "k_fold_cross_validate(model, processed_dataset, \\\n",
    "    5, num_epochs, optimizer, scheduler_class, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.dataset import ProcessedHangmanDataset\n",
    "from scr.custom_sampler import PerformanceBasedSampler\n",
    "\n",
    "\n",
    "print(f\"Total number of data points in the dataset: {len(processed_dataset)}\")\n",
    "\n",
    "# Initialize the sampler\n",
    "sampler = PerformanceBasedSampler(\n",
    "    processed_dataset, \n",
    "    performance_metrics,\n",
    "    max_word_length=max_word_length\n",
    ")\n",
    "\n",
    "print(f\"Sampler created with {len(sampler)} indices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = list(sampler)[:10]  # Get the first 10 sampled indices\n",
    "print(\"Sampled indices:\", sample_indices)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    data_point = processed_dataset[idx]\n",
    "    print(f\"Data at index {idx}: {data_point}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scr.utils import print_scenarios\n",
    "\n",
    "def process_pkl_files(base_dir):\n",
    "    pkl_list = []\n",
    "\n",
    "    # Iterate over all batch directories\n",
    "    for batch_dir in sorted(base_dir.iterdir(), key=lambda x: int(x.name) if x.name.isdigit() else float('inf')):\n",
    "        if batch_dir.is_dir():\n",
    "            # List all .pkl files in the current batch directory\n",
    "            pkl_files = list(batch_dir.glob(\"*.pkl\"))\n",
    "\n",
    "            for pkl_file in pkl_files:\n",
    "                try:\n",
    "                    with open(pkl_file, 'rb') as file:\n",
    "                        game_data = pickle.load(file)\n",
    "                except IOError as e:\n",
    "                    print(f\"Error reading file {pkl_file}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Processing each pickle file\n",
    "                pkl_list.extend(process_pkl_file(pkl_file, game_data))\n",
    "\n",
    "    return pkl_list\n",
    "\n",
    "def process_pkl_file(pkl_file, game_data):\n",
    "    file_scenarios = []\n",
    "    for data in game_data:\n",
    "        game_won, guesses = data\n",
    "        word, initial_state, difficulty, outcome = extract_info_from_filename(pkl_file)\n",
    "        \n",
    "        # Create a scenario dictionary for each data tuple\n",
    "        scenario = {\n",
    "            'word': word,\n",
    "            'difficulty': difficulty,\n",
    "            'outcome': outcome,\n",
    "            'data': (game_won, guesses)\n",
    "        }\n",
    "        file_scenarios.append((pkl_file, scenario))  # Add scenario to the list\n",
    "\n",
    "    return file_scenarios\n",
    "\n",
    "def extract_info_from_filename(pkl_file):\n",
    "    parts = pkl_file.stem.split('_from_')\n",
    "    word_and_state = parts[0].split('_')\n",
    "    word = '_'.join(word_and_state[:-1])\n",
    "    initial_state = word_and_state[-1]\n",
    "    difficulty, outcome = parts[1].split('_')[-2:]\n",
    "    return word, initial_state, difficulty, outcome\n",
    "\n",
    "# def print_scenarios(scenarios):\n",
    "#     # Assuming this function is defined elsewhere\n",
    "#     pass\n",
    "\n",
    "# Process all pickle files\n",
    "pkl_list = process_pkl_files(base_dataset_dir)\n",
    "\n",
    "# Accessing an individual pickle file's content by index\n",
    "index_to_access = 0  # Change this index to access different files\n",
    "if index_to_access < len(pkl_list):\n",
    "    file_path, scenario = pkl_list[index_to_access]\n",
    "    print(f\"Contents of {file_path}:\")\n",
    "    print_scenarios([scenario])  # Wrap scenario in a list for the function\n",
    "else:\n",
    "    print(f\"No pickle file at index {index_to_access}\")\n",
    "\n",
    "No pickle file at index 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_list = []\n",
    "\n",
    "# Iterate over all batch directories\n",
    "for batch_dir in sorted(base_dataset_dir.iterdir(), \\\n",
    "    key=lambda x: int(x.name) if x.name.isdigit() else float('inf')):\n",
    "    if batch_dir.is_dir():\n",
    "        # List all .pkl files in the current batch directory\n",
    "        pkl_files = list(batch_dir.glob(\"*.pkl\"))\n",
    "\n",
    "        for pkl_file in pkl_files:\n",
    "            with open(pkl_file, 'rb') as file:\n",
    "                game_data = pickle.load(file)\n",
    "                # Extract information from file name\n",
    "                parts = pkl_file.stem.split('_from_')\n",
    "                word_and_statet = parts[0].split('_')\n",
    "                word = '_'.join(word_and_state[:-1])\n",
    "                initial_state = word_and_state[-1]\n",
    "                difficulty, outcome = parts[1].split('_')[-2:]\n",
    "\n",
    "                # Assuming game_data is a list of tuples (game_won, guesses)\n",
    "                for data in game_data:\n",
    "                    game_won, guesses = data\n",
    "                    # Create a scenario dictionary for each data tuple\n",
    "                    scenario = {\n",
    "                        'word': word,\n",
    "                        'difficulty': difficulty,\n",
    "                        'outcome': outcome,\n",
    "                        'data': (game_won, guesses)\n",
    "                    }\n",
    "                    pkl_list.append((pkl_file, scenario))  # Add scenario to the list\n",
    "\n",
    "# Accessing an individual pickle file's content by index\n",
    "index_to_access = 0  # Change this index to access different files\n",
    "if index_to_access < len(pkl_list):\n",
    "    file_path, scenario = pkl_list[index_to_access]\n",
    "    print(f\"Contents of {file_path}:\")\n",
    "    print_scenarios([scenario])  # Wrap scenario in a list for the function\n",
    "else:\n",
    "    print(f\"No pickle file at index {index_to_access}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
